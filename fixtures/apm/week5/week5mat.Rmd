

```{r setup, include=FALSE}
knitr::opts_chunk$set(comment=NA, warning = FALSE, message = FALSE)
library(ggplot2)
library(GGally)
library(gridExtra)
library(MASS)

# transparent theme
Rmkd_theme <- theme_light()+
        theme(panel.background = element_rect(fill = "transparent", colour = NA),
              plot.background = element_rect(fill = "transparent", colour = NA),
              panel.border = element_rect(fill = NA, colour = "black", size = 1),
              legend.background = element_rect(fill = "transparent", colour = NA))
theme_set(Rmkd_theme)

g_legend <- function(a.gplot){
  tmp <- ggplot_gtable(ggplot_build(a.gplot))
  leg <- which(sapply(tmp$grobs, function(x) x$name) == "guide-box")
  legend <- tmp$grobs[[leg]]
  return(legend)}


```

# Models for count responses


In this week's material we will cover GLMs that can be applied to count data. Examples of count data can be either rates (counts per unit time/area/distance), such as the number of

* hospital admissions due to respiratory disease in each of the 1235 intermediate geographies that make up Scotland
* nests per $9 m^2$ of birds in a certain habitat
* train accidents in a given year

or cell frequencies in a **contingency table**, for instance numbers cross-classified by sex, age group and importance of power steering and air-conditioning in the car preference data we analysed in Week 4.

A common distribution for such data is the Poisson distribution. For $Y$ the number of occurrences, we assume that $Y$ follows the Poisson distribution $\text{Po}(\mu)$ with probability mass function given by \[f(y)=\frac{\mu^y e^{-\mu}}{y!}, \hspace{1cm} y=0,1,2,\dots\]

The mean and variance of $Y \sim \text{Po}(\mu)$ are both equal to $\mu$. The parameter $\mu$ should be defined carefully: *e.g.* the average number of customers who buy a particular product out of every 100 customers who enter the store.
The rate should also include a time scale, *e.g.* number of motor vehicle crashes per 1000 population per year.
In general, the rate is specified in terms of units of **exposure**. Consider occupational injuries as an example: each worker is exposed for the period spent at work, so the rate can be defined in terms of person-years at risk.
We wish to model the effect of explanatory variables on the response $Y$ through the parameter $\mu$.


## Poisson regression

Let $Y_1,\dots, Y_n$ be independent random variables with $Y_i$ denoting the number of events occurred from exposure $n_i$ for the $i$th covariate pattern. Then \[E(Y_i)=\mu_i=n_i \theta_i.\]
For instance, $Y_i$ could be the number of insurance claims for a particular make and model of car. This will depend on the number, $n_i$, of cars of this type that are insured and other variables that affect $\theta_i$ such as the age of the cars and where they are used.
The subscript $i$ denotes $i$th covariate pattern, that is the different combinations of explanatory variables such as the make and model of the car, its age, location etc.

The dependence on explanatory variables is usually modelled by $\theta_i=e^{\mathbf{x}_i^\intercal \boldsymbol{\beta}}$. The corresponding GLM is 
$$E(Y_i)=\mu_i=n_i e^{\mathbf{x}_i^\intercal \boldsymbol{\beta}}; \hspace{1cm} Y_i \sim \text{Po}(\mu_i)$$
This corresponds to the log link:
$$ \log \mu_i = \log n_i +\mathbf{x}_i^\intercal \boldsymbol{\beta}$$
The term $\log n_i$ is called the **offset**. 


###[example] Epidemiology data
Suppose that we wish to model $Y_i$, the number of cancer cases in the $i$th intermediate geography (IG) in Glasgow for $i=1,\dots,271$. The intermediate geographies are small areas that contain between 2,500 and 6,000 people. As the IGs can differ in population and demographics, a direct comparison of the number of cancer cases from each IG may not be appropriate. Instead, we use offsets, $E_i$, which are expected numbers of cancer cases in each IG, to allow for differences in population sizes and demographic structures.

Assume that the $Y_i$ are independent $\text{Po}(\mu_i)$ with $$\log(\mu_i)=\log(E_i)+\mathbf{x}_i^\intercal \boldsymbol{\beta}.$$ We can write this as $\log(\mu_i/E_i)=\mathbf{x}_i^\intercal \boldsymbol{\beta}$ to emphasise that we are modelling the rate of occurrence of cancer. From the model equation we can see that the offset $\log(E_i)$ is a term with a fixed coefficient of 1.

The dataset `cancer.txt` contains the following variables:

* `Y_all`: number of cases of all types of cancer in the IG for the year 2013

* `E_all`: expected number of cases of all types of cancer for the IG based on the population size and demographics of the IG in 2013

* `pm10`: air pollution 

* `smoke`: percentage of people in an area that smoke

* `ethnic`: percentage of people who are non-white

* `logprice`: natural log of average house price

* `easting` and `northing`: co-ordinates of the central point of the IG divided by 10,000


The first ten rows of the data are shown below.

```{r}
cancer <- read.table(url("http://www.stats.gla.ac.uk/~tereza/rp/cancer.txt"),
                     header=TRUE)
head(cancer)
```
Pair plots of the data are shown below.

<!-- library(gpairs) -->
<!-- gpairs(cancer[,c(-1,-3)], diag.pars = list(fontsize = 16)) -->


```{r}
ggpairs(cancer[,c(-1,-3)], 
        upper=list(continuous=wrap("points", alpha=0.4, color="#d73027")), 
        lower="blank", axisLabels="none")
```

We fit a Poisson regression model for these data, including a term for the offset, as follows: 

```{r}
epid1 <- glm(Y_all ~ pm10 + smoke + ethnic + log.price + easting +
                     northing+offset(log(E_all)), family = poisson, data = cancer)
summary(epid1)
```


###[/example]



## Interpretation of regression coefficients

For a binary explanatory variable denoted by an indicator variable ($x_j=0$ if the factor is absent and $x_j=1$ if it is present), the *rate ratio*, RR, for presence vs. absence is $$RR=\frac{E(Y_i |\text{present})}{E(Y_i |\text{absent})}=e^{\beta_j}.$$

Similarly, for a continuous explanatory variable $x_k$, a one-unit increase is associated with a multiplicative effect of $e^{\beta_k}$ on the rate $\mu$.


## Hypothesis tests

Hypotheses about the parameters $\beta_j$ can be tested using the **Wald** test ($z$-statistic and $p$-value obtained from a standard normal distribution). This is because, as we saw in Week 2, for the MLE $\hat{\beta}_j$ of parameter $\beta_j$, we have the asymptotic result \[\frac{\hat{\beta}_j-\beta_j}{\text{se}(\hat{\beta}_j)} \sim N(0,1) \text{ approximately.}\]
  
**Confidence intervals** can be obtained in a similar way. By the same asymptotic result, we can take \[\hat{\beta}_j \pm 1.96 \text{se}(\hat{\beta}_j)\] to obtain an approximate 95\% confidence interval for $\hat{\beta}_j.$ For intervals on the rate ratio scale, we simply take \[\exp(\hat{\beta}_j \pm 1.96 \text{se}(\hat{\beta}_j)).\]
  
Alternatively, hypothesis tests for nested models can be performed by comparing the difference in **deviance** with a chi-squared distribution with the appropriate degree of freedom.


###[example] Epidemiology data continued

Suppose that we are interested in the effect of air pollution on health, and that we wish to interpret the coefficient of `pm10` to describe this effect on the cancer incidence rate. The coefficient is positive, which suggests that the cancer incidence rate increases with increased pollution. The rate ratio allows us to quantify by how much. For every unit increase in `pm10`, the rate increases by a factor of $\exp(0.0500269)=1.051$. For an approximate confidence interval we take $\exp(0.0500269 \pm 1.96 \times 0.0066724)= (1.038,1.065)$. As the confidence interval does not include 1, this effect is significant.

Note: it is sometimes more useful to interpret the rate ratio associated with an increase of $\omega$ units in the exposure, where $\omega$ is often taken to be one standard deviation of that variable. For `pm10`, the standard deviation is

```{r}
sd(cancer$pm10)
```
and the corresponding rate ratio is

```{r}
exp(0.0500269*sd(cancer$pm10))
```
###[/example]

###[task]
Interpret the coefficient of `smoke` in a similar way.

####[answer]
We can either take $\exp(0.00335159)=1.00336$ and interpret it as the rate ratio associated with one unit increase in the percentage of people who smoke or we can choose another percentage, *e.g.* 10\%, so that we interpret $\exp(0.00335159*10)=1.034$ as the rate ratio associated with an increase of 10 units in the percentage of people who smoke, or we can go with the standard deviation of `smoke`

```{r}
sd(cancer$smoke)
```

which in this case turns out to be quite similar. A point estimate and an approximate confidence interval for the rate ratio associated with one standard deviation increase in the percentage of people who smoke can be obtained as follows:

```{r}
exp(0.00335159*sd(cancer$smoke)) # point estimate

exp((0.00335159-1.96*0.0009462959)*sd(cancer$smoke)) # approx 95% CI lower limit

exp((0.00335159+1.96*0.0009462959)*sd(cancer$smoke)) # approx 95% CI upper limit
```

####[/answer]
###[/task]


## Fitted values and residuals

Fitted values can be obtained as\[\hat{Y}_i = \hat{\mu}_i = n_i \exp(\mathbf{x}^\intercal _i \hat{\boldsymbol{\beta}}), \hspace{1cm} i = 1, \dots, n.\]
These are denoted by $e_i$ as they are estimates of the expected values $E(Y_i)=\mu_i$. In a similar way, we denote the observed values $y_i$  by $o_i$.

For the Poisson distribution $\textrm{Var}(Y_i)=E(Y_i)$ so the standard error of $Y_i$ is estimated by $\sqrt{\hat{\mu}_i}=\sqrt{e_i}$.

In a normal linear model, residuals are defined as $y_i-\hat{\mu}_i=o_i-e_i$, and they are used to check assumptions such as linearity, normality and constant variance. In GLMs the variance is not constant but it varies with the mean and in fact in the Poisson model the variance is equal to the mean. For this reason, we cannot rely on the raw (also known as *response*) residuals $o_i-e_i$ and we use **Pearson** or **deviance** residuals instead. Note that these residuals were first introduced in the context of binomial models in Week 3.

###[definition] Pearson residuals

We  define the $i$th **Pearson residual** as \[r_i=\frac{o_i-e_i}{\sqrt{e_i}}\] where $o_i$ is the observed value of $Y_i$ and $e_i$ is the corresponding fitted value.
###[/definition]

###[definition] Pearson goodness-of-fit statistic

If we sum the Pearson residuals squared we obtain the **Pearson chi-squared statistic** $X^2$: 
\[X^2=\sum r_i^2 = \sum \frac{(o_i-e_i)^2}{e_i}.\]
Note that this is the usual chi-squared goodness-of-fit statistic for contingency tables.
###[/definition]

It can be shown that the **deviance** $D$ for a Poisson model is \[D=2 \left[\sum y_i \log(y_i/\hat{y}_i)-\sum(y_i-\hat{y}_i)\right] =2 \left[\sum o_i \log(o_i/e_i)-\sum(o_i-e_i) \right]\]

For most models $\sum o_i=\sum e_i$ so this simplifies even further to \[D=2 \sum o_i \log(o_i/e_i) .\]

### [definition] Deviance residuals

The $i$th **deviance residual** is the square root of the contribution of the $i$th covariate pattern to the deviance, $D$:
\[d_i = \text{sign}(o_i-e_i) \sqrt{2 [o_i \log(o_i/e_i)-(o_i-e_i)]}, \hspace{1cm} i=1,\dots, n, \] so that $D=\sum d_i^2$. Here \begin{align*} \text{sign}(o_i-e_i)= \left \lbrace \begin{array}{rl} 1 & \text{ if } o_i>e_i,\\
0 &  \text{ if } o_i=e_i, \\
-1 & \text{ if } o_i<e_i.\\  \end{array} \right. \end{align*}

###[/definition]

Pearson and deviance residuals can be used for identifying outliers and for checking the linearity assumption when plotted against explanatory variables in a GLM. However, residuals are not useful for GLMs with binary responses, binomial responses with small group sizes and Poisson responses that are relatively small because of the discreteness of the residuals in these cases.

## Goodness-of-fit statistics

In addition to residual plots, we can check for lack of fit of a model using the **Pearson chi-squared statistic** and the **deviance**. These are closely related (they are both obtained by summing the respective residuals squared). Furthermore, they can be compared with a $\chi^2(n-p)$ distribution to assess the goodness of fit of a model in which $p$ parameters are estimated.
  
The chi-squared distribution is likely to be a better approximation for $X^2$ than for $D$, although both rely on having sufficiently large fitted values.

<!-- Another goodness of fit statistic is the **likelihood ratio chi-squared statistic**, $C=2[l(\hat{\boldsymbol{\beta}})-l(\hat{\boldsymbol{\beta}}_{\min})]$, which provides an overall test of the hypotheses that the coefficients of all of the explanatory variables are zero, by comparison with the $\chi^2(p-1)$ distribution. -->

We will look at diagnostics, measures of goodness of fit and overdispersion in Poisson regression through an example.

## [video, videoid="GPsFwdZoDKo", duration="10m11s"] Regression models for count data -- Galapagos example

###[example] GLMs for plant species in the Galapagos

For 30 Galapagos islands the number of plant species found in each was recorded, along with several geographical variables.[^1]

[^1]: M. P. Johnson and P. H. Raven (1973) "Species number and endemism: The Galapagos Archipelago revisited" Science, 179, 893-895.

The dataset `gala` is available from `library(faraway)` in R and contains the following variables: 

* *Species*, the number of species found on the island, 
* *Endemics*, the number of endemic species, 
* *Area*, the area of the island (km$^2$), 
* *Elevation*, the highest elevation of the island (m), 
* *Nearest*, the distance from the nearest island (km), 
* *Scruz*, the distance from Santa Cruz island (km), 
* *Adjacent*, the area of the adjacent island (km$^2$).

The first six rows of the data are shown below.

```{r}
library(faraway)
head(gala)
```

<!-- ```{r, echo = FALSE} -->
<!-- knitr::kable(gala[1:10,], format = "markdown", padding = 2) -->
<!-- ``` -->

We can explore these variables by constructing a pairs plot; notice there is a positive relationship between `Species` and `Elevation`, for instance, but that for many of the variables it is hard to see what the relationship with `Species` might be without log-transforming first.

```{r }
ggpairs(gala, upper=list(continuous=wrap("points", alpha=0.4, color="#d73027")), 
        lower="blank", axisLabels="none")
```


We fit a Poisson model for the number of species as a function of the geographical variables using the `glm()` function in R, making sure to specify `family = poisson` in the arguments:

```{r}
gal1 <- glm(Species ~ Area + Elevation + Nearest + Scruz + Adjacent,
            family = poisson, data = gala)
summary(gal1)
```


The deviance is $D = 716.85$ which is very large compared with a $\chi^2(24)$, indicating a poor fit if the Poisson is the correct model for the response. The Pearson statistic is $X^2= 761.97$, also indicating a poor fit. 

We can check that the large deviance is not the result of an outlier by looking at residual plots. Although the first two plots below are normal probability plots, we are only using them here to spot any points that don't follow the straight line. We can also plot the deviance (or Pearson) residuals against the linear predictor to look for nonlinearity in the relationship between the fitted values and the residuals as shown in the third panel below. There is no obvious pattern here.

```{r, fig.height= 2.5}
resp <- resid(gal1, type = "pearson")
resd <- resid(gal1, type = "deviance")

p1<- ggplot(gal1, aes(sample = resp)) + geom_point(stat = "qq", color = "#7fc97f") + 
     ylab("Pearson residuals")
p2<- ggplot(gal1, aes(sample = resd)) + geom_point(stat = "qq", color = "#7fc97f") + 
     ylab("Deviance residuals") 
p3<- ggplot(gal1, aes(x = predict(gal1, type="link"), y =resd))+ 
        geom_point(col = "#7fc97f") + 
        ylab("Deviance residuals") + xlab("Linear predictor")
grid.arrange(p1, p2, p3, nrow = 1)
```

###[/example]

## Overdispersion

Suppose that in a Poisson regression model the link function and choice of explanatory variables is correct, but that the assumption that $\mathrm{Var}(Y_i)=\mu_i$ does not hold. If $\mathrm{Var}(Y_i)>\mu_i$ we say that we have **overdispersion**. This appears to be the case for the Galapagos data -- notice in the figure below that most of the points lie above the line of equality for mean and variance.


```{r, out.width='75%', fig.align='center', fig.asp=0.8}
 ggplot(gal1, aes(x=log(fitted(gal1)), y=log((gala$Species-fitted(gal1))^2)))+
        geom_point(col="#f46d43") +
        geom_abline(slope=1, intercept=0, col="#a6d96a", size=1) +
        ylab(expression((y-hat(mu))^2)) + xlab(expression(hat(mu)))
```

The issue with overdispersion is that while the regression parameter estimates are stil consistent, their standard errors will be wrong. In this case we are not able to determine which explanatory variables are significant.

## Quasi-Poisson model

One way to deal with overdispersion is to introduce a dispersion parameter $\phi$ such that $\textrm{Var}(Y_i)= \phi \mu_i$. We can estimate this dispersion parameter by $$ \hat \phi = \frac {X^2} {n-p}.$$

```{r}
X2 <- sum(resid(gal1, type = "pearson")^2)
dp <- X2 / gal1$df.res
dp
```

The dispersion parameter is then used to adjust the standard errors in the summary. Notice that the regression coefficients do not change.

```{r}
summary(gal1, dispersion = dp)
```

With the use of the estimated dispersion parameter the Wald tests are not very reliable, so we turn to an F test to determine the significance of the regression coefficients:

```{r, warning=FALSE}
drop1(gal1, test = "F")
```

We could now perform variable selection by dropping `Nearest` first and then repeating the process until only significant terms are left in the model.

The following residual plots show the effect of the quasi-Poisson model on the residuals.


```{r, fig.height=4}
# Residual plots vs. predicted
pred <- predict(gal1, type = "response")
stand.resid <- rstandard(model = gal1, type = "pearson") # Standardised Pearson residuals

par(mfrow=c(1,2))
plot(x = pred, y = stand.resid, xlab = "Predicted count", ylab = "Standardised Pearson residuals",
   main = "Regular likelihood", ylim = c(-5,5))
abline(h = c(-3, -2, 0, 2, 3), lty = "dotted", col = "red")


gal2 <- glm(Species ~ Area + Elevation + Nearest + Scruz + Adjacent,
            family = quasipoisson(link = "log"), data = gala) # Quasi-Poisson model

pred <- predict(gal2, type = "response")
stand.resid <- rstandard(model = gal2, type = "pearson") # Standardised Pearson residuals

plot(x = pred, y = stand.resid, xlab = "Predicted count", ylab = "Standardised Pearson residuals",
   main = "Quasi-likelihood", ylim = c(-5,5))
abline(h = c(-3, -2, 0, 2, 3), lty = "dotted", col = "red")
```

We can see in the plots that all residuals are contained within $\pm 3$ in the quasi-Poisson model, while quite a few are outside this range for the original Poisson model.

Another way to deal with overdispersion is to assume a more flexible distribution for the response that would allow for a variance that is larger than the mean. The negative binomial distribution is one such distribution.

## Negative binomial models

We can deal with overdispersion by assuming a negative binomial distribution for the response, which allows for a variance larger than the mean. The form of the GLM is still the same as in the Poisson model with a linear component $\mathbf{x}_i^\intercal \boldsymbol{\beta}$. The link function is taken to be the log link, and the model equation is $g(\mu_i)=\log(\mu_i)=\mathbf{x}_i^\intercal \boldsymbol{\beta}$. Hence, the interpretation of the regression coefficients is similar to the Poisson case.

One parameterisation for a random variable $Y_i$ following the negative binomial distribution is 
$$f(y_i; \theta, \mu_i)= \frac{\Gamma(\theta+y_i)}{\Gamma(\theta)y_i!} \cdot \frac{\mu_i^{y_i} \theta^\theta}{(\mu_i+\theta)^{y_i+\theta}} ~ \text{ for } y=0,1,2,\dots$$

The mean is $E(Y_i)=\mu_i$ but we can also see that  $\mathrm{Var}(Y_i)=\mu_i+\dfrac{\mu_i^2}{\theta}>E(Y_i)$. When fitting a negative binomial GLM, we estimate both the mean, $\mu_i$, and the parameter $\theta$.

## [example] Negative binomial model for the Galapagos plant species data

To fit a negative binomial model to the Galapagos data to account for the overdispersion, we use the function `glm.nb()` from `library(MASS)`:

```{r}
library(MASS)
gal3 <- glm.nb(Species ~ Area + Elevation + Nearest + Scruz + Adjacent,  
               data = gala)
summary(gal3)
```

We can compare the Poisson and negative binomial models by looking at their deviances and AIC scores:
```{r}
# Poisson model
c(gal1$deviance, gal1$aic)
# Negative binomial model
c(gal3$deviance, gal3$aic)
```
Both are much smaller for the negative binomial model.

Which correction for overdispersion should we choose, the quasi-Poisson or the negative binomial model? It may not matter much in practice, but one way to decide between the two is by plooting $(y_i-\hat{\mu}_i)^2$ v $\hat{\mu}_i$. We then plot a linear and quadratic fit to see which one of them fits better. If the relationship is linear the quasi-Poisson model is better -- if quadratic, the negative binomial model is better.

```{r, fig.height=4, fig.width=5}
# Plot of squared residuals v predicted values
res.sq <- residuals(gal1, type = "response")^2
set1 <- data.frame(res.sq, mu.hat = gal1$fitted.values)

fit.lin <- lm(formula = res.sq ~ mu.hat, data = set1)
fit.quad <- lm(formula = res.sq ~ mu.hat + I(mu.hat^2), data = set1)
summary(fit.quad)

plot(set1$mu.hat, y = set1$res.sq, xlab = "Predicted count",
     ylab = "Squared Residual")
curve(expr = predict(fit.lin, newdata = data.frame(mu.hat = x), type = "response"),
      col = "blue", add = TRUE, lty = "solid")
curve(expr = predict(fit.quad, newdata = data.frame(mu.hat = x), type = "response"),
      col = "red", add = TRUE, lty = "dashed")
legend("topleft", legend = c("Linear", "Quadratic"), col = c( "blue", "red"),
    lty = c("solid", "dashed"), bty = "n")
```

Here the blue line is the straight line fit which would indicate that the quasi-Poisson model suffices, and the red line is the quadratic fit which would suggest using a negative binomial model.

The quadratic coefficient in the above model is not significant ($p$-value of 0.2399). We don't have any evidence that the negative binomial model is better.

## [/example]

**Note:** Correcting for overdispersion should **not** be the first step in an analysis. If more explanatory variables are available or if the model can be improved in other ways, it's best to do that rather than just account for the excess variance. Corrections for overdispersion are just a patch, and are unlikely to solve problems with poor predictive performance. It's preferable to try and build a better model before resorting to these fixes.

###[task]
Fit a Poisson regression model to the Galapagos data after log-transforming the explanatory variables. Does this model fit the data better than the original Poisson model? Are all the explanatory variables significant? 

####[answer]
We first take log of every variable with the exception of distance to Santa Cruz for which we have to take log of `Scruz` plus a small value because Santa Cruz has distance to Santa Cruz=0.

```{r}
gal4<- glm(Species ~ log(Area)+ log(Elevation) + log(Nearest) +
            log(Scruz+0.1) + log(Adjacent),family=poisson,data = gala)
summary(gal4)
```
In the Poisson model there is a substantial reduction in deviance when using the log-transformed variables. Also `log(Elevation)` does not appear to be significant. We can drop the term for elevation from the model:

```{r}
gal5 <- glm(Species ~ log(Area) + log(Nearest) +
            log(Scruz+0.1) + log(Adjacent),family=poisson, data = gala)
summary(gal5)
```

Since there are still signs of overdispersion, we can estimate the dispersion parameter and use it in a quasi-Poisson model:

```{r}
dp <- sum(residuals(gal5,type="pearson")^2)/gal5$df.res #dispersion parameter
summary(gal5, dispersion=dp) # update standard errors

```
It looks like more terms can be dropped. When dropping terms, make sure you don't drop more than one at a time.

```{r}
drop1(gal5, test="F")
```
It looks like distance to Santa Cruz can be dropped.

Repeating the process we see that distance to nearest neighbour can also be dropped and the final model is one with terms for the logarithm of the area of the island and the logarithm of the area of the adjacent island.

####[/answer]
###[/task]

###[task]
In Example 1, the residual deviance of the fitted model is quite high compared to the degrees of freedom. Could this be due to overdispersion? Fit a quasi-Poisson model and a negative binomial model and explore diagnostic plots to check if one of these might be more appropriate.

####[answer]
Start by fitting a quasi-Poisson model following the analysis steps for the Galapagos data:
```{r, fig.height=4}
epid2 <- glm(Y_all ~ pm10 + smoke + ethnic + log.price + easting +
                     northing+offset(log(E_all)), family=quasipoisson(link = "log"),
                     data = cancer)
summary(epid2)

# Calculate the dispersion parameter:
pearson <- residuals(epid1, type = "pearson")
sum(pearson^2)/epid1$df.residual

# Residual plots vs. predicted (using standardised residuals):
pred <- predict(epid1, type = "response")
stand.resid <- rstandard(model = epid1, type = "pearson")

par(mfrow=c(1,2))
plot(x = pred, y = stand.resid, xlab = "Predicted count", ylab = "Standardized Pearson residuals",
   main = "Regular likelihood", ylim = c(-5,5))
abline(h = c(-3, -2, 0, 2, 3), lty = "dotted", col = "red")

pred <- predict(epid2, type = "response")
stand.resid <- rstandard(model = epid2, type = "pearson")

plot(x = pred, y = stand.resid, xlab = "Predicted count", ylab = "Standardized Pearson residuals",
   main = "Quasi-likelihood", ylim = c(-5,5))
abline(h = c(-3, -2, 0, 2, 3), lty = "dotted", col = "red")
```

We see that for the cancer data, just like for the Galapagos data, all of the residuals are contained within $\pm 3$ in the quasi-Poisson model, while quite a few are outside this range for the original Poisson model.

Next we fit a negative binomial model:

```{r, fig.height=4}
library(MASS)
epid3 <- glm.nb(Y_all ~ pm10 + smoke + ethnic + log.price + easting +
                     northing+offset(log(E_all)),  data = cancer)
summary(epid3)
```

To decide between the quasi-Poisson and the negative binomial model, we plot $(y_i-\hat{\mu}_i)^2$ v $\hat{\mu}_i$ and compare the linear and quadratic fit to see which one of them fits better.

```{r, fig.height=4, fig.width=5}
# Plot of squared residuals v predicted
res.sq <- residuals(epid1, type = "response")^2
set1 <- data.frame(res.sq, mu.hat = epid1$fitted.values)

fit.lin <- lm(formula = res.sq ~ mu.hat, data = set1)
fit.quad <- lm(formula = res.sq ~ mu.hat + I(mu.hat^2), data = set1)
summary(fit.quad)

plot(set1$mu.hat, y = set1$res.sq, xlab = "Predicted count",
     ylab = "Squared Residual")
curve(expr = predict(fit.lin, newdata = data.frame(mu.hat = x), type = "response"),
      col = "blue", add = TRUE, lty = "solid")
curve(expr = predict(fit.quad, newdata = data.frame(mu.hat = x), type = "response"),
      col = "red", add = TRUE, lty = "dashed")
legend("topleft", legend = c("Linear", "Quadratic"), col = c("blue","red"),
    lty = c("solid", "dashed"), bty = "n")
```

The straight line fit (blue line) would indicate that the quasi-Poisson model should be preferred, and the quadratic fit (red line) would suggest using a negative binomial model.

The quadratic coefficient in the above model has a $p$-value of 0.411 -- not significant. We don't have any evidence that the negative binomial model is better and we can probably go with either one.
####[/answer]
###[/task]

###[example] Predicting the total number of medals in the 2012 Olympics

We wish to model the number of medals won by each country in the London Olympics in 2012 as a function of the country’s population and GDP per capita. The dataset `OlympicMedals2012.csv` contains this information for all the countries that won at least one medal in the 2012 Games. A ranking of countries based on the total medals won in the Olympics can be found at https://en.wikipedia.org/wiki/2012_Summer_Olympics_medal_table.
We start by creating a variable for GDP per capita in 1000 US dollars and looking at some plots of the data.

```{r}
olympics0 <- read.csv(url("http://www.stats.gla.ac.uk/~tereza/rp/OlympicMedals2012.csv"))
olympics <- data.frame(country = olympics0$Country, medals = olympics0$Medals,
                        population = olympics0$Population, 
                        gold = olympics0$Gold.Medal,
                        GDP = olympics0$GDP..US.Billion)
olympics$GDPpercapita <- olympics$GDP * 10^6 / olympics$population
head(olympics)
```
The wide range of values for both population and GDP per capita suggests log-transforming both explanatory variables. From the plots we see a positive association between log(population) and log(medals) and also between log(GDP per capita) and log(medals). 

```{r, fig.height= 3}
p1 <- ggplot(olympics, aes(x=log(population), y=log(medals))) +
       geom_point(col="#f46d43")
p2 <- ggplot(olympics, aes(x=log(GDPpercapita), y=log(medals))) +
       geom_point(col="#f46d43")

grid.arrange(p1, p2, nrow=1)
```

We start the analysis by fitting a Poisson regression model.

```{r}
ol1 <- glm(medals ~ log(population) + log(GDPpercapita), 
              family = poisson, data = olympics)
summary(ol1)
```

Large population and high GDP per capita appear to contribute to Olympic success as indicated by the positive coefficients of both terms in the Poisson model. 
###[/example]

<!-- Remove this: -->
###[task]
The fit of the Poisson model is not particularly good (just look at the deviance of 547.5 on 82 degrees of freedom). Supposing that this is due to overdispersion, fit a quasi-Poisson model to adjust for it.

####[answer]
Estimate the dispersion parameter: 

```{r}
X2 <- sum(resid(ol1, type = "pearson")^2)
X2
dp <- X2/ol1$df.res
dp
```

Refit the model using the dispersion parameter:
```{r}
summary(ol1, dispersion= dp)

drop1(ol1, type="F")
```

The parameter estimates (and fitted values) are the same, and although the standard errors change, both predictors are still highly significant.
####[/answer]

###[/task]


###[task] 
Fit a negative binomial model to the Olympics data.

####[answer]
A negative binomial model can be fit using

```{r}
library(MASS)
ol2 <- glm.nb(medals ~ log(population) + log(GDPpercapita), data=olympics)
summary(ol2)
```

####[/answer]
###[/task]

###[task]
Examine the residuals from the negative binomial model and comment on any unusual observations.

####[answer]

Pearson residual plots for this model:
```{r, message=FALSE}
res <- resid(ol2, type = "pearson")
d2 <- data.frame(res=resid(ol2, type = "pearson"))

p3 <- ggplot(d2, aes(sample=res)) + geom_point(stat="qq",col="#a6d96a") +
    xlab("Theoretical quantiles") + ylab("Sample quantiles")

p4 <- ggplot(d2, aes(x=predict(ol2, type="link"), y=res)) +
        geom_point(col="#a6d96a") + geom_hline(yintercept = 0) +
        xlab("Linear predictor") + ylab("Pearson residuals")

p5 <- ggplot(d2, aes(x=log(olympics$GDPpercapita), y=res)) +
        geom_point(col="#a6d96a") + xlab("log(GDP per capita)") +
        ylab("Pearson residuals")

p6 <- ggplot(d2, aes(x=log(olympics$population), y=res)) +
        geom_point(col="#a6d96a") + xlab("log(population)") +
        ylab("Pearson residuals")

grid.arrange(p3, p4, p5, p6, nrow = 2)
```

The normal probability plot shows some large positive residuals. No patterns are seen in the plot of residuals against the linear predictor, and there is no obvious nonlinear pattern in the plots of residuals against the explanatory variables. 

We can also look at the cases where the model does not fit the data better (these correspond to large positive or negative residuals).
```{r}
pres.n2 <- resid(ol2, type = "pearson")
expmedals2 <- round(fitted(ol2)[abs(pres.n2) > 1], 2)
cbind(olympics[abs(pres.n2) > 1, 1:2], expmedals2)
```

Here we see that the model does a poor job of predicting strong Olympic performances by countries such as the USA, UK and China, and not so strong performances by countries such as India and Indonesia.
 
####[/answer]

###[/task]

###[task]
Do you think that either the Poisson or the negative binomial model with log(GDP) and log(population) as predictors would do a good job predicting the total number of medals won by countries in 2012? Explain.

####[answer]
Clearly there is more to winning Olympic medals than just the country's population and GDP per capita. The large residuals from the negative binomial model show that this is the case. Adjusting for overdispersion does not improve predictive performance in general, and may not be as useful as including additional predictors in the model.
####[/answer]
###[/task]

## Excess zeros

Sometimes overdispersion is observed due to excess zeros in the data. As an example, consider counting burst pipes in a city's water mains system: most of the time the number will be zero. A Poisson distribution does not cope well with too many zeros. A negative binomial distribution usually does better, but there are other options specifically for this type of data. Two types of models, **zero-inflated** and **hurdle** are particularly relevant.

### Zero-inflated models

In zero-inflated Poisson or negative binomial models, we assume that there are two processes that could be generating zeros in the data. One is a Bernoulli process and the other a Poisson process and the resulting data distribution is a mixture of the two. Such models may be appropriate in the following contexts:

* We are interested in the number of items bought, where the decision to buy these items can be influenced by a number of factors (these would go into the binary logistic regression part of the model). But even after the decision to buy the items, some people end up with zero items bought because they were out of stock or for some other reason.

* We are interested in the number of visits to the doctor, where the *decision* to visit the doctor is influenced by gender, age and other variables, and the *number* of visits by some other factors (or even the same, but not necessarily so). Zero visits could be because a person never goes to the doctor or because there was no occasion for which to go to the doctor.

To fit zero-inflated models, one can use the `zeroinfl()` function from `library(pscl)` in R. You can read more about this type of model and see examples of zero-inflated Poisson and negative binomial models in the package vignette available from https://cran.r-project.org/web/packages/pscl/vignettes/countreg.pdf.


### Hurdle models

Another type of model for data with excess zeros is a hurdle model. It assumes that there is a sequential process that first determines whether the count will be zero, and if the count is not zero then it has to be positive. Examples:

* Consider the number of days spent in hospital for patients arriving at A&E. The patients will either be admitted, in which case they will spend a number of days in hospital, or not, in which case the number of days hospitalised should be zero.

* Suppose that we are interested in the number of cigarettes consumed per week. Subjects will either be non-smokers, in which case their consumption is zero, or smokers and will therefore have a positive consumption. A set of explanatory variables could be used to predict whether a person is a smoker or not, and a potentially different set of explanatory variables could be used to predict the number of cigarettes for those who smoke.

To fit hurdle models, one can use the `hurdle()` function from `library(pscl)` in R. 

###[task]
In the Olympic medals example, suppose we also had data on the countries that participated in the 2012 Olympics but did not win any medals. What type of model would you consider for these data and why?

####[answer]
A zero-inflated Poisson or negative binomial model might be appropriate for simultaneously considering

* the probability of a country winning a medal in the 2012 Olympics (using a logit model), and

* the number of medals won (using a Poisson or negative binomial model).

In practice such a model could be fit using function `zero.infl()` from `library(pscl)`.

####[/answer]

###[/task]

## Loglinear models

The last component of this unit is on models for contingency tables, in which we have a count response tabulated by the levels of categorical explanatory variables. An example of such data is the car preference dataset we analysed in Week 4. 

If there is no constraint on the row or column totals of the table, we can assume that the counts are Poisson-distributed. Otherwise, depending on the constraint, we have a multinomial or product-multinomial distribution. All of these can be modelled using a GLM with a Poisson response and the log link. Usually the question of interest for such data is whether there is an association between the factors. To illustrate this, let us look at an example.

## [video, videoid="Ye6DSu_nRp0", duration="8m14s"] Loglinear models applied to food poisoning data

###[example] Loglinear model for food poisoning data
After a food poisoning outbreak, conference participants were surveyed in an effort to identify the cause of the outbreak. The potato salad and crab salad served at the conference were considered as possible sources. Participants were asked if they had either, both or none, and also if the got sick (food poisoned). The data are shown in the table below.

\begin{center}
\begin{tabular}{rrrrr}
\hline
&  \multicolumn{2}{c}{Potato} &  \multicolumn{2}{c}{No Potato}\\
& Crab  & No Crab  & Crab  & No Crab\\
\hline
Not sick & 80 & 24 & 31 & 23\\
Sick & 120 & 22 & 4 & 0\\
\hline
Total & 200 & 46 & 35 & 23\\
\hline
\end{tabular}
\end{center}


The data can be read into R as follows:

```{r}
fp <- data.frame(potato=rep(c("yes","yes","no","no"),2),
                            crab=rep(c("yes","no"),4),
                            sick=c(rep("no",4),rep("yes",4)),
                            freq = c(80,24,31,23,120,22,4,0))
```
<!-- We can visualise the data in a mosaic plot. On the left is the plot of the data and on the right the plot we'd expect to see if sickness was independent of the two types of salad. In the latter we have the same proportion of people having each salad combination between those who got sick and those who didn't.  -->

<!-- ```{r} -->
<!-- fp0 <- data.frame(potato = factor(rep(c(rep("Potato", 2), rep("No potato", 2)), 2)), -->
<!--                       crab = factor(rep(c("Crab", "No crab"), 4)), -->
<!--                       sick = factor(c(rep("Not sick", 4), rep("Sick", 4))), -->
<!--                       freq = c(80, 24, 31, 23, 120, 22, 4, 0)) -->

<!-- par(mfrow= c(1,2), oma = c(0,0,0,0) + 0.1, mar = c(0,0,2,0.5) + 0.1) -->
<!-- plot(xtabs(freq ~ sick + potato + crab, fp0), xlab="", ylab="",  -->
<!--      main = "Food poisoning data") -->

<!-- ind <- glm (freq ~ sick + potato*crab, poisson, fp0) -->
<!-- plot(xtabs(fitted(ind) ~ sick + potato+crab, fp0), xlab="", ylab="",  -->
<!--      main = "Independence model") -->
<!-- ``` -->

Because the column totals in the contingency table are fixed (they are determined by which salad(s) people had), the minimal model we can fit must include terms for `crab`, `potato` and the interaction between the two. We use the shortcut notation $[PC]$ for this model. The model fit is given below: 

```{r}
summary(glm(freq ~ crab*potato, family=poisson, data=fp))
```

Note that we won't focus on the regression coefficients themselves, but rather on which terms are included in the model. 

To explore any relationship with food poisoning, we need to include terms involving `sick`. The independence model $[PC, S]$, which assumes that neither the potato salad, nor the crab salad had anything to do with the food poisoning, can be fit by using:

```{r}
glm(freq ~ crab*potato+sick, family=poisson, data=fp)
```

What we want to keep from this model is the residual deviance, to be used for comparisons with other nested models:

```{r}
deviance(glm(freq ~ crab*potato+sick, family=poisson, data=fp))
```


Model $[PC, SC]$ assumes that only the crab salad is associated with food poisoning:

```{r}
deviance(glm(freq ~ crab*potato+sick*crab, family=poisson, data=fp))
```

Model $[PC, SP]$ assumes that only the potato salad is associated with food poisoning:

```{r}
l0 <-glm(freq ~ crab*potato+sick*potato, family=poisson, data=fp)

deviance(l0)
```

Model $[PC, SP, SC]$ assumes that both the potato salad and the crab salad are associated with food poisoning:

```{r}
l1 <- glm(freq ~ crab*potato + crab*sick+potato*sick, family=poisson, data=fp)
summary(l1)
```

We see that the interaction term between `sick` and `crab` has a $p$-value of just above 0.05, suggesting that the relationship between crab salad and sickness is marginally significant. The potato salad has a much smaller $p$-value and looks like the most likely source of the outbreak, although there is some indication that the crab salad may have something to do with it, too.


<!-- We can compare models $[PC, SP]$ and $[PC, SP, SC]$ using the difference in deviance as follows: -->

<!-- ```{r} -->
<!-- anova(l0,l1) -->
<!-- ``` -->


In terms of how well the two models fit the data, we can look at their fitted values in comparison to the observed data:

```{r}
cbind(fp$freq, round(fitted(l0),2), round(fitted(l1),2))
```


<!-- Finally the saturated (full) model $[SPC]$ can be fit using -->
<!-- ```{r, results='hide'} -->
<!-- summary(glm(freq ~ sick*crab*potato, family=poisson, data=fp)) -->
<!-- ``` -->

In general, we can tabulate the deviances and degrees of freedom from each of the possible models.

\begin{center}
\begin{tabular}{llr}
\hline
Terms in model	& Deviance & DF\\
\hline
$[PC]$ & $63.669$ & 4\\
$[PC,S]$ & $63.196$& 3\\
$[PC,SC]$ & $53.683$ & 2\\
$[PC,SP]$ & $6.482$ & 2\\
$[PC,SP,SC]$ & $2.743$ & 1\\
$[PCS]$ & $4.123\times 10^{-10}$ & 0\\
\hline
\end{tabular}
\end{center}

We see that the models in the first three rows have relatively large deviance indicating a poor fit, but that models $[PC,SP]$ and $[PC,SP,SC]$ fit the data well. (Notice though that in both models there are some small fitted values, so we shouldn't overinterpret the deviance as a measure of goodness of fit.) $[PCS]$ is the saturated model so it fits the data perfectly -- notice the zero deviance.

The comparison between models $[PC,SP]$ and $[PC,SP,SC]$ is based on a $\chi^2(1)$ distribution:

```{r}
6.482-2.743

qchisq(df=1,p=0.95)
```

This is the same test that is performed when using

```{r}
anova(l0,l1)
```
###[/example]

###[task]

When one of the binary variables in a contingency table can be thought of as a response, it is possible to use a logistic regression model to test for association between this response and the other factors in the contingency table. Try this with the food poisoning data, by fitting logistic regression models corresponding to the loglinear models [PC,SP] and [PC,SP,SC]. Confirm that you reach the same conclusions and get the same fitted values.

####[answer]
First read in the data in a slightly different format:
```{r}
fp2 <- data.frame(sick=c(120,22,4,0), not.sick=c(80,24,31,23), 
                    potato=c("yes","yes","no","no"),crab= c("yes","no","yes","no"))
fp2
```

This is the logistic regression model corresponding to $[PC,SP]$:
```{r}
s0 <- glm(cbind(sick,not.sick) ~ potato, data=fp2, family=binomial)
summary(s0)
```

This is the logistic regression model corresponding to $[PC,SP,SC]$:

```{r}
s1 <- glm(cbind(sick,not.sick) ~ potato+crab, data=fp2, family=binomial)
summary(s1)
```
The deviance comparison between the two is the same as the comparison between the corresponding nonlinear models:

```{r}
anova(s0,s1)
```

The fitted values are the same as those of the corresponding loglinear models:

```{r}
cbind(c(fp2$sick, fp2$not.sick), round(fitted(l0),2), round(fitted(l1),2))
```

####[/answer]

###[/task]


## Simpson's paradox

Simpson's paradox is a phenomenon in which associations get reversed when we look at aggregates. A very simple example of this is the following:

Suppose that I get better grades than you in easy courses and I also get better grades than you in hard courses, and yet your GPA is higher than mine. How can this be? 

I take lots of hard courses where I get mostly Cs and you get mostly Ds, and you take lots of easy courses where you get mostly Bs and I get mostly As. Even though I do better than you when we control for the difficulty of the course, your overall GPA will be higher.

Here is another example with a contingency table.

###[example] Death penalty in the US

This dataset comes from \emph{Categorical Data Analysis} by Alan Agresti. The $2 \times 2 \times 2$ table shows homicide cases in Florida over the period 1976-77. The defendant's race and victim's race, each having categories white or black, and whether there was a death penalty verdict (yes/no), was recorded. 

\begin{center}
\begin{tabular}{ccccc}
  \hline
  Defendant's & Victim's & \multicolumn{2}{c}{Death Penalty} & Percentage\\
  Race & Race & Yes & No & Yes\\
  \hline
  White & White & 19 & 132 & 12.6\\
   & Black & 0 & 9 & 0\\
   Black & White & 11 & 52 & 17.5\\
   & Black & 6 & 97 & 5.8\\
   \hline
\end{tabular}
\end{center}

The following is the marginal table obtained by summing the cell counts over the level's of victim's race.

\begin{center}
\begin{tabular}{ccccc}
  \hline
 &  \multicolumn{2}{c}{Death Penalty}&  & Percentage\\
   Defendant's Race & Yes & No & Total & Yes\\
  \hline
  White & 19 & 141 & 160 & 11.9\\
   Black & 17 & 149 & 166& 10.2\\
  Total & 36 & 290 & 326 & \\
   \hline
\end{tabular}
\end{center}

About 12\% of white defendants and about 10\% of black defendants received the death penalty. Ignoring the victim's race, the percentage of "yes" death penalty verdicts was lower for blacks than for whites.

However taking the victim's race into account, things look completely different: When the victim was white, the death penalty was imposed about 5 percentage points more often for black defendants than for white defendants. When the victim was black, the death penalty was imposed over 5 percentage points more often for black defendants than for white defendants. Controlling for the victim's race, the percentage of "yes" death penalty verdicts was higher for blacks than for whites.

The phenomenon in which a pair of variables have marginal association of different direction from their partial associations is called *Simpson's paradox*. In the death penalty example, it arises because whites tended to kill whites, and killing a white person was more likely to result in the death penalty.
###[/example]

###[task]

Fit a suitable loglinear model to the death penalty data and interpret it in terms of the associations between the variables.

####[answer]

Let $P$ stand for death penalty, $V$ for victim's race and $D$ for defendant's race. Create a data.frame with the data:
```{r}
deathpenalty <- data.frame (D=rep(c("white","white","black","black"),2),
                            V=rep(c("white","black"),4),
                            P=c(rep("yes",4),rep("no",4)),
                            freq = c(19,0,11,6,132,9,52,97))

xtabs(freq ~ D + V+ P, data=deathpenalty)
```

Start from fitting
```{r}
glm(freq ~ D + V+ P, family=poisson, data=deathpenalty) #[D,V,P]
```
and try all models up to the saturated model

```{r}
glm(freq ~ D*V*P, family=poisson, data=deathpenalty) #[DVP]
```

A summary of all possible models is given in the following table:
\begin{center}
\begin{tabular}{lcr}
  \hline
 Terms in the model & DF & Deviance \\
  \hline
 $[D,V,P]$ & 4 & 137.93\\
$[D,VP]$ & 3 & 131.68\\
$[V,DP]$ & 3 & 137.71\\
$[P,DV]$ & 3 & 8.13\\
$[DP,VP]$  & 2& 131.46\\
$[DP,DV]$ & 2 & 7.91\\
$[VP,DV]$ & 2 & 1.88\\
$[DP,VP,DV]$ & 1 & 0.70\\
$[DPV]$ & 0 & 0\\
   \hline
\end{tabular}
\end{center}

Notice that any model that does not include the term `D*V` has a large deviance. This suggests an important association between the defendant's race and the victim's race. Other than the saturated model, the two models that appear to fit the data well when we compare the deviance with a chi-squared distribution with the corresponding degrees of freedom, are $[VP,DV]$  and $[DP,VP,DV]$. The simpler model says that the death penalty verdict is independent of the defendant's race, given the victim's race. In the model with all two-way interactions, all pairs of variables are conditionally dependent.
####[/answer]

###[/task]

## Additional resources on count regression and loglinear models

###[weblink,target="", icon=book]

[**Extending linear models with R: generalized linear, mixed effects and nonparametric regression models by Julian Faraway**](http://encore.lib.gla.ac.uk/iii/encore/record/C__Rb2939999):

* Chapter 3 has more details and examples of count regression (Poisson, quasi-Poisson and negative binomial models).

* Chapter 4 has information on contingency tables and loglinear models.

* Chapter 6 discusses GLM diagnostics including residuals and goodness of fit statistics.

R examples on the following topics are available from UCLA's Institute for Digital Research and Education:

* 
[Poisson regression](https://stats.idre.ucla.edu/r/dae/poisson-regression/)

* 
[Negative binomial regression](https://stats.idre.ucla.edu/r/dae/negative-binomial-regression/)

* 
[Zero-inflated Poisson regression](https://stats.idre.ucla.edu/r/dae/zip/)

* 
[Zero-inflated negative binomial regression](https://stats.idre.ucla.edu/r/dae/zinb/)

###[/weblink]


## Week 5 learning outcomes

* Be able to select an appropriate model for count responses (count regression for rates, loglinear model for contingency tables)

* Fit Poisson regression models for count data, using an offset when appropriate

* Fit negative binomial models for count responses using the `glm.nb()` function in `library(MASS)`, using an offset when appropriate

* Interpret Poisson and negative binomial model coefficients in terms of rate ratios

* Obtain fitted values and residuals from a Poisson regression or a negative binomial model

* Use deviance and Pearson residuals for diagnostic plots, recognising the limitations of these residuals

* Use the deviance and Pearson $X^2$ goodness of fit statistics to detect lack of fit in a GLM for counts, remembering that these tests only apply when the fitted values are large

* Recognise the signs of overdispersion in a Poisson model fit and be able to adjust the model output using a quasi-Poisson approach

* Identify the presence of excess zeros in a count regression and be able to explore a zero-inflated or hurdle model as an alternative to Poisson/negative binomial regression

* Choose between nested models by comparing deviances and between non-nested models by comparing AIC (or similar)

* Use loglinear models for contingency tables and be able to test hypotheses about independence between variables

* Recognise the relationship between loglinear models and logistic regression when one of the categorical variables in the contingency table is binary and can be thought of as a response

* Be familiar with Simpson's paradox: the direction of association can change if we aggregate over one of the variables in a three-way (or multi-way) contingency table