```{r setup, include=FALSE}
knitr::opts_chunk$set(comment=NA, warning = FALSE, message = FALSE)
library(ggplot2)
library(GGally)
library(gridExtra)

# transparent theme
Rmkd_theme <- theme_light()+
        theme(panel.background = element_rect(fill = "transparent", colour = NA),
              plot.background = element_rect(fill = "transparent", colour = NA),
              panel.border = element_rect(fill = NA, colour = "black", size = 1),
              legend.background = element_rect(fill = "transparent", colour = NA))
theme_set(Rmkd_theme)

g_legend <- function(a.gplot){
  tmp <- ggplot_gtable(ggplot_build(a.gplot))
  leg <- which(sapply(tmp$grobs, function(x) x$name) == "guide-box")
  legend <- tmp$grobs[[leg]]
  return(legend)}

```

Last week we described how to remove trend and seasonal variation from time series data, leaving a stationary residual series with short-term correlation. This week we will introduce the two main classes of time series process for stationary time series data, which differ in their short-term correlation structures. 

<!-- We will then introduce models which are applicable when you meet data that are not well represented by either of these time series processes. -->

We begin with defining an autoregressive (AR) process, and introducing the partial autocorrelation function (PACF). We then proceed to define a moving average (MA) process and describe its properties and how to identify it. 

<!-- We also define an autoregressive moving average (ARMA) process and describe how to decide if it is appropriate. Finally we introduce autoregressive integrated moving average (ARIMA) processes and discuss when it is appropriate to use them for modelling time series data. -->

# Autoregressive processes

In [Week 6](https://moodle.gla.ac.uk/pluginfile.php/1951085/mod_resource/content/2/week6.pdf), we described how to remove trend and seasonal variation from time series data, leaving a stationary residual series with short-term correlation. Now we introduce an autoregressive process, and for this we assume that the time series being modelled is stationary, which can be achieved by removing the trend and working with the residual series.

##[video,videoid="bS1y4GUvBSY", duration="11m55s"] Autoregressive processes
 
###[definition] Autoregressive process of order $p$
Let $Z_{t}$ be a purely random process (i.e. each $Z_{t}$ is independent) with mean 0 and variance $\sigma^{2}_{z}$.  An **autoregressive process of order $p$**, denoted AR($p$), is given by

$$X_{t} = \alpha_{1} X_{t-1} + \dots + \alpha_{p} X_{t-p} + Z_{t}$$

where $Z_{t}$ is a purely random process with mean zero and variance $\sigma^{2}_{z}$ and we assume that $X_{0} = X_{-1} = \dots = X_{-p} = 0$.
###[/definition]

In this model correlation is introduced between the random variables by regressing $X_{t}$ on past values $X_{t-1}, \dots , X_{t-p}$. The parameters $\alpha_{1}, \dots , \alpha_{p}$ are the coefficients of the autoregressive process, where $\alpha_{1}$ is called the lag 1 coefficient, $\alpha_{2}$ is called the lag 2 coefficient and so on.

This model essentially regresses $X_{t}$ on its own past values, which is the reason for the prefix *auto*. We will describe the properties of autoregressive processes, discuss how to choose the order $p$ when modelling real data, and how to estimate the coefficients $\alpha_{1},\ldots,\alpha_{p},\sigma^{2}_{z}$. For simplicity, we begin by describing the properties of a first order process.
  
## AR(1) process

An AR(1) process is given by

$$X_{t}=\alpha X_{t-1}+Z_{t}$$

where $Z_{t}$ is a purely random process with mean zero and variance $\sigma_{z}^{2}$ and $X_{0}=0$.

We now derive its mean, variance and correlation structure, which will tell us the conditions under which the process is stationary.

### Mean of AR(1) process

To calculate the mean of the process we need to re-write it as an infinite sum of a purely random process, using back substitution as follows.

\begin{eqnarray}
X_{t}&=&\alpha X_{t-1}+Z_{t}\nonumber\\
&=&\alpha(\alpha X_{t-2}+Z_{t-1}) + Z_{t}\nonumber\\
&=& \alpha^{2}X_{t-2} + \alpha Z_{t-1} + Z_{t}\nonumber\\
&=& \alpha^{2}(\alpha X_{t-3}+Z_{t-2}) + \alpha Z_{t-1} + Z_{t}\nonumber\\
&=& \alpha^{3}X_{t-3}+\alpha^{2}Z_{t-2} + \alpha Z_{t-1} + Z_{t}\nonumber\\
&=&\vdots\nonumber\\
&=&\sum_{j=0}^{\infty}\alpha^{j}Z_{t-j}\nonumber
\end{eqnarray}

Then the mean is given by

\begin{eqnarray}
E(X_{t})&=&E(\sum_{j=0}^{\infty}\alpha^{j}Z_{t-j})\nonumber\\
&=&\sum_{j=0}^{\infty}\alpha^{j}E(Z_{t-j})\nonumber\\
&=&\sum_{j=0}^{\infty}\alpha^{j}\times0\nonumber\\
&=&0\nonumber
\end{eqnarray}

So regardless of the value of the lag one coefficient $\alpha$, an AR(1) process has mean zero.

### Variance of AR(1) process

The variance can also be calculated by writing $X_{t}$ as an infinite sum of a purely random process.

\begin{eqnarray}
\mathrm{Var}(X_{t})&=&\mathrm{Var}(\sum_{j=0}^{\infty}\alpha^{j}Z_{t-j})\nonumber\\
&=&\sum_{j=0}^{\infty}\mathrm{Var}(\alpha^{j}Z_{t-j})\nonumber\\
&=&\sum_{j=0}^{\infty}\alpha^{2j}\mathrm{Var}(Z_{t-j})\nonumber\\
&=&\sum_{j=0}^{\infty}\alpha^{2j}\times\sigma_{z}^{2}\nonumber\\
&=&\sigma_{z}^{2}\sum_{j=0}^{\infty}\alpha^{2j}\nonumber
\end{eqnarray}

There are no covariance terms in the above expression because each $Z_{t}$ is independent as it is a purely random process. The variance is obtained from an infinite sum, so its value depends on $\alpha$. Two cases are possible.

1. If  $|\alpha|\geq1$ then $\mathrm{Var}(X_{t})=\infty$.

2. If $|\alpha|<1$ then $\alpha^{2}<1$, and we know that for a geometric progression

$$\sum_{j=0}^{\infty}\alpha^{2j}=1+\alpha^{2}+\alpha^{4}+\alpha^{6}+\ldots~=~\frac{1}{1-\alpha^{2}}$$


so we have

$$\mathrm{Var}(X_{t})=\sigma_{z}^{2}\sum_{j=0}^{\infty}\alpha^{2j}=\frac{\sigma_{z}^{2}}{1-\alpha^{2}}$$

### Notes

1. As a weakly stationary process must have a finite constant variance, an AR(1) process is not stationary if $|\alpha|\geq 1$.

2. To prove that an AR(1) process is stationary for $|\alpha|<1$, we need to check the autocorrelation function does not depend on $t$.


### Autocorrelation function of AR(1) process

The autocovariance function at lag $\tau$ is calculated as follows.


\begin{eqnarray*}
\mathrm{Cov}(X_t,X_{t+\tau}) &=& E(X_tX_{t+\tau})-E(X_{t})E(X_{t+\tau})\\
    &=& E(X_{t}X_{t+\tau}) \\
    &=&E\left[\sum_{j=0}^{\infty}\left(\alpha^{j}Z_{t-j}\right)\times\sum_{k=0}^{\infty}\left(\alpha^{k}Z_{t-k+\tau}\right)\right]\\
    &=&\sum_{j=0}^{\infty}\sum_{k=0}^{\infty}\alpha^{j}\alpha^{k}E\left(Z_{t-j}Z_{t-k+\tau}\right)
\end{eqnarray*}


Multiplying out the above sum will give pairs $E(Z_{r}Z_{s})$ (excluding constants).


*  For any $r\neq s$ we have  $E(Z_{r}Z_{s})=E(Z_{r})E(Z_{s})=0$ because the $Z_{t}$ are independent.

* For $r=s$ we have $E(Z_{r}^{2})=\mathrm{Var}(Z_{r})+[E(Z_{r})]^{2}=\sigma^{2}_{z}$.


Therefore the only pairs of terms that remain are when $j=k+\tau$, simplifying the sum to


\begin{eqnarray}
\mathrm{Cov}(X_{t},X_{t+\tau})&=&\sigma^{2}_{z}\sum_{j=0}^{\infty}\alpha^{j}\alpha^{j+\tau}\nonumber\\
&=&\sigma^{2}_{z}\alpha^{\tau}\sum_{j=0}^{\infty}\alpha^{2j}\nonumber
\end{eqnarray}


Therefore the covariance is also infinite for $|\alpha|\geq 1$. For $|\alpha|<1$ the covariance is given by

$$\gamma_{\tau}=\mathrm{Cov}(X_{t},X_{t+\tau})=\frac{\sigma^{2}_{z}\alpha^{\tau}}{1-\alpha^{2}}$$

The correlation function is therefore given by

$$\rho_{\tau}=\mathrm{Corr}(X_{t},X_{t+\tau})=\frac{\gamma_{\tau}}{\gamma_{0}}=
\frac{\sigma^{2}_{z}\alpha^{\tau}/(1-\alpha^{2})}{\sigma_{z}^{2}/(1-\alpha^{2})}=\alpha^{|\tau|}$$

where the modulus operator is required because $\gamma_{\tau}=\gamma_{-\tau}$. Therefore if $|\alpha|<1$ the mean and variance are constant and finite, and the autocorrelation function only depends on $\tau$. Hence the process is weakly stationary.

###[example]

Here are some time plots and correlograms of AR(1) processes with different values of $\alpha$. The processess are simulated and graphed using the following R code.

```{r echo=TRUE,  fig.align='center', fig.width=5, fig.height=6}

data1 <- arima.sim(model=list(ar=-0.8), n=100, sd=10)
data2 <- arima.sim(model=list(ar=0.01), n=100, sd=10)
data3 <- arima.sim(model=list(ar=0.4), n=100, sd=10)
data4 <- arima.sim(model=list(ar=0.8), n=100, sd=10)

par(mfrow=c(4,2), mar=c(3,2,2,1))
plot(data1, main="alpha=-0.8")
acf(data1, main =" ")
plot(data2, main="alpha=0.01")
acf(data2, main =" ")
plot(data3, main="alpha=0.4")
acf(data3, main =" ")
plot(data4, main="alpha=0.8")
acf(data4, main =" ")
```

Alternatively, we can use `library(ggfortify)` to plot the time series using `ggplot` graphics:


```{r, fig.align='center', fig.width=6, fig.height=7}
library(ggfortify)
library(gridExtra)
p1 <- autoplot(data1, xlab = "Time", ylab = "Data1", main = "alpha=-0.8")
p1acf <- autoplot(acf(data1, plot = FALSE), main = "")

p2 <- autoplot(data2, xlab = "Time", ylab = "Data2", main = "alpha=0.01")
p2acf <- autoplot(acf(data2, plot = FALSE), main = "")

p3 <- autoplot(data3, xlab = "Time", ylab = "Data3", main = "alpha=0.4")
p3acf <- autoplot(acf(data3, plot = FALSE), main = "")

p4 <- autoplot(data4, xlab = "Time", ylab = "Data4", main = "alpha=0.8")
p4acf <- autoplot(acf(data4, plot = FALSE), main = "")

grid.arrange(p1, p1acf, p2, p2acf, p3, p3acf, p4, p4acf, nrow=4)
```
###[/example]

### Notes

1. A random walk process occurs when $\alpha=1$ and is hence not stationary.

2. A purely random process occurs when $\alpha=0$ and is hence stationary.

\pagebreak

###[supplement] Yule-Walker equations

Conditional on the mean and variance of the AR(1) process calculated above, we can derive the covariance in a shorter way assuming that the process is stationary.

\begin{eqnarray}
X_{t}&=&\alpha X_{t-1}+Z_{t}\nonumber\\
X_{t-k}X_{t}&=&\alpha X_{t-k}X_{t-1}+X_{t-k}Z_{t}\nonumber\\
E(X_{t-k}X_{t})&=&\alpha E(X_{t-k}X_{t-1})+E(X_{t-k}Z_{t})\nonumber\\
E(X_{t-k}X_{t})&=&\alpha E(X_{t-k}X_{t-1})\nonumber
\end{eqnarray}

Writing  $X_{t-k}=\sum_{j=0}^{\infty}\alpha^{j}Z_{t-k-j}$, the last line is true because

\begin{eqnarray}
E(X_{t-k}Z_{t})&=&E(\sum_{j=0}^{\infty}\alpha^{j}Z_{t-k-j}Z_{t})\nonumber\\
&=&\sum_{j=0}^{\infty}\alpha^{j}E(Z_{t-k-j}Z_{t})\nonumber\\
&=&0\nonumber
\end{eqnarray}

Furthermore, as $E(X_{t})=0$ then

$$\gamma_{-k}~=~E(X_{t-k}X_{t})-E(X_{t})E(X_{t-k})~=~E(X_{t-k}X_{t})$$

so that the above equation becomes

$$\gamma_{-k}=\alpha\gamma_{-k+1}\hspace{1cm}\mbox{and}\hspace{1cm}\rho_{-k}=\alpha\rho_{-k+1}$$

Finally, as both the autocovariance and autocorrelation functions are even, i.e. $\gamma_{-k}=\gamma_{k}$ we have

$$\gamma_{k}=\alpha\gamma_{k-1}\hspace{1cm}\mbox{and}\hspace{1cm}\rho_{k}=\alpha\rho_{k-1}$$

This recursive relationship is a special case of the **Yule-Walker equations**, named after G.U. Yule and G. Walker. The equations are one method used to calculate the autocorrelation function for a general AR($p$) model. The equations can be used to determine the autocorrelations by noting that $\rho_{0}=1$, which leads to $\rho_{1}=\alpha$ and so on.
###[/supplement]

## AR($p$) processes

An AR($p$) process is given by

$$X_{t}=\alpha_{1}X_{t-1}+\ldots+\alpha_{p}X_{t-p}+Z_{t}$$

where $Z_{t}$ is a purely random process with mean zero and variance $\sigma^{2}_{z}$. It can be written in the following alternative form:

\begin{eqnarray}
X_{t}&=&\alpha_{1}X_{t-1}+\ldots+\alpha_{p}X_{t-p}+Z_{t}\nonumber\\
X_{t}-\alpha_{1}X_{t-1}-\ldots-\alpha_{p}X_{t-p}&=&Z_{t}\nonumber\\
(1-\alpha_{1}B-\alpha_{2}B^{2}-\ldots-\alpha_{p}B^{p})X_{t}&=&Z_{t}\nonumber\\
\phi(B)X_{t}&=&Z_{t}\nonumber
\end{eqnarray}

where $B$ is the backshift operator. The function $\phi(B)=1-\alpha_{1}B-\alpha_{2}B^{2}-\ldots-\alpha_{p}B^{p}$ is called the **characteristic polynomial** and is important for determining whether the process is stationary.

## Mean of AR($p$) process

We can use the **invertibility theorem** to find the mean of an AR($p$) process. 

###[theorem] Invertibility

Any AR($p$) process can be written as an infinite sum of a purely random process

$X_{t} = \alpha_{1} X_{t-1} + \dots + \alpha_{p} X_{t-p} + Z_{t} = \sum_{j=0}^{\infty}{\beta_{j} Z_{t-j}}$.

###[/theorem]

Note that the relationship between the $\alpha$'s and $\beta$'s can be hard (algebraically messy) to find.

We can use the invertibility theorem to show that the mean of an autoregressive process is zero:

\begin{eqnarray}
E(X_{t})&=&E(\sum_{j=0}^{\infty}\beta_{j} Z_{t-j})\nonumber\\
&=&\sum_{j=0}^{\infty}\beta_{j} E(Z_{t-j})\nonumber\\
&=&0\nonumber
\end{eqnarray}

<!-- ###[task] -->
<!-- Find the mean of an AR(1) process by using back substitution. -->

<!-- ####[answer] -->
<!-- To calculate the mean of the process we need to re-write it as an infinite sum of a purely random process, using back substitution as follows. -->

<!-- \begin{eqnarray} -->
<!-- X_{t}&=&\alpha X_{t-1}+Z_{t}\nonumber\\ -->
<!-- &=&\alpha(\alpha X_{t-2}+Z_{t-1}) + Z_{t}\nonumber\\ -->
<!-- &=& \alpha^{2}X_{t-2} + \alpha Z_{t-1} + Z_{t}\nonumber\\ -->
<!-- &=& \alpha^{2}(\alpha X_{t-3}+Z_{t-2}) + \alpha Z_{t-1} + Z_{t}\nonumber\\ -->
<!-- &=& \alpha^{3}X_{t-3}+\alpha^{2}Z_{t-2} + \alpha Z_{t-1} + Z_{t}\nonumber\\ -->
<!-- &=&\vdots\nonumber\\ -->
<!-- &=&\sum_{j=0}^{\infty}\alpha^{j}Z_{t-j}\nonumber -->
<!-- \end{eqnarray} -->

<!-- Then the mean is given by -->

<!-- \begin{eqnarray} -->
<!-- E(X_{t})&=&E(\sum_{j=0}^{\infty}\alpha^{j}Z_{t-j})\nonumber\\ -->
<!-- &=&\sum_{j=0}^{\infty}\alpha^{j}E(Z_{t-j})\nonumber\\ -->
<!-- &=&\sum_{j=0}^{\infty}\alpha^{j}\times 0\nonumber\\ -->
<!-- &=&0\nonumber -->
<!-- \end{eqnarray} -->

<!-- So regardless of the value of the lag one coefficient $\alpha$, an AR(1) process has mean zero. -->
<!-- ####[/answer] -->
<!-- ###[/task] -->

## Stationarity for AR($p$) processes

The variance and autocorrelation function can be calculated by determining the $\beta_{j}$'s, although these are hard to calculate. Instead, we calculate the variance and autocorrelation function conditional on the process being stationary. The following theorem tells us when an AR($p$) process is stationary.

###[theorem] Stationarity 

Write an AR($p$) process as

$$\phi(B)X_{t}=Z_{t}$$

where $\phi(B)$ is the characteristic polynomial $\phi(B)=1-\alpha_{1}B-\alpha_{2}B^{2}-\ldots-\alpha_{p}B^{p}$. Then the process is stationary if the roots of the **characteristic equation**

$$\phi(B)=1-\alpha_{1}B-\alpha_{2}B^{2}-\ldots-\alpha_{p}B^{p}=0$$

have modulus greater than 1, i.e. they lie outside the unit circle. Here we consider $B$ as the variable of the polynomial equation.
###[/theorem]

###[example]
To work out for which values of $\alpha$ the AR(1) process $X_{t}=\alpha X_{t-1}+Z_{t}$ is stationary, we need to look at the roots of the characteristic polynomial. The characteristic polynomial is given by $\phi(B)=1-\alpha B$, which has a single root $1/\alpha$. Hence the process is stationary when $1/\alpha$ is greater than 1 in absolute value, i.e. when $|\alpha|<1$.
###[/example]

###[example]
The AR(2) process $X_{t}=X_{t-1}-0.5X_{t-2}+Z_{t}$ is stationary. The characteristic equation is given by $1-B+0.5B^{2}=0$. Using the quadratic formula gives

\begin{eqnarray}
\mbox{roots}&=&\frac{1\pm\sqrt{(-1)^2-4\times0.5\times1}}{2\times 0.5}\nonumber\\
\mbox{roots}&=&\frac{1\pm\sqrt{-1}}{1}\nonumber\\
\mbox{roots}&=&1+i\mbox{~and~}1-i\nonumber
\end{eqnarray}

where $i$ is the complex part of the number, i.e. $i=\sqrt{-1}$. Therefore as both roots have modulus greater than one the process is stationary.
###[/example]

###[task]
Is the AR(2) process $X_{t}=0.75X_{t-1}-0.125X_{t-2}+Z_{t}$ stationary?
####[answer]
The characteristic polynomial is given by

\begin{eqnarray}
X_{t}&=&0.75X_{t-1}-0.125X_{t-2}+Z_{t}\nonumber\\
X_{t}-0.75X_{t-1}+0.125X_{t-2}&=&Z_{t}\nonumber\\
(1-0.75B+0.125B^{2})X_{t}&=&Z_{t}\nonumber
\end{eqnarray}

So solving $1-0.75B+0.125B^{2}=0$ using the quadratic formula gives
\begin{eqnarray}
\mbox{roots}&=&\frac{0.75\pm\sqrt{(-0.75)^2-4\times0.125\times1}}{2\times 0.125}\nonumber\\
\mbox{roots}&=&\frac{0.75\pm0.25}{0.25}\nonumber\\
\mbox{roots}&=&2\mbox{~and~}4\nonumber
\end{eqnarray}

Therefore as both roots have modulus greater than one the process is stationary.
####[/answer]
###[/task]


## Variance of AR($p$) process

The variance of an AR($p$) process is only finite if the process is stationary. For a stationary AR($p$) process the variance is given by

$$\mathrm{Var}[X_{t}]=\sigma^{2}_{z}+\sum_{\tau=1}^{p}\alpha_{\tau}\gamma_{\tau}.$$
Note that the variance depends on the autocovariance functions at lags 1 to $p$.

###[supplement] Calculation of the variance of an AR($p$) process

Assuming the process is stationary, we calculate its variance as follows.
\begin{eqnarray}
X_{t}&=&\alpha_{1}X_{t-1}+\ldots+\alpha_{p}X_{t-p}+Z_{t}\nonumber\\
X_{t}^{2}&=&\alpha_{1}X_{t}X_{t-1}+\ldots+\alpha_{p}X_{t}X_{t-p}+X_{t}Z_{t}\nonumber\\
E(X_{t}^{2})&=&\alpha_{1}E(X_{t}X_{t-1})+\ldots+\alpha_{p}E(X_{t}X_{t-p})+E(X_{t}Z_{t})\nonumber\\
E(X_{t}^{2})&=&\alpha_{1}E(X_{t}X_{t-1})+\ldots+\alpha_{p}E(X_{t}X_{t-p})+\sigma^{2}_{z}\nonumber
\end{eqnarray}

The last line holds because

\begin{align*}
E(X_{t}Z_{t})&=E[(\alpha_{1}X_{t-1}+\dots+\alpha_{p}X_{t-p}+Z_{t})Z_{t}]\\
&=E[(\alpha_{1}X_{t-1}+\dots+\alpha_{p}X_{t-p})Z_{t}]+E(Z_{t}^{2})\\
&=E(\alpha_{1}X_{t-1}+\dots+\alpha_{p}X_{t-p})E(Z_{t})+\mathrm{Var}(Z_{t})+[E(Z_{t})]^2\\
&=\sigma^{2}_{z}
\end{align*}

Therefore as $E(X_{t})=0$ we have that

$\mathrm{Var}(X_{t})=E(X_{t}^{2}) - E(X_{t})^{2}=E(X_{t}^{2})$

and

$\gamma_{\tau}=\mathrm{Cov}(X_{t}, X_{t+\tau})= \mathrm{Cov}(X_{t}, X_{t-\tau})=E(X_{t}X_{t-\tau}) - E(X_{t}) E(X_{t-\tau})=E(X_{t}X_{t-\tau})$.

so that the variance simplifies to

$$\mathrm{Var}[X_{t}]=\sigma^{2}_{z}+\sum_{\tau=1}^{p}\alpha_{\tau}\gamma_{\tau}.$$
###[/supplement]


## Autocorrelation function of AR($p$) process

The autocovariance function is given by the recursive equation

$$\gamma_{\tau}=\alpha_{1}\gamma_{\tau-1}+\ldots+\alpha_{p}\gamma_{\tau-p}.$$

Dividing by the variance gives the autocorrelation function as

$$\rho_{\tau}=\alpha_{1}\rho_{\tau-1}+\ldots+\alpha_{p}\rho_{\tau-p}$$

which is called the **Yule-Walker** equation.

###[supplement] Calculation of the autocovariance and autocorrelation function of an AR($p$) process

The autocovariance and autocorrelation functions can be calculated in the same way as the variance.

\begin{eqnarray}
X_{t}&=&\alpha_{1}X_{t-1}+\ldots+\alpha_{p}X_{t-p}+Z_{t}\nonumber\\
X_{t}X_{t-\tau}&=&\alpha_{1}X_{t-\tau}X_{t-1}+\ldots+\alpha_{p}X_{t-\tau}X_{t-p}+X_{t-\tau}Z_{t}\nonumber\\
E(X_tX_{t-\tau})&=&\alpha_{1}E(X_{t-\tau}X_{t-1})+\ldots+\alpha_{p}E(X_{t-\tau}X_{t-p})+E(X_{t-\tau}Z_{t})\nonumber\\
E(X_tX_{t-\tau})&=&\alpha_{1}E(X_{t-\tau}X_{t-1})+\ldots+\alpha_{p}E(X_{t-\tau}X_{t-p})\nonumber
\end{eqnarray}

As $\gamma_{-\tau}=\gamma_{\tau}$, the autocovariance function is given by the recursive equation

$$\gamma_{\tau}=\alpha_{1}\gamma_{\tau-1}+\ldots+\alpha_{p}\gamma_{\tau-p}.$$


Dividing by the variance gives the autocorrelation function as

$$\rho_{\tau}=\alpha_{1}\rho_{\tau-1}+\ldots+\alpha_{p}\rho_{\tau-p}$$

which is called the **Yule-Walker** equation.

These equations can be written in matrix form for $\tau=1,2,\ldots,p$ as

\begin{gather*}
  \begin{bmatrix}
    1 & \rho_{1} & \cdots & \rho_{p-1} \\
    \rho_{1} & 1 & \cdots & \rho_{p-2} \\
    \vdots & \vdots &  & \vdots \\
    \rho_{p-1} & \rho_{p-2} & \cdots & 1
  \end{bmatrix}
  \begin{bmatrix}
    \alpha_1 \\
    \alpha_2 \\
    \vdots \\
    \alpha_p
  \end{bmatrix}
  =
  \begin{bmatrix}
   \rho_{1} \\
   \rho_{2} \\
   \vdots \\
   \rho_{p} \\
  \end{bmatrix}
\end{gather*}

**Note**: The Yule-Walker equations can be used repeatedly to calculate the autocorrelation function for any AR($p$) process. However, as $p$ increases the equations get more complex.

###[/supplement]

<!-- ###[task] -->
<!-- Calculate the autocorrelation function for the AR(2) process, $X_{t}=\alpha_{1}X_{t-1}+\alpha_{2}X_{t-2}+Z_{t}$. -->

<!-- ####[answer] -->
<!-- The Yule-Walker equation with $\tau=1$ gives $\rho_{1}=\alpha_{1}\rho_{0}+\alpha_{2}\rho_{-1}$ where as $\rho_{0}=1$ and $\rho_{-1}=\rho_{1}$ we get that -->

<!-- $$\rho_{1}=\frac{\alpha_{1}}{1-\alpha_{2}}$$ -->


<!-- Using the Yule-Walker equations again for $\tau=2$ gives -->

<!-- $$\rho_{2}=\alpha_{1}\rho_{1}+\alpha_{2}\rho_{0}=\frac{\alpha_{1}^{2}}{1-\alpha_{2}}+\alpha_{2}$$ -->


<!-- This can be repeated to give $\rho_{3}$, $\rho_{4}$ etc, but with increasing algebraic complexity. -->

<!-- ####[/answer] -->
<!-- ###[/task] -->


## Model identification

Given a stationary time series how do you determine:

(a) if an autoregressive process is an appropriate model, and

(b) if it is, which order process should you use (i.e. which value of $p$)?

The obvious solution would be to base your decision on the correlogram of the time series, but this does not help. Let us illustrate this with an example, by simulating AR(1) and AR(2) data.

###[task] 
Can you work out the equations of the AR models simulated using the following code?

####[answer]
For`data1` the `model` argument of `arima.sim()` takes the value `list(ar=0.6)`, suggesting that there is only one AR coefficient and it's equal to 0.6. This gives the model equation for `data1` as 
\begin{eqnarray}
X_{t}&=&0.6X_{t-1}+Z_{t} \nonumber
\end{eqnarray}
Similarly for `data2` we have two AR coefficients, and the model equation is given by 
\begin{eqnarray}
X_{t}&=&0.75X_{t-1}-0.125X_{t-2}+Z_{t}\nonumber
\end{eqnarray}
####[/answer]
###[/task]

```{r, fig.align='center', fig.width=7, fig.height=4.5}
data1 <- arima.sim(model=list(ar=0.6), n=100, sd=10)
data2 <- arima.sim(model=list(ar=c(0.75, -0.125)), n=100, sd=10)

p1 <- autoplot(data1, xlab = "Time", ylab = "Data1", main = "AR(1) data")
p1acf <- autoplot(acf(data1, plot = FALSE), main = "")
p2 <- autoplot(data2, xlab = "Time", ylab = "Data2", main = "AR(2) data")
p2acf <- autoplot(acf(data2, plot = FALSE), main = "")

grid.arrange(p1, p1acf, p2, p2acf, nrow=2)
```

The correlograms for the AR(1) and AR(2) process look very similar, so the correlogram is not an appropriate tool for choosing $p$.

A second approach to choosing $p$ is to start by fitting an AR(1) process because it is the simplest, and look at the correlogram of the residuals. If the residuals resemble a purely random process, then the chosen model is adequate. If there is still residual correlation you could increase $p$ by one (to an AR(2) model) and re-fit the model, again checking for the presence of residual correlation. However, this approach assumes that an autoregressive process of some order is an appropriate model. What happens if it isn't? Therefore we need an alternative approach.


###[definition] Partial autocorrelation function

The **partial autocorrelation function (PACF)** at lag $\tau$ is equal to the estimated lag $\tau$ coefficient $\hat{\alpha}_{\tau}$, obtained when fitting an AR($\tau$) model to a data set. It is denoted by $\pi_{\tau}$, and represents the excess correlation in the time series that has not been accounted for by the $\tau-1$ smaller lags.
###[/definition]


### Notes

The partial autocorrelation function at lag $\tau$ is calculated by fitting an AR($\tau$) process to the data. For example:

* Fit the AR(1) model $X_{t}=\alpha_{1}X_{t-1}+Z_{t}$ to the time series; then $\hat{\pi}_{1}=\hat{\alpha}_{1}$.

* Fit the AR(2) model $X_{t}=\alpha_{1}X_{t-1}+\alpha_{2}X_{t-2}+Z_{t}$ to the time series; then $\hat{\pi}_{2}=\hat{\alpha}_{2}$.

* Fit the AR(3) model $X_{t}=\alpha_{1}X_{t-1}+\alpha_{2}X_{t-2}+\alpha_{3}X_{t-3}+Z_{t}$ to the time series, then $\hat{\pi}_{3}=\hat{\alpha}_{3}$, and so on.

The partial autocorrelation coefficients $\hat{\alpha}_{\tau}$ from fitting an AR($\tau$) model can be estimated using a number of methods.

- The partial autocorrelation function is not defined for lag zero, unlike the autocorrelation function.

The partial autocorrelation function allows us to determine an appropriate AR($p$) process for a given data set. This is because if the data do come from an AR($p$) process then

$$\pi_{\tau}~=~\left\{\begin{array}{cc}\mbox{non-zero}&\tau\leq p\\0&\tau>p\end{array}\right.$$

The justification for this is as follows.

1.  Consider an AR(1) process, $X_{t}=\alpha_{1}X_{t-1}+Z_{t}$. Then

    *  $\hat{\pi}_{1}$ is estimated from fitting an AR(1) model so $\hat{\pi}_{1}=\hat{\alpha}_{1}\approx\alpha_{1}$, which is not zero as this is an AR(1) process.

    *  for any $\tau>1$, $\hat{\pi}_{\tau}$ is estimated from fitting an AR($\tau$) model so $\hat{\pi}_{\tau}=\hat{\alpha}_{\tau}\approx\alpha_{\tau}=0$, as it is an AR(1) process.


2. Consider an AR(2) process, $X_{t}=\alpha_{1}X_{t-1}+\alpha_{2}X_{t-2}+Z_{t}$. Then

    * $\hat{\pi}_{1}$ is estimated from fitting an AR(1) model so $\hat{\pi}_{1}=\hat{\alpha}_{1}\approx\alpha_{1}$ (non-zero), as it is an AR(2) process.

    * $\hat{\pi}_{2}$ is estimated from fitting an AR(2) model so $\hat{\pi}_{2}=\hat{\alpha}_{2}\approx\alpha_{2}$ (non-zero), as it is an AR(2) process.

    * for any $\tau>2$, $\hat{\pi}_{\tau}$ is estimated from fitting an AR($\tau$) model so $\hat{\pi}_{\tau}=\hat{\alpha}_{\tau}\approx\alpha_{\tau}=0$, as it is an AR(2) process.

The same arguments hold for AR($p$) processes for $p>2$.


It can also be shown that for an AR($p$) process, for any lag $\tau>p$

$$\pi_{\tau}\sim\mbox{N}\left(0, \frac{1}{n}\right)$$

which gives an approximate 95$\%$ confidence interval for $\pi_{\tau}$ of $\pm1.96/\sqrt{n}$, when the true value $\pi_{\tau}=0$. Therefore to choose the order of an AR($p$) process, plot the partial autocorrelation function and choose $p$ as the smallest value that is significantly different from zero.

\pagebreak
###[task]
From the PACFs below, determine whether an AR($p$) process would be an appropriate, and if so what value of $p$ you would use.

```{r echo=FALSE,  fig.align='center', out.width='100%'}
knitr::include_graphics('PACFs.pdf')
```

####[answer]

For panel (a) an AR(2) process looks appropriate.

For panel (b), if anything an AR(2) process, but the significant correlations at lags 4 and 7 are slightly concerning. Actually this data were an MA(1) process.
    
For panel (c), an AR(1) process looks appropriate.

In panel (d) there is no evidence of short-term correlation, so a purely random process is appropriate.
####[/answer]
###[/task]


###[supplement] Parameter estimation

Parameter estimation is not generally done by hand so there is no need to learn how to carry out parameter estimation in this course. However, it is useful to be aware of what the methods are. The main two methods are:

1. **The autocorrelation method (Yule-Walker estimation)** uses the Yule-Walker recursive equations and replaces the theoretical autocorrelation function by its sample estimates. For the variance, we use the form of the variance for an AR($p$) process, again, replacing the theoretical quantities by their sample estimates.

2. **Ordinary least squares estimation** estimates the $\alpha$'s by minimising the ordinary least squares objective function and the residual variance is estimated from the residuals.

**Notes**:

- Ordinary least squares (which assumes independence) is used because the correlation in the time series is modelled by the AR($p$) process, hence the errors $Z_{t}$ are independent.

- When the data are normally distributed the ordinary least squares estimates are also the maximum likelihood estimates.

- The Yule-Walker method does not produce accurate estimates when *n* is small (less than 50) and the model is close to being non-stationary.

- There are many other methods of estimation for time series. R uses either maximum likelihood or least squares.

###[/supplement]

###[example]

Consider the data set generated using the following R code.

```{r, fig.align='center', fig.width=6, fig.height=4.5}

#### Generate AR(2) data with a linear trend
time <- 1:200
data.ar <- arima.sim(model=list(ar=c(0.6, 0.3)), n=200, sd=1)
data <- data.ar + 30 + 0.05 * time

## Plot the correlogram and time plot
p1 <- autoplot(data, xlab = "Time", ylab = "Data1", main = "")
p1acf <- autoplot(acf(data, plot = FALSE), main = "")

grid.arrange(p1, p1acf, nrow=2)
```

From looking at the time plot and correlogram the data appear to have a linear trend, which we remove before trying to model the correlation. We remove the trend using regression methods (see Week 6 for details), i.e. by fitting $m_{t}=\beta_{0}+\beta_{1}t$ as shown below, and obtain the following residual series.


```{r, fig.width=6, fig.height=6.5, fig.align='center'}
## Remove the trend
linear.model <- lm(data~time)
summary(linear.model)
residual.series <- data - linear.model$fitted.values

## Plotting
p1 <- autoplot(residual.series, xlab = "Time", ylab = "Residual series")
p1acf <- autoplot(acf(residual.series, plot = FALSE), 
                  main = "")
p1pacf<- autoplot(pacf(residual.series, plot = FALSE), 
                  main = "")
grid.arrange(p1, p1acf, p1pacf, nrow=3)

```

The time plot suggests the residual series is stationary, while the partial autocorrelation function suggests that an AR(2) process is appropriate. Note that the autocorrelation function does not really tell us a great deal here. Finally we fit an AR(2) model to the data using the R function `arima()`, and plot the residuals to ensure they are independent. The R code for this is as follows:


```{r, fig.width=6, fig.height=6.5, fig.align='center'}
## Fit an AR(2) model to the data
model.ar <- arima(residual.series, order=c(2,0,0))
model.ar

## Plotting
p2 <- autoplot(model.ar$residuals, xlab = "Time",  ylab = "Residual series")
p2acf <- autoplot(acf(model.ar$residuals, plot = FALSE), 
                  main = "")
p2pacf<- autoplot(pacf(model.ar$residuals, plot = FALSE), 
                  main = "s")
grid.arrange(p2, p2acf, p2pacf, nrow=3)
```

The residuals look independent, so the model we fitted is appropriate.
###[/example]

\pagebreak

###[example] Temperature data in a Scottish loch

The Centre for Ecology and Hydrology (CEH) in Edinburgh routinely measure the temperature in a number of Scottish lochs (lakes). The data below are monthly temperature levels in a particular loch between 1997 and 2002. The plot below shows the temperature levels exhibit little trend but prominent seasonal variation, with higher values in the summer than in the winter.

```{r, fig.width=6, fig.height=4.3, fig.align='center'}
data <- read.csv(url("http://www.stats.gla.ac.uk/~tereza/rp/lochdata.csv"))
n <- nrow(data)
time <- 1:n
x <- data[,3]

## Plotting time series and ACF
p1 <- ggplot(data, aes(time, temperature))+ geom_line(color = "#4d4d4d")+  
    xlab("Date") + ylab("Temperature (degrees Celsius)") + 
    ggtitle("Loch temperature data 1997-2002")

p1acf <- autoplot(acf(data$temperature, plot = FALSE),main = "")

grid.arrange(p1, p1acf, nrow=2)
```

Given that the seasonal pattern seems regular (the same each year), an appropriate model seems to be


$$s_{t}=\beta_{0}+\beta_{1}\sin(2\pi t/12) + \beta_{2}\cos(2\pi t/12)$$

where we divide by 12 because there are 12 periods (i.e. months) before the seasonal pattern repeats itself. We fit the seasonal model as shown below and obtain the following residual series.

```{r, fig.width=6, fig.height=6.2, fig.align='center'}
model.season <- lm(x~sin(2*pi*time/12) + cos(2*pi*time/12))
residual.series <- x - model.season$fitted.values

# Plotting
p2 <- autoplot(ts(residual.series), ylab = "Residual series")
p2acf <- autoplot(acf(residual.series, plot = FALSE),
                   main = "")
p2pacf <- autoplot(pacf(residual.series, plot = FALSE),
                   main = "", ylab="PACF")

grid.arrange(p2, p2acf, p2pacf, nrow=3)
```


The time plot suggests that the residual series is stationary, while the partial autocorrelation function suggests that an AR(1) process is appropriate. We fit the AR(1) model as shown below and obtain the following residuals.


```{r, fig.width=6, fig.height=6.2, fig.align='center'}
model.ar <- arima(residual.series, order=c(1,0,0))

# Plotting
p3 <- autoplot(model.ar$residuals, ylab = "AR(1) residual series")
p3acf <- autoplot(acf(model.ar$residuals, plot = FALSE),
                   main = "")
p3pacf <- autoplot(pacf(model.ar$residuals, plot = FALSE),
                   main = "", ylab="PACF")

grid.arrange(p3, p3acf, p3pacf, nrow=3)
```

The residuals look independent, so the model we fitted is appropriate.
###[/example]


###[task]

The dataset `glasgowtemp.csv` available from [this link](http://www.stats.gla.ac.uk/~tereza/rp/glasgowtemp.csv) contains the daily minimum temperature levels in Glasgow between 2005 and 2007.

Produce descriptive plots of the data, remove any trend or seasonal variation by fitting an appropriate model and check the residual series for the presence of short-term correlation.

Then proceed to identify an appropriate process to model short-term correlation. Fit the chosen model to the data using R, and analyse the residuals to ensure the chosen model is appropriate.


####[answer]
First let us read in the data:

```{r, fig.width=6, fig.height=6.5, fig.align='center'}

temp <- read.csv(url("http://www.stats.gla.ac.uk/~tereza/rp/glasgowtemp.csv"))
dim(temp)
head(temp)

# Define date variable and plot data
temp$Date2 <- as.Date(temp$Date, format="%d/%m/%Y" )

p1 <- ggplot(temp, aes(Date2, Temperature)) +
    geom_jitter(aes(colour = Temperature), alpha = 0.5) +
    xlab(" ") + scale_x_date(date_labels = "%b-%Y", date_breaks = "6 month") +
    ylab("Temperature (degrees Celsius)") + ggtitle("Minimum temperature levels in Glasgow")

p1acf <- autoplot(acf(temp$Temperature, plot=FALSE))
p1pacf<- autoplot(pacf(temp$Temperature, plot=FALSE), ylab="PACF")

grid.arrange(p1, p1acf, p1pacf, nrow=3)
```

Next model the trend using a sinusoidal curve and obtain the residual series:

```{r, fig.width=6, fig.height=3.8, fig.align='center'}
## Model the trend using a sine and cosine model 
## that repeats itself after 365 days
model <- lm(temp$Temperature~sin(2*pi*temp$Day/365)+cos(2*pi*temp$Day/365))
summary(model)

# Fitted model
temp$fits <- model$fitted.values
p2 <- p1 + geom_line(aes(y=model$fitted.values), color = "#fc8d59", size= 1)
p2  + ggtitle("Fitted model")
```

```{r, fig.width=6, fig.height=6.5, fig.align='center'}

# Residuals
residuals <- temp$Temperature - temp$fits

res <- autoplot(as.ts(residuals), ylab = "Residuals series")
racf <- autoplot(acf(residuals, plot=FALSE), main = "")
rpacf<- autoplot(pacf(residuals, plot=FALSE), main = "", ylab="PACF")

grid.arrange(res, racf, rpacf, nrow=3)
```
The time plot suggests that the residual series is stationary, while the partial autocorrelation function suggests that an AR(1) process is appropriate. We fit the AR(1) model as shown below and obtain the following residuals.


```{r, fig.width=6, fig.height=6.5, fig.align='center'}
model.ar <- arima(residuals, order=c(1,0,0))

# Plotting
p3 <- autoplot(model.ar$residuals, ylab = "AR(1) residuals series")
p3acf <- autoplot(acf(model.ar$residuals, plot = FALSE),
                   main = "")
p3pacf <- autoplot(pacf(model.ar$residuals, plot = FALSE),
                   main = "", ylab="PACF")

grid.arrange(p3, p3acf, p3pacf, nrow=3)
```

The residuals look independent, so the model we fitted is appropriate.

####[/answer]

###[/task]

\pagebreak

# Moving average processes

The second most common type of stationary time series model is called a moving average process. Here we assume that the time series being modelled is weakly stationary, which can be obtained by removing any trend or seasonal variation using the methods mentioned in last week's learning material.

##[video,videoid="hXWa9cF42HM", duration="7m33s"] Moving average processes

###[definition] Moving average process
Let $Z_{t}$  be a purely random process (i.e. each $Z_{t}$ is independent) with mean  0 and variance $\sigma^{2}_{z}$. A **moving average process of order $q$**, denoted MA($q$), is given by

$$X_{t}=\lambda_{0}Z_{t}+\lambda_{1}Z_{t-1}+\ldots+\lambda_{q}Z_{t-q}$$

where traditionally $Z_{t}$ is given a coefficient of 1, i.e. $\lambda_{0}=1$. The model can be re-written using the backshift operator, $BZ_{t}=Z_{t-1}$, as

\begin{eqnarray}
X_{t}&=&Z_{t}+\lambda_{1}Z_{t-1}+\ldots+\lambda_{q}Z_{t-q}\nonumber\\
&=&(1+\lambda_{1}B+\ldots+\lambda_{q}B^{q})Z_{t}\nonumber\nonumber\\
&=&\theta(B)Z_{t}\nonumber
\end{eqnarray}

where $\theta(B)=1+\lambda_{1}B+\ldots+\lambda_{q}B^{q}$ is called the **characteristic polynomial**.
###[/definition]

Correlation is introduced between $X_{t}$ and $X_{t-1}$ because their values both depend on the elements of the purely random process $Z_{t-1},\ldots,Z_{t-q}$. In common with an autoregressive process, $\lambda_{1}$ is the lag 1 coefficient, $\lambda_{2}$ is the lag 2 coefficient and so on.

## Mean and variance of MA($q$) process

The mean and variance of an MA($q$) process are straightforward to calculate. Writing the process as

$$X_{t}=\sum_{j=0}^{q}\lambda_{j}Z_{t-j}$$

where again $\lambda_{0}=1$, the mean can be calculated as

\begin{eqnarray}
E(X_{t})&=&E(\sum_{j=0}^{q}\lambda_{j}Z_{t-j})\nonumber\\
&=&\sum_{j=0}^{q}\lambda_{j}E(Z_{t-j})\nonumber\\
&=&0\nonumber
\end{eqnarray}

So for any parameters $\lambda_{1},\ldots,\lambda_{q}$ an MA($q$) process has mean zero. The variance is given by

\begin{eqnarray}
\mathrm{Var}(X_{t})&=&\mathrm{Var}(\sum_{j=0}^{q}\lambda_{j}Z_{t-j})\nonumber\\
&=&\sum_{j=0}^{q}\lambda_{j}^{2}\mathrm{Var}(Z_{t-j})\nonumber\\
&=& \sigma^{2}_{z}\sum_{j=0}^{q}\lambda_{j}^{2}\nonumber
\end{eqnarray}

where again $\lambda_{0}=1$ so

$$\mathrm{Var}(X_{t})~=~\sigma^{2}_{z}[1+\sum_{j=1}^{q}\lambda_{j}^{2}]$$

There are no covariance terms in the calculation because $Z_{t},Z_{t-1},\ldots,Z_{t- q}$ are a purely random process and are therefore independent ($\mathrm{Cov}(Z_{t},Z_{t-k})=0$ for $k\neq 0$).


## Autocorrelation function of MA($q$) process

The autocovariance function is given by

$$\gamma_{\tau}=\mathrm{Cov}(X_{t},X_{t+\tau})=\left\{\begin{array}{cc}\sigma_{z}^{2}\sum_{j=0}^{q-\tau}\lambda_{j}\lambda_{j+\tau}&\mbox{if}~\tau=0,1,\ldots,q\\
0&\tau>q\\
\end{array}\right.$$

and the autocorrelation function is given by 

$$\rho_{\tau}=\mathrm{Corr}(X_{t},X_{t+\tau})=\left\{\begin{array}{cc}1&\mbox{if}~\tau=0\\
\frac{\sum_{j=0}^{q-\tau}\lambda_{j}\lambda_{j+\tau}}{\sum_{j=0}^{q}\lambda_{j}^{2}}&\mbox{if}~\tau=1,\ldots,q\\
0&\tau>q\\
\end{array}\right.$$ where $\lambda_{0}=1$.

###[supplement] Calculation of the autocovariance function

The autocovariance function is calculated as:

\begin{eqnarray}
\mathrm{Cov}(X_{t},X_{t+\tau})&=&E(X_{t}X_{t+\tau}) - E(X_{t})E(X_{t+\tau})\nonumber\\
&=&E(X_{t}X_{t+\tau})\nonumber\\
&=&E\left\{\sum_{j=0}^{q}\lambda_{j}Z_{t-j}\right\}\times\left\{\sum_{k=0}^{q}\lambda_{k}Z_{t-k+\tau}\right\}\nonumber\\
&=&\sum_{j=0}^{q}\sum_{k=0}^{q}\lambda_{j}\lambda_{k}E(Z_{t-j}Z_{t-k+\tau})\nonumber
\end{eqnarray}

The expectations $E(Z_{r}Z_{s})$ can be split into two cases:

1. If $r\neq s$ we have  $E(Z_{r}Z_{s})=E(Z_{r})E(Z_{s})=0$ because the $Z_{t}$ are independent.

2. If $r=s$ we have $E(Z_{r}^{2})=\mathrm{Var}(Z_{r})+[E(Z_{r})]^{2}=\sigma^{2}_{z}$.


Therefore, only the $Z_{r}$ included in both $X_{t}$ and $X_{t+\tau}$ contribute to the covariance function.

*  When $\tau=0$, $Z_{t},\ldots,Z_{t-q}$ are in both $X_{t}$ and $X_{t}$.

*  When $\tau=1$, $Z_{t},\ldots,Z_{t-q+1}$ are in both $X_{t}$ and $X_{t+1}$.

*  When $\tau=q-1$, $Z_{t},Z_{t-1}$ are in both $X_{t}$ and $X_{t+q-1}$.

* When $\tau=q$, $Z_{t}$ is in both $X_{t}$ and $X_{t+q}$.

*  When $\tau=q+1$ there are no elements in common.

Therefore the autocovariance function is given by

$$\gamma_{\tau}=\mathrm{Cov}(X_{t},X_{t+\tau})=\left\{\begin{array}{cc}\sigma_{z}^{2}\sum_{j=0}^{q-\tau}\lambda_{j}\lambda_{j+\tau}&\mbox{if}~\tau=0,1,\ldots,q\\
0&\tau>q\\
\end{array}\right.$$

where $\lambda_{0}=1$.
###[/supplement] 

### Notes

1. The mean and variance of any MA($q$) process are finite and constant, while the autocorrelation function is finite and does not depend on $t$. Therefore any MA($q$) is weakly stationary.

2. The autocorrelation function of an MA($q$) process is positive at lags $1,\ldots,q$ and zero for any lag greater than $q$. This gives us a method for detecting whether an MA($q$) process is an appropriate model for a given data set.

###[example]
Consider the MA(1) process $X_{t}=Z_{t}+\lambda Z_{t-1}$. Its variance is given by

\begin{eqnarray}
\mathrm{Var}(X_{t})&=&\mathrm{Var}(Z_{t}+\lambda Z_{t-1})\nonumber\\
&=&\sigma^{2}_{z}(1+\lambda^{2})\nonumber
\end{eqnarray}

Its lag one autocovariance is given by

\begin{eqnarray}
\mathrm{Cov}(X_{t},X_{t+1})&=&\mathrm{Cov}(Z_{t}+\lambda Z_{t-1},~Z_{t+1}+\lambda Z_{t})\nonumber\\
&=&\mathrm{Cov}(Z_{t},~Z_{t+1})+\lambda \mathrm{Cov}(Z_{t},~Z_{t})+\lambda \mathrm{Cov}(Z_{t-1},~Z_{t+1})+\lambda^{2}\mathrm{Cov}(Z_{t-1},~Z_{t})\nonumber\\
&=&\lambda \sigma_{z}^{2}\nonumber
\end{eqnarray}

while its lag 2 autocovariance is given by

\begin{eqnarray}
\mathrm{Cov}(X_{t},X_{t+2})&=&\mathrm{Cov}(Z_{t}+\lambda Z_{t-1},~Z_{t+2}+\lambda Z_{t+1})\nonumber\\
&=&\mathrm{Cov}(Z_{t},~Z_{t+2})+\lambda \mathrm{Cov}(Z_{t},~Z_{t+1})+\lambda \mathrm{Cov}(Z_{t-1},~Z_{t+2})+\lambda^{2}\mathrm{Cov}(Z_{t-1},~Z_{t+1})\nonumber\\
&=&0\nonumber
\end{eqnarray}

The same argument holds true for $\tau>2$. Therefore the autocorrelation function is given by

$$\rho_{\tau}=\left\{\begin{array}{cc}1&\mbox{if}~\tau=0\\
\frac{\lambda}{1+\lambda^{2}}&\mbox{if}~\tau=1\\
0&\tau>1\\
\end{array}\right.$$
###[/example]

<!-- ###[example] -->
<!-- Consider the MA(2) process $X_{t}=Z_{t}+0.9 Z_{t-1} + 0.5Z_{t-2}$. Its variance is given by -->

<!-- \begin{eqnarray} -->
<!-- \var{X_{t}}&=&\var{Z_{t}+0.9 Z_{t-1} + 0.5Z_{t-2}}\nonumber\\ -->
<!-- &=&\sigma^{2}_{z}(1+0.81 + 0.25)\nonumber\\ -->
<!-- &=&2.06\sigma^{2}_{z}\nonumber -->
<!-- \end{eqnarray} -->

<!-- Its lag one autocovariance is given by -->

<!-- \begin{eqnarray} -->
<!-- \cov{X_{t},X_{t+1}}&=&\cov{Z_{t}+0.9 Z_{t-1}+0.5Z_{t-2},~Z_{t+1}+0.9 Z_{t} +0.5Z_{t-1}}\nonumber\\ -->
<!-- &=&0.9\cov{Z_{t},~Z_{t}}+0.45 \cov{Z_{t-1},~Z_{t-1}}\nonumber\\ -->
<!-- &=&1.35\sigma_{z}^{2}\nonumber -->
<!-- \end{eqnarray} -->

<!-- while its lag 2 autocovariance is given by -->

<!-- \begin{eqnarray} -->
<!-- \cov{X_{t},X_{t+2}}&=&\cov{Z_{t}+0.9 Z_{t-1}+ 0.5Z_{t-2},~Z_{t+2}+0.9 Z_{t+1}+ 0.5Z_{t}}\nonumber\\ -->
<!-- &=&0.5\cov{Z_{t},~Z_{t}}\nonumber\\ -->
<!-- &=&0.5\sigma^{2}_{z}\nonumber -->
<!-- \end{eqnarray} -->

<!-- Therefore the autocovariance function is given by -->

<!-- $$\gamma_{\tau}=\left\{\begin{array}{cc}2.06\sigma^{2}_{z}&\mbox{if}~\tau=0\\ -->
<!-- 1.35\sigma^{2}_{z}&\mbox{if}~\tau=1\\ -->
<!-- 0.5\sigma^{2}_{z}&\mbox{if}~\tau=2\\ -->
<!-- 0&\tau>2\\ -->
<!-- \end{array}\right.$$ -->

<!-- while the autocorrelation function is given by -->

<!-- $$\rho_{\tau}=\left\{\begin{array}{cc}1&\mbox{if}~\tau=0\\ -->
<!-- 0.655&\mbox{if}~\tau=1\\ -->
<!-- 0.243&\mbox{if}~\tau=2\\ -->
<!-- 0&\tau>2\\ -->
<!-- \end{array}\right.$$ -->
<!-- ###[/example] -->


###[task]
For the MA(1) processes $X_t = Z_t + \lambda Z_{t-1}$ and $X_t = Z_t + \frac{1}{\lambda} Z_{t-1}$, show that they have the same autocorrelation function and find the autocorrelation function when $\lambda\!=\!1$ and $\lambda\!=\!-1$. Can you find a $\lambda$ so that $|\rho_{1}| > 1/2$?

####[answer]
As shown in the lecture notes, the ACF for an MA(1) process is given by

$$\rho_{\tau}=\left\{\begin{array}{cc}1&\mbox{if}~\tau=0\\
\frac{\lambda}{1+\lambda^{2}}&\mbox{if}~\tau=1\\
0&\tau>1\\
\end{array}\right.$$

Then replacing $\lambda$ with $1/\lambda$ gives 

$$\rho_{1}=\frac{\frac{1}{\lambda}}{1+\frac{1}{\lambda^{2}}}=\frac{\lambda}{\lambda^{2}+1}$$

hence the autocorrelation function remains unchanged.

Then replacing $\lambda$ with $1/\lambda$ gives 

$$\rho_{1}=\frac{\frac{1}{\lambda}}{1+\frac{1}{\lambda^{2}}}=\frac{\lambda}{\lambda^{2}+1}$$

hence the autocorrelation function remains unchanged.

When $\lambda=1$ $\rho_{1}=0.5$ and when $\lambda=-1$ $\rho_{1}=-0.5$, so no, we cannot find a $\lambda$ so that $|\rho|>1/2$. To see this simply plot $\rho_1$ against $\lambda$, which gives the following plot.

```{r, echo=FALSE, fig.align='center', fig.width=6, fig.height=3}
lambda <- seq(-5, 5, l=100)
rho <- lambda/(lambda^2+1)
data <- data.frame(lambda=lambda, rho=rho)
equation<- data.frame(x = -4, y = -0.15, label = "rho==frac(lambda, 1+lambda^2)")
ggplot(data, aes(x=lambda, y=rho)) + geom_line() + xlab(expression(lambda)) +ylab(expression(rho)) + geom_text(data = equation, aes(x = x, y = y, label = label), parse = TRUE )

```


####[/answer]
###[/task]

\pagebreak
###[example]
The graphs below show time plots and correlograms of simulated data with MA(1) and MA(2) correlation structures. As suggested by theory, the sample autocorrelation functions are only significantly different from zero for lags less than or equal to $q$. The code to simulate the data is given below.

```{r, fig.width=6, fig.height=5, fig.align='center'}
# Simulated MA(1) and MA(2) data
data1 <- arima.sim(model=list(ma=0.8), n=100, sd=10)
data2 <- arima.sim(model=list(ma=c(0.8, 0.8)), n=100, sd=10)

# Plotting
p1 <- autoplot(data1, main = "MA(1) data")
p1acf <- autoplot(acf(data1, plot = FALSE),
                  main = "", ylab="ACF")
p2 <- autoplot(data2, main = "MA(2) data")
p2acf <- autoplot(acf(data2, plot = FALSE),
                  main = "", ylab="ACF")

grid.arrange(p1, p1acf, p2, p2acf, nrow=2)
```

###[/example]

## Invertibility
If our data gives a sample autocorrelation function similar to an MA(1) process, we cannot estimate $\lambda$ from the data, because $\lambda$ and $1/\lambda$ will both fit the data equally well, as we've seen in Task 5.


For a given MA($q$) autocorrelation function there are two sets of possible parameters $(\lambda_{1},\ldots,\lambda_{q})$ that could have produced it. However, only one of these has the desirable property of being **invertible**.  **Invertibility** roughly means that we can invert the formula for the MA process and express $X_{t}$ in terms of past values $X_{t-1}$, $X_{t-2}$, etc. The other solution in general has the property that $X_t$ can only be expressed in terms of future values  $X_{t+1}$, $X_{t+2}$, etc. Since the latter is less intuitive than the former, typically one focuses in estimation on the invertible solution.

###[definition] Invertibility

An MA($q$) process is **invertible** if it can be written as an autoregressive process of infinite order, i.e. as

$$X_t = \sum_{j=1}^\infty b_j X_{t-j} + Z_t$$

where the coefficients $b_{j}$ form a convergent sum, i.e. $\sum_{j=1}^{\infty} b_{j}<\infty$.

###[/definition]

###[example]
Consider the MA(1) process  $X_{t}=Z_{t}+\lambda Z_{t-1}$. We can invert it as follows.

\begin{eqnarray*}
Z_t &=& X_t - \lambda Z_{t-1} \\
   &=&  X_t - \lambda (X_{t-1} -\lambda Z_{t-2}) \\
   &=&  X_t - \lambda X_{t-1} + \lambda^2 (X_{t_2} - \lambda Z_{t-3}) \\
   &=& \vdots\\
   &=& \sum_{j=0}^\infty (-\lambda)^j X_{t-j}
\end{eqnarray*}

Then using this description we can write the MA(1) process as

$$X_{t}=Z_{t}+\lambda Z_{t-1}=Z_{t} + \lambda\sum_{j=0}^\infty (-\lambda)^j X_{t-j-1}$$

which is an AR($\infty$) process. This process is invertible if the coefficients form a convergent sum, i.e. if

$$\sum_{j=0}^{\infty}(-\lambda)^{j}=1+(-\lambda)+(-\lambda)^{2}+(-\lambda)^{3}+\ldots<\infty$$

We know that the sum of a geometric progression is only convergent if $|\lambda|<1$. Therefore, for the MA(1) processes 
\begin{eqnarray}
\mbox{\textbf{A}}\hspace{1cm}X_{t}&=&Z_{t}+\lambda Z_{t-1} \mbox{ and }\nonumber\\
\mbox{\textbf{B}}\hspace{1cm}X_{t}&=&Z_{t}+\frac{1}{\lambda} Z_{t-1},\nonumber
\end{eqnarray}
either $|\lambda|<1$ or $|1/\lambda|<1$, so only one of them is invertible.
###[/example]

Let us now specify the condition that determines whether a process is invertible.

###[theorem] Invertibility
The MA($q$) process

\begin{eqnarray}
X_{t}&=&Z_{t}+\lambda_{1}Z_{t-1}+\ldots+\lambda_{q}Z_{t-q}\nonumber\\
&=&(1+\lambda_{1}B+\ldots+\lambda_{q}B^{q})Z_{t}\nonumber\nonumber\\
&=&\theta(B)Z_{t}\nonumber
\end{eqnarray}

is invertible if and only if the roots of the characteristic polynomial $\theta(B)$ have modulus greater than one and hence lie outside the unit circle.
###[/theorem]

**Note**: The condition for invertibility of an MA($q$) process is identical to the condition for stationarity of an AR($p$) process.

<!-- ###[example] -->
<!-- For an MA(1) process $X_{t}=Z_{t}+\lambda Z_{t-1}$ the characteristic equation $\theta(B)=1+\lambda B$ has the single root $-1/\lambda$, and hence it is stationary if $|-1/\lambda|>1$, which is identical to $|\lambda|<1$. This is what we found before. -->
<!-- ###[/example] -->

###[task]
Is the MA(2) process $X_{t}=Z_{t} + 4.25Z_{t-1} + Z_{t-2}$ invertible?

####[answer]

The characteristic polynomial is given by

$$\theta(B)~=~1 + 4.25B + B^{2}.$$

Solving $1 + 4.25B + B^{2}=0$ using the quadratic formula gives

\begin{eqnarray}
\mbox{roots}&=&\frac{-4.25\pm\sqrt{(4.25)^2-4\times1\times1}}{2\times 1}\nonumber\\
\mbox{roots}&=&\frac{-4.25\pm3.75}{2}\nonumber\\
\mbox{roots}&=&-0.25\mbox{~and~}-4\nonumber
\end{eqnarray}

Therefore as one of the roots has modulus less than one the process is not invertible.
####[/answer]
###[/task]


## Model identification

Given a stationary time series, how do we determine if:

(a) a moving average process is appropriate, and

(b) if it is, which order process should we use (i.e. which value of $q$)?

We can answer both of these questions using the autocorrelation function (correlogram). Recall that for an MA($q$) process the ACF is given by

$$\rho_{\tau}=\mathrm{Corr}[X_{t},X_{t+\tau}]=\left\{\begin{array}{cc}1&\mbox{if}~\tau=0\\
\frac{\sum_{j=0}^{q-\tau}\lambda_{j}\lambda_{j+\tau}}{\sum_{j=0}^{q}\lambda_{j}^{2}}&\mbox{if}~\tau=1,\ldots,q\\
0&\tau>q\\
\end{array}\right.$$

This function is non-zero for $\tau\leq q$ and zero for $\tau>q$. It can also be shown that for $\tau>q$

$$\rho_{\tau}\sim\mbox{N}\left(0, \frac{1}{n}\right)$$

which results in approximate 95$\%$ confidence intervals for $\rho_{\tau}$ of $\pm1.96/\sqrt{n}$. Therefore it is straightforward to identify whether an MA($q$) process is appropriate by plotting the sample autocorrelation function (correlogram). If an MA($q$) process is appropriate the correlogram will be significantly different from zero at lags $\tau=1,\ldots,q$, and within the 95$\%$ confidence intervals for $\tau>q$.


**Note**: The autocorrelation function (ACF) tells us whether an MA($q$) process is appropriate while the partial autocorrelation function (PACF) suggests whether an AR($p$) process is appropriate. Therefore for a given time series, both should be plotted to show which process would be a good model.

###[task]
Sample autocorrelation and partial autocorrelation functions are shown below for four data sets, one data set on each row. What type of MA($q$) process is an appropriate model in each case?

```{r, echo=FALSE, fig.width=6, fig.height=9.5, fig.align='center'}
data1 <- arima.sim(model=list(ma=0.7), n=1000, sd=1)
data2 <- arima.sim(model=list(ma=c(0.7, 0.7)), n=1000, sd=1)
data3 <- arima.sim(model=list(ma=c(0.6, 0.6, 0.6)), n=1000, sd=1)
data4 <- arima.sim(model=list(ar=c(0.8)), n=1000, sd=1)

# Plotting

p1acf <- autoplot(acf(data1, plot = FALSE),
                   main = "Series 1")
p1pacf <- autoplot(pacf(data1, plot = FALSE),  main = "", ylab="PACF")

p2acf <- autoplot(acf(data2, plot = FALSE),
                   main = "Series 2")
p2pacf <- autoplot(pacf(data2, plot = FALSE),  main = "", ylab="PACF")

p3acf <- autoplot(acf(data3, plot = FALSE),
                   main = "Series 3")
p3pacf <- autoplot(pacf(data3, plot = FALSE),  main = "",  ylab="PACF")

p4acf <- autoplot(acf(data4, plot = FALSE),
                   main = "Series 4")
p4pacf <- autoplot(pacf(data4, plot = FALSE),  main = "", ylab="PACF")

grid.arrange(p1acf, p1pacf, p2acf, p2pacf, 
             p3acf, p3pacf, p4acf, p4pacf,
             nrow=4)

```

####[answer]
Series 1 is an MA(1), Series 2 is an MA(2), Series 3 is an MA(3) and Series 4 is an AR(1). Notice that when an MA($q$) process is not appropriate, the sample ACF does not become zero at low lag.
####[/answer]
###[/task]

###[supplement] Parameter estimation

Parameter estimation is harder than for an AR($p$) process because in the AR($p$) case, ordinary least squares can be used. For an MA($q$) process an ordinary least squares procedure would require minimisation of random variables of which values are not known. Instead, a modification of ordinary least squares is used, called **conditional least squares**.

Consider the MA(1) model $X_{t}=\mu+Z_{t}+\lambda Z_{t-1}$, where $\mathrm{Var}(Z_{t})=\sigma^{2}_{z}$. The conditional least squares algorithm works as follows.

1. Select starting values for $\mu$ and $\lambda$,

$$\tilde{\mu}=\frac{1}{n}\sum_{t=1}^{n}x_{t}\hspace{1cm}\mbox{and by solving}\hspace{1cm}\hat{\rho}_{1}=\frac{\tilde{\lambda}}{1+\tilde{\lambda}^{2}}$$

2. Calculate the conditional residual sum of squares

$$S(\tilde{\mu},\tilde{\lambda})=\sum_{t=1}^{n}[x_{t}-\tilde{\mu}-\tilde{\lambda}Z_{t-1}]^{2}$$

where $Z_{0}=0$ and $Z_{t}$ is calculated recursively using

$$Z_{t}=x_{t}-\tilde{\mu}-\tilde{\lambda}Z_{t-1}$$

3. Repeat Step 2 for a range of values of $(\mu, \lambda)$ that are close to the initial estimates in Step 1. Then determine the estimates $(\hat{\mu},\hat{\lambda})$ as the values that minimise $S(\tilde{\mu},\tilde{\lambda})$ over all those values considered.

4. Using the fact that the variance of an MA(1) process is $\mathrm{Var}(X_{t})=\sigma^{2}_{z}(1+\lambda^{2})$, we obtain that

$$\hat{\sigma}^{2}_{z}=\frac{\hat{\sigma}^{2}}{(1+\hat{\lambda}^{2})}$$

where $\hat{\sigma}^{2}$ is the overall variance of the process and is estimated as we saw earlier.

Parameter estimation for more general MA($q$) processes can be implemented in a similar way. The conditional least squares method is used by R.
###[/supplement]


###[example]
Consider the simulated data set shown below, which was generated using the following R code.

```{r, fig.width=6, fig.height=4.8, fig.align='center'}
time <- 1:200
data.ar <- arima.sim(model=list(ma=c(0.6, 0.3, 0.6)), n=200, sd=10)
data <- data.ar + 30 + 0.05 * time + 0.004*time^2

# Plotting

p1 <- autoplot(data, main="Raw data plot")
p1acf <- autoplot(acf(data, plot = FALSE),
                  main = "")

grid.arrange(p1, p1acf, nrow=2)
```


From looking at the time plot and correlogram the data appear to have a quadratic trend, which we remove before trying to model the correlation. Removing the trend using regression methods (see Week 6 for details), i.e. fitting $m_{t}=\beta_{0}+\beta_{1}t+\beta_{2}t^{2}$, gives the following residual series.

```{r, fig.width=6, fig.height=6, fig.align='center'}

## Remove the trend
time2 <- time^2
linear.model <- lm(data~time+time2)
residual.series <- data - linear.model$fitted.values

# Plotting
    p <- autoplot(residual.series, main="Residual series")
 pacf <- autoplot(acf(residual.series, plot = FALSE),
                  ylab="ACF", main="")
ppacf <- autoplot(pacf(residual.series, plot = FALSE),  
                  ylab="PACF", main="")

grid.arrange(p, pacf, ppacf, nrow=3)
```

The time plot suggests the residual series is stationary, while the autocorrelation function (correlogram) suggests an MA(3) process is appropriate. Note that the partial autocorrelation function does not really tell us a great deal here. Finally we fit an MA(3) model to the data using the R function `arima()`, and plot the residuals to ensure they are independent.

```{r, fig.width=6, fig.height=6, fig.align='center'}
## Fit an MA(3) model to the data
model.ma <- arima(residual.series, order=c(0,0,3))
model.ma

# Plotting
p <- autoplot(model.ma$residuals, main="MA(3) residual series")
pacf <- autoplot(acf(model.ma$residuals, plot = FALSE),
                  main="", ylab="ACF")
ppacf <- autoplot(pacf(model.ma$residuals, plot = FALSE),  
                  main="", ylab="PACF")

grid.arrange(p, pacf, ppacf, nrow=3)
```

The residuals look independent, so the model below is appropriate.

$$X_{t}=\beta_{0}+\beta_{1}t+\beta_{2}t^{2}+\lambda_{1}Z_{t-1}+\lambda_{2}Z_{t-2}+\lambda_{3}Z_{t-3}+Z_{t}$$
###[/example]

<!-- \pagebreak -->

<!-- # More general time series processes -->

<!-- We have discussed the two most important models for stationary time series data, autoregressive and moving average processes. For most data sets these models will be an adequate representation of the short-term correlation, with autoregressive correlation occurring more often than moving average. However, occasionally you may meet data that are not well represented by either of these time series processes, an example of which is shown below. -->

<!-- ###[example] -->
<!-- Consider the following data which appear to be stationary but contain short-term correlation. -->

<!-- ```{r, echo = FALSE, fig.height=6, fig.width=6, fig.align='center' } -->
<!-- ## Simulate ARMA(1,1) data -->
<!-- data <- arima.sim(model=list(ar=0.7, ma=0.9), n=1000, sd=1) -->

<!-- # Plotting -->
<!-- p <- autoplot(data, main="Time plot") -->
<!-- pacf <- autoplot(acf(data, plot = FALSE), main="",ylab="ACF") -->
<!-- ppacf <- autoplot(pacf(data, plot = FALSE), main="", ylab="PACF") -->

<!-- grid.arrange(p, pacf, ppacf, nrow=3) -->
<!-- ``` -->


<!-- The ACF and PACF suggest that neither an AR not an MA process is appropriate, but as these are the only models we know, we fit them to the data to see how well they remove the short-term correlation. We chose the order ($p$ and $q$) as the lowest values that removed the majority of the correlation, which resulted in an AR(6) model or an MA(5) model as shown below. -->


<!-- ```{r, fig.width=6, fig.height=6, fig.align='center'} -->

<!-- ## Fit AR an MA models to the data -->
<!-- model.ma <- arima(data, order=c(0,0,5)) -->
<!-- model.ar <- arima(data, order=c(6,0,0)) -->

<!-- # Plotting -->
<!-- p1 <- autoplot(model.ma$residuals, main="", ylab="MA(5) residual series") -->
<!-- p2 <- autoplot(model.ar$residuals, main="", ylab="AR(6) residual series") -->
<!-- p1acf <- autoplot(acf(model.ma$residuals, plot = FALSE),  -->
<!--                   ylab="ACF", main="") -->
<!-- p2acf <- autoplot(acf(model.ar$residuals, plot = FALSE),  -->
<!--                   ylab="ACF", main="") -->
<!-- p1pacf <- autoplot(pacf(model.ma$residuals, plot = FALSE),  -->
<!--                   ylab="PACF", main="") -->
<!-- p2pacf <- autoplot(pacf(model.ar$residuals, plot = FALSE),  -->
<!--                   ylab="PACF", main="") -->

<!-- grid.arrange(p1, p2, p1acf, p2acf, p1pacf, p2pacf, nrow=3) -->
<!-- ``` -->


<!-- Neither of these models fit the data perfectly, and both use high order processes ($p=6$ and $q=5$ respectively), which include a relatively large number of parameters. This emphasises two important points: -->

<!-- 1. Even if the correlation structure does not look like an AR($p$) or an MA($q$) process, fitting these models with large enough $p$ and $q$ will remove the majority of the correlation. Therefore it is better to model correlation with the wrong time series process than not to model it at all. -->

<!-- 2. However,  AR($p$) or MA($q$) processes are not always appropriate models for short-term correlation. -->

<!-- ###[/example] -->

<!-- In this section we discuss a wider class of processes that encompass both AR($p$) and MA($q$) processes as special cases. The first type of processes we describe are for modelling stationary data, while the second class can additionally model long-term trends. -->

<!-- Recall that an AR($p$) process is given by -->

<!-- \begin{eqnarray} -->
<!-- X_{t}&=&\alpha_{1}X_{t-1}+\ldots+\alpha_{p}X_{t-p}+Z_{t}\nonumber\\ -->
<!-- X_{t}-\alpha_{1}X_{t-1}-\ldots-\alpha_{p}X_{t-p}&=&Z_{t}\nonumber\\ -->
<!-- (1-\alpha_{1}B-\alpha_{2}B^{2}-\ldots-\alpha_{p}B^{p})X_{t}&=&Z_{t}\nonumber\\ -->
<!-- \phi(B)X_{t}&=&Z_{t}\nonumber -->
<!-- \end{eqnarray} -->

<!-- while an MA($q$) process is given by -->

<!-- \begin{eqnarray} -->
<!-- X_{t}&=&Z_{t}+\lambda_{1}Z_{t-1}+\ldots+\lambda_{q}Z_{t-q}\nonumber\\ -->
<!-- &=&(1+\lambda_{1}B+\ldots+\lambda_{q}B^{q})Z_{t}\nonumber\nonumber\\ -->
<!-- &=&\theta(B)Z_{t}\nonumber -->
<!-- \end{eqnarray} -->

<!-- We generalise these time series models by combining them together. -->


<!-- ###[definition] Autoregressive moving average process -->

<!-- An **autoregressive moving average process of order $(p,q)$** denoted ARMA($p,q$) is given by -->

<!-- \begin{eqnarray} -->
<!-- X_{t}&=&\alpha_{1}X_{t-1}+\ldots+\alpha_{p}X_{t-p} + Z_{t} + \lambda_{1}Z_{t-1}+\ldots+\lambda_{q}Z_{t-q}\nonumber\\ -->
<!-- &=&\sum_{j=1}^{p}\alpha_{j}X_{t-j}+\sum_{j=1}^{q}\lambda_{j}Z_{t-j} + Z_{t}\nonumber -->
<!-- \end{eqnarray} -->

<!-- and is a combination of an AR($p$) process and an MA($q$) process. Using the backshift operator the model can be re-written as -->

<!-- \begin{eqnarray} -->
<!-- X_{t}&=&\sum_{j=1}^{p}\alpha_{j}X_{t-j}+\sum_{j=1}^{q}\lambda_{j}Z_{t-j}+Z_{t}\nonumber\\ -->
<!-- X_{t}-\sum_{j=1}^{p}\alpha_{j}X_{t-j}&=&\sum_{j=1}^{q}\lambda_{j}Z_{t-j}+Z_{t}\nonumber\\ -->
<!-- (1-\alpha_{1}B-\ldots-\alpha_{p}B^{p})X_{t}&=&(1+\lambda_{1}B+\ldots+\lambda_{q}B^{q})Z_{t}\nonumber\\ -->
<!-- \phi(B)X_{t}&=&\theta(B)Z_{t}\nonumber -->
<!-- \end{eqnarray} -->
<!-- ###[/definition] -->

<!-- An ARMA($p,q$) model is a more flexible representation of short-term correlation than using an AR($p$) or an MA($q$) process alone. -->

<!-- ###[example] -->
<!-- The data in this example can be modelled by an ARMA(1,1) process -->

<!-- $$X_{t}=\alpha X_{t-1}+\lambda Z_{t-1}+Z_{t}$$ -->

<!-- using the R code: -->

<!-- ```{r, fig.width=6, fig.height=6, fig.align='center'} -->
<!-- model.arma <- arima(data, order=c(1,0,1)) -->

<!-- # Plotting -->
<!-- p <- autoplot(model.arma$residuals, ylab="ARMA(1,1) residual series", main="") -->
<!-- pacf <- autoplot(acf(model.arma$residuals, plot = FALSE), -->
<!--                   ylab="ACF", main="") -->
<!-- ppacf <- autoplot(pacf(model.arma$residuals, plot = FALSE),   -->
<!--                   ylab="PACF", main="") -->
<!-- grid.arrange(p, pacf, ppacf, nrow=3) -->
<!-- ``` -->


<!-- ###[/example] -->

<!-- ## Mean of an ARMA($p,q$) process -->

<!-- The mean of an ARMA($p,q$) process can be calculated in the same way as for an AR($p$) process, using the following result. -->

<!-- ###[theorem] -->
<!-- Any ARMA($p,q$) process can be written as an infinite sum of a purely random process, i.e. an MA($\infty$) process. That is -->

<!-- $$X_{t}=\sum_{j=1}^{p}\alpha_{j}X_{t-j}+\sum_{j=1}^{q}\lambda_{j}Z_{t-j}+Z_{t}=\sum_{j=0}^{\infty}\beta_{j}Z_{t-j}$$ -->

<!-- for some coefficients  $\beta_{j}$. -->
<!-- ###[/theorem] -->

<!-- ## Variance and autocorrelation function -->

<!-- Calculating the variance and autocorrelation function for an ARMA($p,q$) process can be done by assuming the process is stationary and deriving the Yule-Walker equations. However, this becomes algebraically messy for even small values of $p$ and $q$. Therefore we illustrate the calculation for the ARMA(1,1) process -->

<!-- $$X_{t}=\alpha X_{t-1}+\lambda Z_{t-1}+Z_{t}$$ -->

<!-- The variance is calculated as follows. -->

<!-- \begin{eqnarray} -->
<!-- X_{t}&=&\alpha X_{t-1}+\lambda Z_{t-1}+Z_{t}\nonumber\\ -->
<!-- X_{t}^{2}&=&\alpha X_{t-1}X_{t}+\lambda Z_{t-1}X_{t}+Z_{t}X_{t}\nonumber\\ -->
<!-- E(X_{t}^{2})&=&\alpha E(X_{t-1}X_{t})+\lambda E(Z_{t-1}X_{t})+E(Z_{t}X_{t})\nonumber\\ -->
<!-- \mathrm{Var}(X_{t})&=&\alpha \gamma_{1}+\lambda E(Z_{t-1}X_{t})+E(Z_{t}X_{t})\nonumber -->
<!-- \end{eqnarray} -->

<!-- where the last line holds true because $E(X_{t})=0$, so that $\mathrm{Var}(X_{t})=E(X_{t}^2)$. This also means that $\gamma_{1}=\mathrm{Cov}(X_{t-1}X_{t})=E(X_{t-1}X_{t})$. The latter two expectations are straightforward to calculate if we recall that -->

<!-- $$E(Z_{t}Z_{t-k})=0~\forall ~k>0\hspace{1cm}\mbox{and}\hspace{1cm}E(Z_{t}X_{t-k})=0~\forall ~k>0$$ -->

<!-- which are true because $Z_{t}$ is a purely random process. Then -->

<!-- \begin{eqnarray} -->
<!-- E(Z_{t}X_{t})&=&E(Z_{t}\{\alpha X_{t-1}+\lambda Z_{t-1}+Z_{t}\})\nonumber\\ -->
<!-- &=&E(Z_{t}^{2})\nonumber\\ -->
<!-- &=&\sigma^{2}_{z}\nonumber -->
<!-- \end{eqnarray} -->

<!-- and -->

<!-- \begin{eqnarray} -->
<!-- E(Z_{t-1}X_{t})&=&E(Z_{t-1}\{\alpha X_{t-1}+\lambda Z_{t-1}+Z_{t}\})\nonumber\\ -->
<!-- &=&\alpha E(Z_{t-1}X_{t-1})+\lambda E(Z_{t-1}^{2})\nonumber\\ -->
<!-- &=&\alpha\sigma^{2}_{z} + \lambda\sigma^{2}_{z}\nonumber -->
<!-- \end{eqnarray} -->

<!-- Therefore  we get that -->

<!-- \begin{eqnarray} -->
<!-- \mathrm{Var}(X_{t})&=&\alpha \gamma_{1}+\lambda (\alpha\sigma^{2}_{z} + \lambda\sigma^{2}_{z})+\sigma^{2}_{z}\nonumber\\ -->
<!-- &=&\alpha \gamma_{1}+\sigma^{2}_{z}(\lambda^{2}+\lambda\alpha+1)\nonumber -->
<!-- \end{eqnarray} -->

<!-- The autocorrelation function at lag 1 is calculated in the same way, -->

<!-- \begin{eqnarray} -->
<!-- X_{t}&=&\alpha X_{t-1}+\lambda Z_{t-1}+Z_{t}\nonumber\\ -->
<!-- X_{t-1}X_{t}&=&\alpha X_{t-1}^{2}+\lambda X_{t-1}Z_{t-1}+X_{t-1}Z_{t}\nonumber\\ -->
<!-- E(X_{t-1}X_{t})&=&\alpha E(X_{t-1}^{2})+\lambda E(X_{t-1}Z_{t-1})+E(X_{t-1}Z_{t})\nonumber\\ -->
<!-- \gamma_{1}&=&\alpha \gamma_{0}+\lambda \sigma^{2}_{z}\nonumber -->
<!-- \end{eqnarray} -->

<!-- Dividing by $\gamma_{0}=\mathrm{Var}(X_{t})$ gives the autocorrelation function at lag 1 -->


<!-- \begin{eqnarray} -->
<!-- \rho_{1}&=&\alpha+\frac{\lambda\sigma^{2}_{z}}{\gamma_{0}}\nonumber\\ -->
<!-- &=&\alpha+\frac{\lambda\sigma^{2}_{z}}{\alpha \gamma_{1}+\sigma^{2}_{z}(\lambda^{2}+\lambda\alpha+1)}\nonumber -->
<!-- \end{eqnarray} -->


<!-- The lag $\tau$ autocorrelation function for any $\tau>1$ is calculated analogously as -->

<!-- \begin{eqnarray} -->
<!-- X_{t}&=&\alpha X_{t-1}+\lambda Z_{t-1}+Z_{t}\nonumber\\ -->
<!-- X_{t-\tau}X_{t}&=&\alpha X_{t-\tau}X_{t-1}+\lambda X_{t-\tau}Z_{t-1}+X_{t-\tau}Z_{t}\nonumber\\ -->
<!-- E(X_{t-\tau}X_{t})&=&\alpha E(X_{t-\tau}X_{t-1})+\lambda E(X_{t-\tau}Z_{t-1})+E(X_{t-\tau}Z_{t})\nonumber\\ -->
<!-- \gamma_{\tau}&=&\alpha \gamma_{\tau-1}\nonumber -->
<!-- \end{eqnarray} -->

<!-- Therefore the autocorrelation function is given by -->

<!-- $$\rho_{\tau}=\alpha \rho_{\tau-1}$$ -->


<!-- which is the same Yule-Walker equation as an AR(1) process. Therefore we have -->


<!-- \begin{eqnarray} -->
<!-- \rho_{0}&=&1\nonumber\\ -->
<!-- \rho_{1}&=&\alpha+\frac{\lambda\sigma^{2}_{z}}{\alpha \gamma_{1}+\sigma^{2}_{z}(\lambda^{2}+\lambda\alpha+1)}\nonumber\\ -->
<!-- \rho_{2}&=&\alpha^{2}+\frac{\alpha\lambda\sigma^{2}_{z}}{\alpha \gamma_{1}+\sigma^{2}_{z}(\lambda^{2}+\lambda\alpha+1)}\nonumber -->
<!-- \end{eqnarray} -->

<!-- and so on. The autocorrelation function for a higher order ARMA($p,q$) process can be calculated using the same method, although it becomes increasingly messy. -->


<!-- ## Stationarity and invertibility -->

<!-- We know that an MA($q$) process is always stationary and an AR($p$) process is always invertible. Therefore an ARMA($p,q$) process -->

<!-- $$\phi(B)X_{t}=\theta(B)Z_{t}$$ -->

<!-- * is stationary if the AR($p$) part is stationary, i.e. if the roots of the AR($p$) characteristic polynomial  $\phi(B)$ have modulus larger than one; -->

<!-- * is invertible if the MA($q$) part is invertible, i.e. if the roots of the MA($q$) characteristic polynomial  $\theta(B)$ have modulus larger than one. -->

<!-- ###[example] -->

<!-- Consider the ARMA(1,1) process -->

<!-- $$X_{t}=2X_{t-1}-0.4Z_{t-1}+Z_{t}$$ -->

<!-- which can be re-written as -->

<!-- $$X_{t}(1-2B)=Z_{t}(1-0.4B)$$ -->

<!-- The characteristic equation for the AR part is $\phi(B)=1-2B=0$, which has a single root $B=0.5<1$. Therefore the process is not stationary. The characteristic equation for the MA part is $\theta(B)=1-0.4B=0$, which has a single root $B= 2.5>1$. Therefore the process is invertible. -->
<!-- ###[/example] -->

<!-- ###[task] -->
<!-- For the following ARMA($p,q$) models write down the characteristic equations. -->

<!-- * $X_{t}=5X_{t-1}+Z_{t}-0.2Z_{t-1}$ -->

<!-- * $X_{t}=X_{t-1}+Z_{t}-X_{t-2}+0.1Z_{t-1}$ -->

<!-- * $X_{t}=0.4X_{t-1}+Z_{t}-0.9Z_{t-1}+0.3Z_{t-2}$ -->

<!-- ####[answer] -->
<!-- The characteristic equations are: -->

<!-- * $\phi(B)=1-5B$ and $\theta(B)=1-0.2B$. -->

<!-- * $\phi(B)=1-B+B^{2}$ and $\theta(B)=1+0.1B$. -->

<!-- * $\phi(B)=1-0.4B$ and $\theta(B)=1-0.9B+0.3B^{2}$. -->

<!-- ####[/answer] -->
<!-- ###[/task] -->

<!-- ## Model identification -->

<!-- AR($p$) and MA($q$) processes are straightforward to identify from the ACF and PACF functions as follows. -->

<!-- * If the ACF is significantly different from zero for only the first $q$ lags (for small $q$), then an MA($q$) model is appropriate. -->

<!-- * If the PACF is significantly different from zero for only the first $p$ lags (for small $p$), then an AR($p$) model is appropriate. -->


<!-- However identifying an ARMA model is not that easy. The data in the first example in this section are simulated from an ARMA(1,1) process, and are shown below again. -->


<!-- ```{r, echo = FALSE, fig.height=6, fig.width=6, fig.align='center' } -->
<!-- ## Simulate ARMA(1,1) data -->
<!-- # data <- arima.sim(model=list(ar=0.7, ma=0.9), n=1000, sd=1) -->
<!-- # Plotting -->
<!-- library(gridExtra) -->
<!-- p <- autoplot(data, main="Time plot") -->
<!-- pacf <- autoplot(acf(data, plot = FALSE), main="ACF") -->
<!-- ppacf <- autoplot(pacf(data, plot = FALSE),  main="PACF") -->

<!-- grid.arrange(p, pacf, ppacf, nrow=3) -->
<!-- ``` -->

<!-- Neither the ACF or PACF give any clues as to the appropriate type of time series process. All they tell us is that it is not an AR($p$) process or an MA($q$) process. Furthermore, you may hope that if -->
<!-- you fit an AR(1) process to these data the residuals will look like an MA(1) process and if you fit an MA(1) process the residuals would resemble an AR(1) process, but as the graphs below show this is not the case. -->



<!-- ```{r, fig.width=6, fig.height=6, fig.align='center'} -->
<!-- ## Fit an ARMA model to the data -->
<!-- model.arma <- arima(data, order=c(1,0,1)) -->
<!-- model.arma -->

<!-- # Plotting -->
<!-- p <- autoplot(model.arma$residuals, main="",  -->
<!--               ylab="ARMA(1,1) residual series") -->
<!-- pacf <- autoplot(acf(model.arma$residuals, plot = FALSE),  -->
<!--                 ylab="ACF", main="") -->
<!-- ppacf <- autoplot(pacf(model.arma$residuals, plot = FALSE),   -->
<!--                    ylab="PACF", main="") -->
<!-- grid.arrange(p, pacf, ppacf, nrow=3) -->
<!-- ``` -->

<!-- ### Notes -->

<!-- 1. Model identification for an ARMA($p,q$) process where $p,q>0$ is difficult. -->

<!-- 2. First determine if the ACF and PACF resemble either an MA($q$) or an AR($p$) process. -->

<!-- 3. If not then adopt a trial and error approach, starting with the simplest model (i.e. an ARMA(1,1)) and increasing the complexity until the correlation has been removed. -->

<!-- ###[task] -->
<!-- Determine an appropriate ARMA($p,q$) model for the time series processes $X$ and $Y$ from their ACF and PACF. -->

<!-- ```{r echo=FALSE,  fig.align='center', out.width='100%'} -->
<!-- knitr::include_graphics('ARMA.pdf') -->
<!-- ``` -->

<!-- ####[answer] -->
<!-- The ACFs and PACFs do not look like either an AR($p$) process or an MA($q$) process, which makes identification difficult. A good approach in this case is to fit a simple ARMA($p,q$) model, *e.g.* ARMA(1,1), and see if that removes the correlation. In fact, both time series processes $X$ and $Y$ were generated from different ARMA(1,1) models. -->

<!-- ####[/answer] -->
<!-- ###[/task] -->

<!-- ###[supplement] Parameter estimation -->

<!-- Parameter estimation for an ARMA($p,q$) process is also not straightforward, and can be implemented using a similar conditional least squares algorithm to that used for an MA($q$) process. The details are omitted here, but it is a simple extension of the algorithm discussed in the context of a moving average process. This is also the algorithm used by R to estimate the parameters in an ARMA($p,q$) model. -->
<!-- ###[/supplement] -->

<!-- # Non-stationary models -->

<!-- So far we have modelled trend and seasonal variation first, before representing the residuals with a short-term correlation model. This is the approach most commonly adopted in time series modelling, because it allows the shape of the trend and seasonal variation to be estimated, before representing the correlation structure with a fairly simple time series model. -->

<!-- An alternative approach is to model the trend, seasonal variation and correlation simultaneously. However, although such an approach has been widely used, it simply removes the trend rather than modelling it. Therefore if capturing the shape of the trend or seasonal variation is the goal of the analysis  this approach is not appropriate. -->

<!-- The class of non-stationary time series models described here combine ARMA($p,q$) models and differencing. Recall in [Week 6](http://moodle2.gla.ac.uk/pluginfile.php/1508882/mod_resource/content/3/week6.pdf) that one way to eliminate a trend in a non-stationary time series $X_{t}$, is to calculate its first order differences -->

<!-- $$Y_t = \nabla X_t = (1 - B)X_t = X_t - X_{t-1}.$$ -->

<!-- Usually, first or second order differences are enough to obtain a stationary series $\{Y_t\}$, but in general we can difference $d$ times: -->

<!-- $$Y_t = \nabla^d X_t = (1-B)^d X_t.$$ -->

<!-- Combining this operator with an ARMA($p,q$) process leads to the following general class of models. -->

<!-- ###[definition] Autoregressive integrated moving average process -->

<!-- $\{X_t\}$ is an **autoregressive integrated moving average process of order ($p,d,q$)**, denoted ARIMA($p,d,q$) if the $d^{th}$ order differenced process -->

<!-- $$Y_t = \nabla^d X_t =(1-B)^d X_t$$ -->

<!-- is an ARMA($p,q$) process. An ARIMA($p,d,q$) process can be written most easily in terms of characteristic polynomials. If we write the ARMA($p,q$) process for $Y_{t}$ as -->

<!-- $$\phi(B) Y_t = \theta(B) Z_t$$ -->

<!-- then as $Y_{t}=(1-B)^d X_t$, an ARIMA($p,d,q$) process can be written as -->

<!-- $$\phi(B) (1-B)^d X_t = \theta(B) Z_t.$$ -->
<!-- ###[/definition] -->


<!-- ## Notes -->

<!-- * The characteristic polynomial for the AR part of the ARIMA model is equal to $\phi^*(B) = \phi(B) (1-B)^d$, which has $d$ roots that equal 1. Hence an ARIMA($p,d,q$) process cannot be stationary unless $d=0$. -->

<!-- * The parameter $d$ controls the number of times the process is differenced, and must be a non-negative integer. -->

<!-- * When $d=0$ we have an ARMA($p,q$) model, i.e. ARIMA($p, 0, q$) = ARMA($p,q$). -->

<!-- ###[example] -->

<!-- For an ARIMA(0,1,0) process the characteristic polynomials are given by $\phi(B)=1$ and $\theta(B)=1$, meaning that the full model is given by -->

<!-- \begin{eqnarray} -->
<!-- (1-B)X_{t}&=&Z_{t}\nonumber\\ -->
<!-- X_{t}-X_{t-1}&=&Z_{t}\nonumber\\ -->
<!-- X_{t}&=&X_{t-1}+Z_{t}\nonumber -->
<!-- \end{eqnarray} -->

<!-- which is a random walk process. Note that the AR(1) process $X_{t}=\alpha X_{t-1}+Z_{t}$ is also a random walk process when $\alpha=1$. -->
<!-- ###[/example] -->

<!-- ###[example] -->
<!-- For an ARIMA(0,0,0) process the characteristic polynomials are given by $\phi(B)=1$ and $\theta(B)=1$, meaning that the full model is given by -->

<!-- \begin{eqnarray} -->
<!-- (1-B)^{0}X_{t}&=&Z_{t}\nonumber\\ -->
<!-- X_{t}&=&Z_{t}\nonumber -->
<!-- \end{eqnarray} -->

<!-- which is a purely random process. -->
<!-- ###[/example] -->

<!-- ###[example] -->
<!-- For an ARIMA(1,1,1) process the characteristic polynomials are given by $\phi(B)=1-\alpha B$ and $\theta(B)=1+\lambda B$, meaning that the full model is given by -->

<!-- \begin{eqnarray} -->
<!-- (1-\alpha B)(1-B)X_{t}&=&(1+\lambda B)Z_{t}\nonumber\\ -->
<!-- (1-B-\alpha B + \alpha B^{2})X_{t}&=&\lambda Z_{t-1}+Z_{t}\nonumber\\ -->
<!-- X_{t}&=&(1+\alpha)X_{t-1}-\alpha X_{t-2} +\lambda Z_{t-1}+Z_{t}\nonumber -->
<!-- \end{eqnarray} -->

<!-- So an ARIMA(1,1,1) model is essentially a non-stationary ARMA(2,1) model. -->
<!-- ###[/example] -->

<!-- ###[task] -->
<!-- Consider the ARIMA time series process -->

<!-- $$(1-B)(1-0.2B)X_{t}=(1-0.5B)Z_{t}$$ -->

<!-- * What type of process is defined here (i.e. what are $p, d, q$)? -->
<!-- * Write out the model in terms of $X_{t}$. -->
<!-- * Is this process stationary? -->

<!-- ####[answer] -->
<!-- This is an ARIMA(1,1,1) process. -->

<!-- In terms of $X_{t}$ the model is given by -->

<!-- \begin{eqnarray} -->
<!-- (1-B)(1-0.2B)X_{t}&=&(1-0.5B)Z_{t}\nonumber\\ -->
<!-- (1-1.2B+0.2B^{2})X_{t}&=&Z_{t} - 0.5Z_{t-1}\nonumber\\ -->
<!-- X_{t} - 1.2X_{t-1} + 0.2X_{t-2}&=&Z_{t} - 0.5Z_{t-1}\nonumber\\ -->
<!-- X_{t} &=&1.2X_{t-1} - 0.2X_{t-2}- 0.5Z_{t-1} + Z_{t}\nonumber -->
<!-- \end{eqnarray} -->

<!-- This process is not stationary, because the characterisitic equation for the AR($p$) part has an root equal to one (as it is differenced once). Therefore it cannot be stationary. -->
<!-- ####[/answer] -->
<!-- ###[/task] -->

<!-- ###[task] -->
<!-- Consider an ARIMA$(0,2,1)$ process. -->

<!-- * Write out the model in backshift (B) notation. -->
<!-- * Expand this equation and write the model in terms of $X_{t}$. -->

<!-- ####[answer] -->
<!-- In backshift notation the model is given by -->

<!-- $$(1-B)^{2}X_{t}=(1+\lambda B)Z_{t}.$$ -->

<!-- In terms of $X_{t}$ this model is given by -->

<!-- \begin{eqnarray} -->
<!-- (1-B)(1-B)X_{t}=(1+\lambda B)Z_{t}\nonumber\\ -->
<!-- (1-2B + B^{2})X_{t}&=&+\lambda Z_{t-1} + Z_{t}\nonumber\\ -->
<!-- X_{t}-2X_{t-1}+ X_{t-2}&=&+\lambda Z_{t-1} + Z_{t}\nonumber\\ -->
<!-- X_{t}&=&2X_{t-1} -X_{t-2} +\lambda Z_{t-1} + Z_{t}\nonumber -->
<!-- \end{eqnarray} -->

<!-- ####[/answer] -->

<!-- ###[/task] -->

<!-- ###[task] -->
<!-- Consider the following ARIMA processes. -->

<!-- * ARIMA(1,0,0) -->
<!-- * ARIMA(0,0,2) -->
<!-- * ARIMA(0,1,0) -->

<!-- Which of these are stationary? -->

<!-- ####[answer] -->
<!-- ARIMA$(1,0,0)$: We cannot tell as it has an AR(1) component, so its stationarity will depend on the value of the lag one autocorrelation function. -->

<!-- ARIMA$(0,0,2)$: This is a purely a moving average process, and is therefore stationary. -->

<!-- ARIMA$(0,1,0)$: This has been differenced and is therefore not stationary. -->
<!-- ####[/answer] -->

<!-- ###[/task] -->


<!-- ## Model identification -->

<!-- To model trend and correlation in a single model using a non-stationary ARIMA($p,d,q$) process the following four step approach seems reasonable. -->


<!-- 1. **Choose $d$**: Plot the time series and its correlogram, and determine whether the data contain a trend and hence need to be differenced. If there is no trend then choose $d=0$, otherwise difference the data and plot the differenced data. If this looks stationary then choose $d=1$, otherwise difference again and plot the second differences. Repeat this process until the data are stationary. Typically $d=1$ or $d=2$ should be enough to obtain a stationary time series. -->

<!-- 2. **Choose $p$ and $q$**: Plot the ACF and PACF of the $d$th order differences, and determine the appropriate ARMA($p,q$) model. -->

<!-- 3. **Estimate the parameters**: Use the `arima()` function in R to estimate the parameters of the ARIMA process. -->

<!-- 4. **Residual diagnosis**: Look at the time plot, ACF and PACF plot of the residuals and determine whether they contain any remaining trend, seasonal variation or short-term correlation. If the residuals resemble a purely random process then stop, otherwise return to stage one and change either $p$, $d$ or $q$. -->


<!-- ###[example] -->
<!-- Below you see realisations of an ARIMA(1,1,0) (left column) and ARIMA(0,1,1) processes together with the R code. Note the data are non-stationary and have a trend. Therefore the ACF is not informative regarding the presence or absence of short-term correlation. -->

<!-- ```{r, fig.width=6, fig.height=6, fig.align='center'} -->
<!-- ## Simulate ARIMA(1,1,0) and ARIMA(0,1,1) data -->
<!-- data1 <- arima.sim(model=list(ar=c(0.7), order=c(1,1,0)), n=1000, sd=1) -->
<!-- data2 <- arima.sim(model=list(ma=c(0.7), order=c(0,1,1)), n=1000, sd=1) -->

<!-- # Plotting -->
<!-- p1 <- autoplot( data1, main="ARIMA(1,1,0) Residual series") -->
<!-- p2 <- autoplot( data2, main="ARIMA(0,1,1) Residual series") -->

<!-- p1acf <- autoplot( acf(data1, plot = FALSE), main="ARIMA(1,1,0) ACF") -->
<!-- p2acf <- autoplot( acf(data2, plot = FALSE), main="ARIMA(0,1,1) ACF") -->

<!-- p1pacf <- autoplot( pacf(data1, plot = FALSE), main="ARIMA(1,1,0) PACF") -->
<!-- p2pacf <- autoplot( pacf(data2, plot = FALSE), main="ARIMA(0,1,1) PACF") -->

<!-- grid.arrange( p1, p2, p1acf, p2acf, p1pacf, p2pacf, nrow=3) -->
<!-- ``` -->
<!-- ###[/example] -->

<!-- ###[example] -->
<!-- Recall again the daily respiratory admissions data for Glasgow between 2000 and 2007. Here we model the trend and correlation using an ARIMA($p,d,q$) model. The graphs below show the original data and the first order differences, the latter appear to be stationary with zero mean. Therefore $d=1$ appears to be adequate. -->
<!-- **NEED CODE HERE** -->

<!-- The ACF and PACF of the first order differences are given below, and show that the differencing process has induced negative correlation into the data. -->

<!-- **AND HERE** -->

<!-- An MA(1) process appears to be an appropriate model so we fit an ARIMA($0,1,1$) to the original data. This provides the following residuals. -->

<!-- **AND HERE** -->

<!-- These residuals appear to resemble a purely random process with no correlation, so the ARIMA($0,1,1$) model appears to be appropriate. -->
<!-- ###[/example] -->

<!-- <!-- ###[task] --> -->

<!-- <!-- ###[/task] --> -->


## Additional resources on AR and MA processes

###[weblink,target="", icon=book]

[**Time Series Analysis with Applications in R by Cryer and Chan**](http://encore.lib.gla.ac.uk/iii/encore/record/C__Rb2727391):

* **Chapter 4**: Models for stationary time series

<!-- * **Chapter 5**: Models for nonstationary time series -->

[**Time Series Analysis and Its Applications: With R Examples by Shumway and Stoffer**](https://glasgow.summon.serialssolutions.com/#!/search?bookMark=ePnHCXMw42LgTQStzc4rAe_hSmFm4DIGRrMpaIYPNHEIbJBYAItLI0MO-EiIBbDRbGDEyaAJ2vWgABoVSi1WgJ3JoQDsUCt4lgAFkCZ0eRhY8oA9NG4GBTfXEGcPXVBLMz2_PB465BGfBOylm5qArvkjQgkA7q4zFA):

* **Chapter 3**: ARIMA Models

###[/weblink]

\pagebreak

## Week 7 learning outcomes

By the end of this week, you should be able to:

* identify when an autoregressive process is appropriate

* fit autoregressive processes in R to model short-term correlation

* identify when a moving average process is appropriate

* fit moving average processes in R to model short-term correlation
 the partial autocorrelation function

<!-- * recognise when AR or MA processes might not be adequate and explore ARMA processes as an alternative -->

<!-- * fit ARMA processes in R to model short-term correlation -->

<!-- * fit ARIMA processes to remove trend/seasonal patterns and model short-term correlation. -->


