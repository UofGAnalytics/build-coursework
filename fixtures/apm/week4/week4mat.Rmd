
```{r setup, include=FALSE, fig.align='center'}
knitr::opts_chunk$set(comment=NA, warning = FALSE, message = FALSE )
library(ggplot2)
library(gridExtra)
library(knitr)   
library(nnet)


Rmkd_theme <- theme_light()+
        theme(panel.background = element_rect(fill = "transparent", colour = NA),
               plot.background = element_rect(fill = "transparent", colour = NA),
               panel.border = element_rect(fill = NA, colour = "black", size = 1),
               legend.background = element_rect(fill = "transparent", colour = NA))
theme_set(Rmkd_theme)

g_legend <- function(a.gplot){
  tmp <- ggplot_gtable(ggplot_build(a.gplot))
  leg <- which(sapply(tmp$grobs, function(x) x$name) == "guide-box")
  legend <- tmp$grobs[[leg]]
  return(legend)}
```

 
## Models for categorical responses
  
In last week's material we covered GLMs for categorical responses with two possible outcomes. This week, we will generalise this to situations where the response variable is categorical with more than two categories. More specifically, we will look at logistic regression models applied to **nominal** (unordered) or **ordinal** (ordered) responses with **more than two categories**.

The basis for modelling categorical data with more than two categories is the **multinomial distribution**.
  
## Multinomial distribution

###[definition]
Consider a random variable  $Y$ with $J$ categories. Let $p_1,p_2,\dots, p_J$ be the respective probabilities associated with each of the $J$ categories, with $p_1+p_2+\dots+p_J=1$.
Suppose there are $n$ independent observations which result in $y_1$ outcomes in category 1, $y_2$ outcomes in category 2, and so on. Let $\mathbf{y}=(y_1,y_2,\dots,y_J)^\intercal$ with $\sum_{j=1}^J y_j = n$. We say that $\mathbf{y}$ follows a **multinomial distribution** with probability mass function (p.m.f.)
\begin{align}
f(\mathbf{y} | n)=\frac{n!}{y_1! y_2! \dots y_J!}p_1^{y_1}p_2^{y_2}\dots p_J^{y_J}.
\label{eqn:multinomial}
\end{align}
###[/definition]

### Properties of the multinomial distribution

If $J=2$ then $p_2=1-p_1$ and $y_2=n-y_1$ so the expression above reduces to the p.m.f. of the binomial distribution: 
\begin{align*}
f(\mathbf{y} | n)=\frac{n!}{y_1! (n-y_1)!} p_1^{y_1}p_2^{n-y_1}
\end{align*}

For the multinomial distribution, we have the following expressions for the mean, variance and covariance: 
\begin{align*}
\textrm{E}(Y_j)&=np_j\\
\textrm{Var}(Y_j)&=np_j(1-p_j)\\
\textrm{Cov}(Y_j,Y_k)&= -n p_jp_k
\end{align*}

Notice that for $J=2$ you obtain the mean and variance for a binomial random variable. Notice also the negative covariance between $Y_j$ and $Y_k$ due to the sum constraint $\sum_{j=1}^J y_j = n$.

In general, equation (\ref{eqn:multinomial}) does not satisfy the exponential family distribution requirement for the response in a GLM, but we can still fit GLMs to multinomial responses thanks to the following relationship with the Poisson distribution, which is a member of the exponential family.

### Relationship with Poisson distribution

The multinomial distribution is not a member of the exponential family. However, we can still use the multinomial distribution in the GLM context if we view it as the joint distribution of Poisson random variables conditional on their sum $n$.

\pagebreak

###[supplement]

Let $Y_j \sim \text{Po}(\mu_j)$ where the $Y_j$ are independent for $j=1,\dots,J$. Their joint p.m.f. is:
$$ f(\mathbf{y})=\prod_{j=1}^J \frac{\mu_j^{y_j}e^{-\mu_j}}{y_j!}$$

The random variable $n=Y_1+Y_2+\dots+Y_J$ follows the Po$(\mu_1+\mu_2+\dots+\mu_J)$ distribution.

Conditional on $n$, $\mathbf{y}$ has the following distribution: 

\begin{align*}
f(\mathbf{y} |n ) =\frac{\prod_{j=1}^J \mu_j^{y_j}e^{-\mu_j}/y_j!}{(\mu_1+\mu_2+\dots+\mu_J)^ne^{-(\mu_1+\mu_2+\dots+\mu_J)}/n!}
\end{align*}

This can be simplified to: 

\begin{align*}
f(\mathbf{y} | n)= \frac{n!}{y_1! y_2! \dots y_J!} \left(\frac{\mu_1}
{\sum \mu_k}\right)^{y_1}  \left(\frac{\mu_2} {\sum \mu_k}\right)^{y_2} ... \left(\frac{\mu_J}{\sum \mu_k}\right)^{y_J}
\end{align*}

which is the same as the expression for the multinomial p.m.f. in equation (\ref{eqn:multinomial}), if we let $p_j=\dfrac{\mu_j}{\sum_{k=1}^J \mu_k}$.

### [/supplement]


## Nominal logistic regression

Nominal logistic regression, also known as *multinomial logistic regression* is used when there is no natural order among the response categories, for example:

* Eye colour: Blue, Green, Brown, Hazel
* House types: Bungalow, Duplex, Terrace
* Type of pet: Dog, Cat, Rodent, Fish, Bird
* Genotype: AA, Aa, aa

One category is arbitrarily chosen as the reference category, and all other categories are compared with it. Suppose the first category is chosen as the reference category. Then the logits for the other categories are defined by
\begin{align}
\text{logit}(p_j)= \log \left(\frac{p_j}{p_1}\right)=\mathbf{x}^\intercal \boldsymbol{\beta}_j, \hspace{1cm} \text{for } j=2,\dots,J.
\label{eqn:nominalresp}
\end{align}


## Parameter estimation and fitted values

The $J-1$ logit equations (\ref{eqn:nominalresp}) are solved simultaneously to estimate the parameters $\boldsymbol{\beta}_j$.

Given parameter estimates $\hat{\boldsymbol{\beta}}_j$, the linear predictors $\mathbf{x}^\intercal\hat{\boldsymbol{\beta}}_j$ can be calculated. From equation (\ref{eqn:nominalresp}), $\hat{p}_j=\hat{p}_1\exp(\mathbf{x}^\intercal\hat{\boldsymbol{\beta}}_j)$.

Since $\hat{p}_1+\hat{p}_2+\dots+\hat{p}_J=1$, \begin{align}\hat{p}_1=\frac{1}{1+\sum_{j=2}^J\exp(\mathbf{x}^\intercal\hat{\boldsymbol{\beta}}_j)} \label{eqn:p1hat} \end{align} and \begin{align} \hat{p}_j=\frac{\exp(\mathbf{x}^\intercal\hat{\boldsymbol{\beta}}_j)}{1+\sum_{j=2}^J\exp(\mathbf{x}^\intercal\hat{\boldsymbol{\beta}}_j)}.\label{eqn:pjhat} \end{align}

Fitted values (expected frequencies) can be calculated for each covariate pattern by multiplying the estimated probabilities $\hat{p}_j$ by the total frequency of the covariate pattern.

Parameter estimates $\hat{\boldsymbol{\beta}}_j$ depend on the choice of reference category, but fitted values don't.


## Model checking and model comparisons
<!-- Residuals can be used to assess the adequacy of a model.  -->

<!-- ##[definition] -->
<!-- The **Pearson chi-squared** residuals are given by \[r_i=\frac{o_i-e_i}{\sqrt{e_i}},\] where $o_i$ and $e_i$ are the observed and expected frequencies for $i=1,\dots,n$, where $n$ is $J$ times the number of distinct covariate patterns.  -->
<!-- ##[/definition] -->

Summary statistics can be used to assess the adequacy of a model and also to compare models. Some of the statistics we can consider are:

<!-- * the **chi-squared statistic**, $X^2=\sum_{i=1}^nr_i^2$ -->
* the **deviance** $D=2[l(\hat{\boldsymbol{\beta}}_{\max})-l(\hat{\boldsymbol{\beta}})]$ (also referred to as *residual deviance*), where $l(\hat{\boldsymbol{\beta}}_{\max})$ is the maximised log-likelihood for the saturated (full) model and $l(\hat{\boldsymbol{\beta}})$ is the maximised log-likelihood for the model of interest;
* the **likelihood ratio statistic**, which is equal to the difference between the residual deviance for the model of interest and the null deviance (deviance of the model with no predictors included);
* the **Akaike information criterion** $AIC=-2l(\hat{\boldsymbol{\beta}};\mathbf{y})+2p$, which equals the maximised log-likelihood of the model of interest plus a penalty term equal to twice the number of parameters in the model. The reason for this is that we can keep adding predictors to the model to improve the log-likelihood, but the cost is increased model complexity. The penalty term attempts to strike a balance between model complexity and how well the model fits.

If the model fits well, the deviance will be asymptotically $\chi^2(N-p)$, where $N$ is $J-1$ times the number of distinct covariate patterns in the data, and $p$ is the number of parameters estimated.

The likelihood ratio statistic will be asymptotically $\chi^2[p-(J-1)]$ because the null (minimal) model will have one parameter for each logit defined in equation (\ref{eqn:nominalresp}). 

The AIC can be used for model selection: calculate the criterion for each model and choose the one with the smallest value of the AIC.

## Coefficient interpretation in terms of odds ratios

Consider a response with $J$ categories and a single explanatory variable $x$ which denotes whether an exposure factor is present ($x=1$) or not ($x=0$). Let $p_{j,\text{present}}$ be the probability of the $j$th category assuming the exposure is present, $p_{j,\text{absent}}$ the probability of the $j$th category assuming the exposure is absent. The odds ratio for exposure $j$, where $j=2,\dots,J$, relative to the reference category $j=1$ is \[OR_j=\frac{p_{j,\text{present}}/p_{j,\text{absent}}}{p_{1,\text{present}}/p_{1,\text{absent}}}\]

The model \[\log\left(\frac{p_j}{p_1}\right)=\beta_{0j}+\beta_{1j}x, \hspace{1cm} j=2,\dots,J\] gives log odds
\begin{align*}
\log\left(\frac{p_{j,\text{absent}}}{p_{1,\text{absent}}}\right)&=\beta_{0j}, \hspace{1cm} \text{when } x=0\\
\log\left(\frac{p_{j,\text{present}}}{p_{1,\text{present}}}\right)&=\beta_{0j}+\beta_{1j}, \hspace{1cm} \text{when } x=1
\end{align*}

The log of the odds ratio can be written as \[\log OR_j=\log \left(\frac{p_{j,\text{present}}}{p_{1,\text{present}}}\right) -\log\left(\frac{p_{j,\text{absent}}}{p_{1,\text{absent}}}\right) =\beta_{1j}\]

Hence, $OR_j=\exp(\beta_{1j})$ which we estimate by $\exp(\hat{\beta}_{1j})$. 
If $\beta_{1j}=0$ (or equivalently $\exp(\beta_{1j})=1$), the exposure factor has no effect.

We can obtain 95% confidence intervals for $OR_j$ using the formula:
\[\exp[\hat{\beta}_{1j} \pm 1.96 \text{se}(\hat{\beta}_{1j})],\] where 1.96 is the 97.5th percentile of the standard normal distribution. 


Recall that this normal approximation is based on the asymptotic distribution of the MLE $\hat{\boldsymbol{\beta}}$ in a GLM, which is $\hat{\boldsymbol{\beta}} \sim N(\boldsymbol{\beta}, \mathcal{I}^{-1})$ where $\boldsymbol{\beta}$ is the true parameter vector and $\mathcal{I}^{-1}$ is the inverse of the information matrix.

A confidence interval which does not include 1 corresponds to a significant $\beta$.

##[video, videoid="SpQM2wTqpx8", duration="11m52s"] Nominal logistic regression

##[example] Car preference data

In this example we look at data on subjects that were interviewed about the importance of various features when buying a car.[^1]

[^1]: Source: McFadden, M. J. Powers, W. Brown, and M. Walker (2000). Vehicle and driver attributes affecting distance from the steering wheel in motor vehicles, *Human Factors 42*, 676-682. 

We focus in particular on the importance of power steering and air conditioning. The variables available in this dataset are:

* `sex`: woman/man
* `age`: 18-23, 24-40, >40
* `response`: no/little, important, very important

```{r, results = 'hide'}
dcars <- read.csv(url("http://www.stats.gla.ac.uk/~tereza/rp/cars.csv"))
dcars$response <- factor(dcars$response, 
                         levels = c("no/little", "important", "very important")) 
dcars$age <- factor(dcars$age, 
                         levels = c("18-23", "24-40", "> 40")) 
```

<!-- ```{r, echo = FALSE, results='hide'} -->
<!-- # make response and age factors otherwise the plots puts the categories in alphabetical order which is annoying to look at and to interpret. This also sorts out the output problem - it is now the same as in the slides, with "no/little" and "18-23" as reference categories. -->
<!-- dcars <- read.csv(url("http://www.stats.gla.ac.uk/~tereza/rp/cars.csv")) -->
<!-- dcars$response <- factor(dcars$response,  -->
<!--                          levels = c("no/little", "important", "very important"))  -->
<!-- dcars$age <- factor(dcars$age,  -->
<!--                          levels = c("18-23", "24-40", "> 40"))  -->
<!-- ``` -->


<!-- knitr kable still puts page breaks everywhere so I made the table in latex :(-->

<!-- ```{r, echo = FALSE, results = 'asis'} -->
<!--  knitr::kable(dcars, format = "markdown", padding = 2) -->
<!-- ``` -->

<!-- |sex    |age    |response        |  frequency| -->
<!-- |:------|:------|:---------------|----------:| -->
<!-- |women  |18-23  |no/little       |         26| -->
<!-- |women  |18-23  |important       |         12| -->
<!-- |women  |18-23  |very important  |          7| -->
<!-- |women  |24-40  |no/little       |          9| -->
<!-- |women  |24-40  |important       |         21| -->
<!-- |women  |24-40  |very important  |         15| -->
<!-- |women  |> 40   |no/little       |          5| -->
<!-- |women  |> 40   |important       |         14| -->
<!-- |women  |> 40   |very important  |         41| -->
<!-- |men    |18-23  |no/little       |         40| -->
<!-- |men    |18-23  |important       |         17| -->
<!-- |men    |18-23  |very important  |          8| -->
<!-- |men    |24-40  |no/little       |         17| -->
<!-- |men    |24-40  |important       |         15| -->
<!-- |men    |24-40  |very important  |         12| -->
<!-- |men    |> 40   |no/little       |          8| -->
<!-- |men    |> 40   |important       |         15| -->
<!-- |men    |> 40   |very important  |         18| -->

\begin{center}
\begin{tabular}{rlllr}
  \hline
 & sex & age & response & frequency \\ 
  \hline
1 & women & 18-23 & no/little &  26 \\ 
  2 & women & 18-23 & important &  12 \\ 
  3 & women & 18-23 & very important &   7 \\ 
  4 & women & 24-40 & no/little &   9 \\ 
  5 & women & 24-40 & important &  21 \\ 
  6 & women & 24-40 & very important &  15 \\ 
  7 & women & $>$ 40 & no/little &   5 \\ 
  8 & women & $>$ 40 & important &  14 \\ 
  9 & women & $>$ 40 & very important &  41 \\ 
  10 & men & 18-23 & no/little &  40 \\ 
  11 & men & 18-23 & important &  17 \\ 
  12 & men & 18-23 & very important &   8 \\ 
  13 & men & 24-40 & no/little &  17 \\ 
  14 & men & 24-40 & important &  15 \\ 
  15 & men & 24-40 & very important &  12 \\ 
  16 & men & $>$ 40 & no/little &   8 \\ 
  17 & men & $>$ 40 & important &  15 \\ 
  18 & men & $>$ 40 & very important &  18 \\ 
   \hline
\end{tabular}
\end{center}


From the plots of the data below, we can see that quite a large proportion of people -- a little over 58% in the over 40 category considered the features *very important* and, similarly 60% of young people (18-23 years old) considered these features as having *no or little importance*. Sex also seems to have an impact on car feature preferences, with over 40% of men considering the features of *no or little importance* and over 40% of women considering them *very important*.

```{r}
p1 <- ggplot(dcars, aes(x = age, y = frequency, fill = response)) +
      geom_bar(stat = "identity", position = "dodge" )+
      xlab("Age groups" ) + ylab("Frequency" ) +
      theme(legend.position = "none")
p2 <- ggplot(dcars, aes(x = sex, y = frequency, fill = response)) +
      geom_bar(stat = "identity", position = "dodge" )+
      xlab("Sex" ) + ylab("Frequency" ) +
      scale_fill_discrete(name = "Response category") + 
      theme(legend.position = "bottom")
```

```{r, echo=FALSE}
carsLeg <- g_legend(p2)
grid.arrange(arrangeGrob(p1 + theme(legend.position="none"),
                          p2 + theme(legend.position="none"),
                          nrow=1), nrow = 2, carsLeg, heights=c(10, 1))
```


Although the response is really an ordinal variable, we will begin by treating it as nominal with “no/little importance” as the reference category (also occasionally referred to as "unimportant" in the rest for brevity.) Later on we will also fit an ordinal model. Similarly we will initially regard `age` as nominal. 

We can fit the following **nominal logistic regression model** using the `multinom()` function from `library(nnet)`:
 \[\log\left(\frac{p_j}{p_1} \right) = \beta_{0j}+\beta_{1j}x_1+\beta_{2j}x_2+\beta_{3j}x_3, \hspace{1cm} j=2,3,\] where

 *  $j=1$ for “no/little importance” (the reference category)
 *  $j=2$ for “important”
 *  $j=3$ for “very important”
 *  $x_1=1$ for women and 0 for men,
 *  $x_2=1$ for age 24-40 years and 0 otherwise 
 *  $x_3=1$ for age $>$40 years and 0 otherwise.

<!-- # ```{r, echo = FALSE, results='hide'} -->
<!-- # # resetting this for the models -->
<!-- # dcars$response <- factor(dcars$response,  -->
<!-- #                          levels = c("no/little", "important", "very important"))  -->
<!-- # dcars$age <- factor(dcars$age,  -->
<!-- #                          levels = c("> 40","18-23", "24-40"))  -->
<!-- # ``` -->

```{r}
m1 <- multinom(response ~ age + sex, weight = frequency, data = dcars)
summary(m1)
```


Notice the two sets of coefficients, for the categories "important" and "very important" that correspond to the two logit equations comparing these to the baseline, which is "no/little importance".

We can interpret these coefficients in terms of odds for each logit equation. For example:
```{r}
exp(1.587703)
```
is the odds multiplier when comparing "important" versus "no/little importance" for age group >40 compared to age group 18-23. The positive coefficient (or greater than 1 odds multiplier) tells us that older people are more likely to consider the features important than young people, which is consistent with what we observed in the exploratory plots. The precise interpretation of the odds ratio is as follows: The odds of considering the features important (versus "no/little importance") for over 40 year-olds are 4.89 times the odds for 18-23 year olds.

Similarly, we can calculate the odds multiplier for comparing "important" versus "no/little importance" for age group 24-40 compared to age group 18-23:
```{r}
exp(1.128266)
```
and the positive coefficient also indicates these odds are larger than the baseline category. 

The positive coefficient for women (corresponding to an odds multiplier greater than 1):
```{r}
exp(0.3881269)
```
indicate that women are more likely than men to consider the features important. 

We can calculate approximate 95% confidence intervals for these odds multipliers to see which of these differences are actually significant. 
```{r}
exp(c(0.3881269-1.96*0.3005110,  0.3881269+1.96*0.3005110))
``` 

In the table below we have a summary of the coefficients, odds ratios and confidence intervals for the logit equation corresponding to "important" versus "no/little importance". The odds ratio comparing women to men is not significant, but all the odds ratios comparing age groups are.

\begin{center}
 \begin{tabular}{lrcc}
 \hline
 Parameter, $\beta$ & Estimate, $\hat{\beta}$ & \multicolumn{2}{c}{$OR=e^{\hat{\beta}}$} \\
 & (std. error) &\multicolumn{2}{c}{ (95\% confidence interval)}\\
 \hline
 \multicolumn{4}{l}{$\log(p_2/p_1)$: important vs. no/little importance}\\
 $\beta_{02}$: constant & -0.979 (0.256) & & \\
  $\beta_{12}$: women & 0.388 (0.301) &1.47 & (0.82, 2.66) \\
    $\beta_{22}$: 24-40 & 1.128 (0.342) &3.09 & (1.58, 6.04)\\
 $\beta_{32}$: >40 & 1.590 (0.403) &4.90 & (2.22, 10.78)\\
    \hline
 \end{tabular}
\end{center}

##[/example]

###[task]
Fill in the table below with the relevant odds multipliers and 95% confidence intervals for the "very important" versus "no/little importance" logit equation.  

\begin{center}
 \begin{tabular}{lrcc}
 \hline
 Parameter, $\beta$ & Estimate, $\hat{\beta}$ & \multicolumn{2}{c}{$OR=e^{\hat{\beta}}$} \\
 & (std. error) &\multicolumn{2}{c}{ (95\% confidence interval)}\\
 \hline
 \multicolumn{4}{l}{$\log(p_3/p_1)$: very important vs. no/little importance}\\
  $\beta_{03}$: constant & -1.852 (0.331) & & \\
  $\beta_{13}$: women & 0.813 (0.321) &? & (?, ?) \\
  $\beta_{23}$: 24-40 & 1.478 (0.401) &? & (?, ?)\\
  $\beta_{33}$: >40 & 2.917 (0.423) &? & (?, ?)\\
    \hline
 \end{tabular}
\end{center}

####[answer]

We calculate the odds ratios and corressponding 95% CI as follows:
```{r}
# women Odds Ratio 
exp(0.813)
# women 95% CI Odds Ratio
exp(c(0.813-1.96*0.321, 0.813+1.96*0.321)) 


# 24-40 Odds Ratio  
exp(1.478)
# 24-40 95% CI Odds Ratio
exp(c(1.478-1.96*0.401, 1.478+1.96*0.401)) 
# >40 Odds Ratio  
exp(2.917)
# >40 95% CI Odds Ratio
exp(c(2.917-1.96*0.423, 2.917+1.96*0.423)) 

```

\begin{center}
 \begin{tabular}{lccc}
 \hline
 Parameter, $\beta$ & Estimate, $\hat{\beta}$ & \multicolumn{2}{c}{$OR=e^{\hat{\beta}}$} \\
 & (std. error) &\multicolumn{2}{c}{ (95\% confidence interval)}\\
 \hline
 \multicolumn{4}{l}{$\log(p_3/p_1)$: very important vs. no/little importance}\\
  $\beta_{03}$: constant & -1.852 (0.331) & &  \\
  $\beta_{13}$: women & 0.813 (0.321) &2.25 & (1.20, 4.23) \\
  $\beta_{23}$: 24-40 & 1.439 (0.401) &4.38 & (2.00, 9.62)\\
  $\beta_{33}$: >40 & 2.917 (0.423) &18.48 & (8.07, 42.34)\\
    \hline
 \end{tabular}
\end{center}

####[/answer]

###[/task]

## Model comparisons for the car preference data

We can compare the nominal logistic regression model with additive terms for `age` and `sex` with the null model by taking the difference in deviances (likelihood ratio test).

The null model can be fit as follows:

```{r}
nullm <- multinom(response ~ 1, data=dcars, weights=frequency)
summary(nullm)
```

The difference in deviance is $658.54-580.70=77.84$ which is significant when compared with a $\chi^2(8-2)$:

```{r}
qchisq(df=6, p=0.95)
```
Overall, the explanatory variables are descriptive of car preferences.

We can also compare this model with the saturated (full) model, which includes an interaction between `age` and `sex`:

```{r}
m2 <- multinom(response ~ age * sex, weight = frequency, data = dcars)
summary(m2)
```

The difference in deviance between the additive and the saturated model is $580.70-576.76=3.94$. This is not significant when compared with a $\chi^2(12-8)$, so the additive model appears to fit the data well. 

```{r}
qchisq(df=4, p=0.95)
```

The same conclusion is supported when comparing the AIC for these models: the additive model has a smaller AIC of 596.7 compared to the interaction model which has AIC of 600.7.

###[task] 
Fit a nominal logistic regression model with a linear term for `age` (create a new `agelin` variable taking values 0, 1 and 2 corresponding to 18-23, 24-40 and >40). Compare this nominal logistic regression model to the model with `age` as a categorical predictor. Which model would you choose and why?

####[answer]
First, construct a linear term for `age`:
```{r}
dcars$agelin <- 0
dcars$agelin[dcars$age=="24-40"] <- 1
dcars$agelin[dcars$age=="> 40"] <- 2
```

Then fit the model with this new term:
```{r}
m3 <- multinom(response ~ agelin + sex, weight = frequency, data = dcars)
summary(m3)
```
The model with a linear term for age fits the data almost as well as that with age as a factor, and has two fewer parameters. As the models are nested, we can use the difference deviance between them to choose a model: 

```{r}
m3$deviance-m1$deviance
qchisq(df=2, p=0.95)
```
As $1.40 < 5.99$, we can go with the simpler model with the linear term in `age`. This assumes that the odds multiplier is the same when comparing 24-40 year olds to 18-23 year olds and over 40s to 24-40 year olds.


####[/answer]

###[/task]

## Ordinal logistic regression 

In the car preferences example there was a natural ordering among the response categories for the importance of power steering and air conditioning when buying a car: "no/little importance", "important", "very important". This ordering can be taken into account in the model specification. Such ordering often arises in market research, opinion polls and questionnaires (*e.g.* student feedback at the University of Glasgow).

### Latent variable view of ordered responses

Sometimes an ordinal response could arise if there is a continuous variable $Z$, such as severity of disease, which is hard to measure. $Z$ is a **latent variable**, because it cannot be observed directly. Instead, cutpoints $C_j$ are identified so that, for instance, patients have "no disease", "mild disease", "moderate disease" or "severe disease" corresponding to values of $Z$ from low to high. $C_1, \dots, C_{J-1}$ identify $J$ ordered categories with associated probabilities $p_1,p_2,\dots, p_J$. An example of the continuous distribution of $Z$ with cutpoints for four categories is shown below. Here, four discrete responses can occur depending on the position of $Z$ relative to the cutpoints $C_j$.

```{r echo=FALSE, out.width='90%', fig.align='center'}
knitr::include_graphics('latent.png')
```

### Proportional odds logistic regression model

There are several ways in which to model logits involving the probabilities $p_j$. The most commonly used model is the **proportional odds logistic regression model**. If the linear predictor $\boldsymbol{x}^\intercal \boldsymbol{\beta}_j$ has an intercept term $\beta_{0j}$ which depends on category $j$, but the other explanatory variables do not depend on $j$, then the model is 
 \begin{align*}
             \log \left(\frac{p_1+p_2+\dots+p_j}{p_{j+1}+\dots+p_J}\right)=\beta_{0j}+\beta_1 x_1 + \dots + \beta_{p-1}x_{p-1}.                        
\end{align*}

This is called the \textbf{proportional odds model} and is based on the assumption that the effects of the covariates $x_1,\dots, x_{p-1}$ are the same for all categories on the logarithmic scale, as illustrated in the figure below.

```{r echo=FALSE, fig.width=5, fig.asp=1.1, fig.align='center'}
    
    par(mar=c(0,0,0,0))

    x <- seq(from=0, to =1,length=100)
    y1 <- 1+2*x
    y2 <- 2+2*x
    y3 <- 3+2*x
    plot(x,y3, type="l", xaxt="n",yaxt="n", xlab="", ylab="",ylim=c(-1,7.5),xlim=c(-0.1,1.1), ax=FALSE)
    mtext(expression(bold("x")),side=1,line=-3)
    mtext(expression(bold("Log odds")),side=2,line=-1.5)
    axis(1, labels = FALSE, pos=0,lwd.tick=0)
    axis(2, labels = FALSE, pos=0,lwd.tick=0)
    lines(x,y2)
    lines(x,y1)
    text(x=0,y=1, pos=2, expression(beta["03"]))
    text(x=0,y=2, pos=2, expression(beta["02"]))
    text(x=0,y=3, pos=2, expression(beta["01"]))
    text(x=1,y=3, pos=4, expression(italic("j=3")))
    text(x=1,y=4, pos=4, expression(italic("j=2")))
    text(x=1,y=5, pos=4, expression(italic("j=1")))

```

###[supplement] 

Some alternatives to the proportional odds model for ordinal responses are given below.

* Cumulative logit model

The cumulative odds for the $j$th category are 
$$\frac{\Pr(Z\leq C_j)}{\Pr(Z>C_j)}=\frac{p_1+p_2+\dots+p_j}{p_{j+1}+\dots+p_J}$$
The cumulative logit model is $$\log \left(\frac{p_1+p_2+\dots+p_j}{p_{j+1}+\dots+p_J}\right)=\boldsymbol{x}^\intercal \boldsymbol{\beta}_j.$$     

* Adjacent categories logit model

If we consider ratios of probabilities, *e.g.* $\frac{p_1}{p_2}, \frac{p_2}{p_3},\dots, \frac{p_{J-1}}{p_J}$ we can define the adjacent category logit model as 
 \begin{align*}
   \log \left(\frac{p_j}{p_{j+1}}\right)=\boldsymbol{x}^\intercal \boldsymbol{\beta}_j, \hspace{1cm} \text{for } j=1,\dots,J-1.  
 \end{align*}
If this is simplified to \begin{align*}
   \log \left(\frac{p_j}{p_{j+1}}\right)=\beta_{0j}+\beta_1 x_1 + \dots + \beta_{p-1}x_{p-1}, 
 \end{align*}
 the effect of each explanatory variable is assumed to be the same for all adjacent pairs of categories.

* Continuation ratio logit model

Another alternative is to consider the ratios of probabilities $\frac{p_1}{p_2}, \frac{p_1+p_2}{p_3},\dots, \frac{p_1+\dots+p_{J-1}}{p_J}$ or $\frac{p_1}{p_2+\dots+p_J}, \frac{p_2}{p_3+\dots+p_J},\dots, \frac{p_{J-1}}{p_J}$.

The equation \begin{align*}
   \log \left(\frac{p_j}{p_{j+1}+\dots+p_J}\right)=\boldsymbol{x}^\intercal \boldsymbol{\beta}_j
 \end{align*}
 models the odds of the response being in category $j$, i.e. $C_{j-1}<Z\leq C_j$ conditional upon $Z>C_{j-1}$.
 
 
For instance, in the car preferences data example we could estimate the odds of respondents regarding air conditioning and power steering as "unimportant" vs. "important" or "very important" using $$\log \left(\frac{p_1}{p_2+p_3}\right).$$

Similarly, the odds of these features being "very important" given that they are "important" or "very important" can be estimated by $$\log \left(\frac{p_2}{p_3}\right).$$

###[/supplement]

##[video, videoid="d9DSd33sUhw", duration="5m35s"] Ordinal logistic regression

\pagebreak

##[example] Proportional odds logistic regression model for the car preference data

Looking at the car preference example again, we can fit the response as an ordinal variable using a proportional odds model of the form:

\begin{align} \log \left(\frac {p_1} {p_2+p_3} \right)= \beta_{01} + \beta_{1}x_1 + \beta_{2}x_2 +\beta_{3}x_3 \label{eqn:polr1} \end{align}
\begin{align} \log \left(\frac {p_1+p_2} {p_3}\right) = \beta_{02} + \beta_{1}x_1 + \beta_{2}x_2 +\beta_{3}x_3 \label{eqn:polr2} \end{align}

where $j=1$ for "no/little importance" (also referred to as "unimportant"), $j=2$ for "important" and $j=3$ for "very important", $x_1 =1$ for women and 0 for men, $x_2 = 1$ for age 24-40 years and 0 otherwise and $x_3 = 1$ for age $> 40$ and 0 otherwise.

We fit this model using the `polr()` function in `library(MASS)`, which, incidentally, uses the parameterisation

\begin{align} \log \left(\frac {p_1} {p_2+p_3} \right)= \beta_{01} - \beta_{1}x_1 - \beta_{2}x_2 -\beta_{3}x_3 \label{eqn:polr3} \end{align}
\begin{align} \log \left(\frac {p_1+p_2} {p_3}\right) = \beta_{02} - \beta_{1}x_1 - \beta_{2}x_2 -\beta_{3}x_3 \label{eqn:polr4} \end{align}

instead of (\ref{eqn:polr1}) and (\ref{eqn:polr2}).

```{r}
library(MASS)
m4 <- polr(response ~ sex + age, data= dcars, weight = frequency, Hess=TRUE)
summary(m4)
```

The intercepts correspond to the $j$th category, so the log odds of considering the features “unimportant” is 0.620 corresponding to a probability of 0.65 for men age 18-23. The log odds of considering the features “unimportant” or “important” is 2.231 corresponding to a probability of 0.903, giving a probability of 0.253 of considering the features “important”. This leaves a probability of 0.097 of considering the features “very important” for men age 18-23. These probabilities are calculated using equations (\ref{eqn:polr3}) and (\ref{eqn:polr4}) together with $p_1+p_2+p_3=1$. For instance for men age 18-23 (baseline) we get $$\hat{p}_1=\frac{\exp(\hat{\beta}_{01})}{1+\exp(\hat{\beta}_{01})}=\frac{\exp(0.6198)}{1+\exp(0.6198)}=0.650$$ and $$\hat{p}_3=\frac{1}{1+\exp(\hat{\beta}_{02})}=\frac{1}{1+\exp(2.2312)}=0.097.$$


The likelihood ratio chi-squared statistic for the proportional odds model is 77.25, and the AIC is 591.3, both very similar to those obtained from the corresponding nominal logistic regression model (77.84 and 596.70 respectively). 
There is little difference in how well the proportional odds and nominal logistic regression models describe the data. 

##[/example]

###[task]
Fit a proportional odds logistic regression model with age as an ordered variable with values 0, 1 and 2 corresponding to 18-23, 24-40 and >40. Is this model preferable to the model with age as a factor?

####[answer]

Model with a linear term for age:

```{r echo=FALSE, results='hide'}
dcars <- read.csv(url("http://www.stats.gla.ac.uk/~tereza/rp/cars.csv"))
dcars$response <- factor(dcars$response, 
                         levels = c("no/little", "important", "very important")) 
dcars$age <- factor(dcars$age, 
                         levels = c("18-23", "24-40", "> 40")) 
dcars$agelin <- 0
dcars$agelin[dcars$age=="24-40"] <- 1
dcars$agelin[dcars$age=="> 40"] <- 2
```

```{r}
m5 <- polr(response ~ sex + agelin, data= dcars, weight = frequency, Hess=TRUE)
summary(m5)
```

The parameter estimates from the proportional odds model are very similar to those of the nominal regression model, whether age is included as a factor or as an ordered variable. Fitted probabilities for each covariate pattern are also very similar. The proportional odds model would be preferred because it fits the data as well as the nominal regression model but uses fewer parameters, and because it takes into account the ordinal nature of the response.
####[/answer]
###[/task]


###[task]
The `housing` dataset from `library(MASS)` shows a four-way classification of 1681 householders in Copenhagen who were surveyed on the type of rental accommodation they occupied, the degree of contact they had with other residents, their feeling of influence on apartment management and their level of satisfaction with their housing conditions. The response, `Sat`, gives the satisfaction of householders with their present housing circumstances, (High, Medium or Low). The explanatory variables are 

* `Infl`, the perceived degree of influence householders have on the management of the property (High, Medium, Low), 

* `Type`, the type of rental accommodation (Tower, Atrium, Apartment, Terrace), and 

* `Cont`, the contact residents are afforded with other residents (Low, High).

Fit nominal and ordinal logistic regression models to these data and interpret the results.

####[answer]

First fit a nominal logistic regression model:

``` {r}
library(MASS)
library(nnet)
house.nom <- multinom(Sat ~ Infl + Type + Cont, weights = Freq, data = housing)
summary(house.nom, digits=3)
```
The baseline type of housing used for comparisons is tower block, with low influence on apartment management and low contact with other residents. The positive coefficients of `Infl` (medium and high) and `Cont` indicate that satisfaction increases with the feeling of influence and with more contact with other residents. Other types of housing are associated with lower satisfaction ratings than tower block (with the exception of atrium which has a non-significant coefficient). 

Given the ordinal nature of the response, we can fit a proportional odds model to see if it fits the data just as well as the nominal logistic regression.

``` {r}
library(MASS)
house.plr <- polr(Sat ~ Infl+Type+Cont, weights=Freq, data=housing, Hess=TRUE)
summary(house.plr, digits = 3)
```

The results obtained from a proportional odds regression are qualitatively similar: in general satisfaction increases with influence and contact. Tower block seems to be the type of housing with the highest satisfaction ratings, followed by atrium, apartment and terrace. The proportional odds model is simpler and takes into account the ordinal nature of the response variable.

The following code produces approximate confidence intervals for the coefficients of the proportional odds model: 

``` {r}
confint(house.plr)
```
We can convert to odds ratios by exponentiating:

```{r}
round(exp(confint(house.plr)),2)
```
Interpretation for the Influence coefficients: The odds of higher satisfaction (low to medium/medium to high) are between 1.44 and 2.16 times higher for medium influence compared to low influence, and between 2.83 and 4.66 times higher for high influence compared to low influence. The other coefficients are interpreted similarly.

####[/answer]

###[/task]

Finally let us look at an application of nominal logistic regression to a classification problem.

## Nominal logistic regression as a classification tool

We can use the predicted probabilities from a nominal logistic regression model to classify an observation to the category with the highest predicted probability.


##[example]  Iris data

This is a very famous example used for illustrating various classification methods. The data set gives the measurements in centimeters of the variables sepal length and width and petal length and width, respectively, for 50 flowers from each of three species of iris: setosa, versicolor, and virginica. Suppose that we have these measurements from an iris and we wish to classify it into one of the three species. We can fit a nominal logistic regression model to the data and predict the probability of each species from the fitted model. Let us start with a model with only sepal length as the predictor.

```{r}
library(nnet)
m.iris <- multinom(Species ~ Sepal.Length, data=iris)
summary(m.iris)
```

We can get predicted probabilities from this model by using `fitted(m.iris)`. Here are the first few values:

```{r}
head(round(fitted(m.iris),3))
```

And if we use `predict(m.iris)`, we get a list which assigns each observation to the category with the highest predicted probability, for instance the first observation is assigned to `setosa` with predicted probability = 0.807. Here are the first few values:

```{r}
head(predict(m.iris))
```

We can see how many predictions the model got wrong:

```{r}
sum(iris$Species!=predict(m.iris))
```

Of course, if we were to properly assess the classification performance of this model, we should look at out-of-sample prediction by first splitting the data into a training and a test set, then fitting the model to the training data and finally predicting the class of the each observation in the test data. Otherwise we run the risk of overstating the classification accuracy of the model.
##[/example]

###[task] 

Fit a nominal logistic regression model to the iris data using all four predictors. Do you get better classification performance?

####[answer]
Without increasing the maximum number of iterations you would get a warning about lack of convergence.

```{r, warning=TRUE}
m.iris.all <- multinom(Species ~ . , data=iris)
```

The maximum number of iterations can be increased using `maxit`:

```{r, warning=TRUE}
m.iris.all <- multinom(Species ~ . , data=iris, maxit=1000)
summary(m.iris.all)
```

Notice the large standard errors in the model output. These point to perfect prediction/separation. Check:

```{r}
sum(iris$Species!=predict(m.iris.all))
```
All but two observations appear to have been predicted correctly.

The function `glmnet()` might be helpful for dealing with separation.

####[/answer]
###[/task]

###[weblink,target="", icon=book]

More examples and details on GLMs for nominal and ordinal data can be found in 

* Chapter 5 from [**Extending linear models with R: generalized linear, mixed effects and nonparametric regression models by Julian Faraway**](http://encore.lib.gla.ac.uk/iii/encore/record/C__Rb2939999) and in 

* Chapter 6 of [**Regression: models, methods and applications by Fahrmeir et al. **](http://encore.lib.gla.ac.uk/iii/encore/record/C__Rb2991222)

R examples are also available from UCLA's Institute for Digital Research and Education:

* 
[Nominal logistic regression example](https://stats.idre.ucla.edu/r/dae/multinomial-logistic-regression/)

* 
[Ordinal logistic regression example](https://stats.idre.ucla.edu/r/dae/ordinal-logistic-regression/)

###[/weblink]


## Week 4 learning outcomes

* Identify categorical responses as nominal or ordinal

* Fit nominal logistic regression models to both nominal and ordinal data using the `multinom()` function in `library(nnet)`

* Fit ordinal logistic regression models for ordered responses using the `polr()` function in `library(MASS)`

* Choose a model by comparing deviances and/or AIC 

* Interpret model coefficients in terms of odds ratios

* Obtain predicted probabilities and fitted values from a nominal logistic regression model or a proportional odds model

* Use nominal logistic regression as a classification tool