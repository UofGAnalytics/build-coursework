# Introduction to Generalised Linear Models

## Introduction and motivating examples

```{r,echo=FALSE, include=FALSE, fig.height=4}
knitr::opts_chunk$set(comment=NA, warning = FALSE)
```

##[video, videoid="5u1w6eROypI", duration="9m57s"] Introduction to GLMs


<!-- In this course we will extend the theory of linear regression models, covered in [Predictive Modelling](http://moodle2.gla.ac.uk/course/view.php?id=12738) and [Learning from Data](http://moodle2.gla.ac.uk/course/view.php?id=12736).  -->

In this course we will extend the theory of linear regression models to non-normal responses. Before we begin with introducing the class of models known as Generalised Linear Models (GLMs), we will briefly illustrate why linear models are not sufficient for all types of data. Throughout the course, we will see how we can deal with a variety of situations where the linear model may not be adequate.

The main objective of this week's learning material is to introduce Generalised Linear Models (GLMs), which extend the linear model framework to outcome variables that don't follow the normal distribution. GLMs can be used to model non-normal continuous outcome variables, but they are most frequently used to model **binary**, **categorical** or **count data**. We will focus on these latter types of outcome variables. To see why extensions to the normal linear model are needed, let's look at a couple of examples, one where the normal linear model is appropriate and one where it's not.


###[example]Bollywood box office revenue

Possibly the simplest scenario of a predictive model is when we want to predict an outcome variable based on a predictor which displays a linear relationship to the variable of interest. Consider the following dataset on Bollywood film revenues (sourced from http://www.bollymoviereviewz.com](http://www.bollymoviereviewz.com)) which contains data on 190 films made during the period 2013-2017. We would like to predict the gross revenue of a film from the film's budget. Both the gross revenue and the budget are measured in [crore](https://en.wikipedia.org/wiki/Crore). Here are the first few rows of the data:

```{r, results = 'hide'}
bollywood <-
  read.csv(url("http://www.stats.gla.ac.uk/~tereza/rp/bollywood_boxoffice.csv"))
head(bollywood)
```

   Movie                            Gross       Budget
   -------------------------------- ----------- -----------
     Ek Villain                       95.64       36.0
     Humshakals                       55.65       77.0
     Holiday                         110.01      90.0
     Fugly                            11.16       16.0
     City Lights                       5.19        9.5
     Kuku Mathur Ki Jhand Ho Gayi      2.23        4.5


We can plot the gross revenue against the budget to explore the relationship between the two variables.


```{r, echo=FALSE, include=FALSE, fig.height=4.5}
library(ggplot2)
```

```{r, results='hide', fig.height=4}
b.plot <- ggplot(data = bollywood, aes(y = Gross, x = Budget)) +
          geom_point(col = "#66a61e") +
          scale_x_continuous("Budget (crore)") + scale_y_continuous("Gross (crore)")
```

```{r, echo=FALSE}
b.plot + theme(panel.background = element_rect(fill = "transparent", colour = NA),
               plot.background = element_rect(fill = "transparent", colour = NA),
               panel.border = element_rect(fill = NA, colour = "black", size = 1))
```


Looking at the scale of the values on both the horizontal and vertical axes, we might want to transform the data by taking logs.

```{r, results='hide', fig.height=4}
b.plot.l <- ggplot(data = bollywood, aes(y = log10(Gross), x = log10(Budget))) +
          geom_point(col = "#1b9e77") +
          scale_x_continuous("log(Budget) (crore)") +
          scale_y_continuous("log(Gross) (crore)")
```

```{r, echo=FALSE, fig.height=4.5}
b.plot.l + theme(panel.background = element_rect(fill = "transparent", colour = NA),
                 plot.background = element_rect(fill = "transparent", colour = NA),
                 panel.border = element_rect(fill = NA, colour = "black", size = 1))
```

Now let's fit a model with the $\log_{10}$ transformed gross revenue as the response ($Y_{i}$)  and the $\log_{10}$ transformed budget ($x_{i}$) as the explanatory/predictor variable. We can use the `lm()` function to fit this linear model in R.



```{r}
bol.lm <- lm(log10(Gross) ~ log10(Budget), data = bollywood)
```

The model equation in mathematical notation is
$$E(Y_{i}) = \mu_i= \beta_0 + \beta_1 x_{i}; ~~~ \text{where the}~ Y_{i} \text{ are independent }  N(\mu_{i}, \sigma^2),~~~i=1,...,190 $$
The model fit is shown below:

```{r }
summary(bol.lm)
```

We can visualise this regression model by plotting the data and fitted regression line:


```{r, results='hide', fig.height=4.5}
b.plot.lm <- ggplot(data = bollywood, aes(y = log10(Gross), x = log10(Budget))) +
          geom_point(col ="#1b9e77") +
          scale_x_continuous("log(Budget) (crore)") +
          scale_y_continuous("log(Gross) (crore)") +
          geom_smooth(method = lm, colour="#e7298a", se=FALSE)
```

```{r, echo=FALSE, fig.height=4.5}
b.plot.lm + theme(panel.background = element_rect(fill = "transparent", colour = NA),
                 plot.background = element_rect(fill = "transparent", colour = NA),
                 panel.border = element_rect(fill = NA, colour = "black", size = 1))
```


###[/example]

###[task]
Using the fitted model equation, predict the gross revenue for a film with a budget of (i) 10, (ii) 50, and (iii) 100 crore.

*Hint: Remember that the variables have been log-transformed.*

####[answer]
For the *Bollywood box office revenue* example, we can write down the fitted model equation from the `summary(bol.lm)`:
$$\log10(\text{Gross}) =-0.62549 + 1.31955\times \log10(\text{Budget})$$

We can use this equation to predict the gross revenue of a film by simply substituting the relevant budget value, and transforming the result from the log10 scale. Thus:

(i) budget = 10:
 $$\log10(\text{Gross}) =-0.62549 + 1.31955\times \log10(10) =  0.69406  \Rightarrow \text{Gross}= 10^{0.69406} = 4.94379$$


(ii) budget = 50:
 $$\log10(\text{Gross}) =-0.62549 + 1.31955\times \log10(50) =  1.616386  \Rightarrow \text{Gross}= 10^{1.616386} = 41.34148$$


(iii) budget = 100:
  $$\log10(\text{Gross}) =-0.62549 + 1.31955\times \log10(100) =  2.01361 \Rightarrow \text{Gross}= 10^{2.01361} = 103.1834$$

####[/answer]
###[/task]


###[example] GPA and admission to medical school
Now let's look at a different kind of dataset, where the outcome we want to predict is not continuous-valued but binary. This is a dataset on admissions to US medical schools, which gives the  admission status, GPA and standardised test scores for 55 medical school applicants from a liberal arts college in the US Midwest and it can be loaded from the `Stat2Data` package in R.

<!-- This is a dataset on admissions to US medical schools which you have first seen in [Predictive Modelling](http://moodle2.gla.ac.uk/pluginfile.php/1457608/mod_resource/content/0/week9.pdf). -->

```{r}
library(Stat2Data)
data(MedGPA)
```

The first few rows of the data are given below.
```{r, echo=FALSE}
knitr::kable(head(MedGPA), format = "markdown", padding = 2)
library(ggplot2)
library(ggthemes)
```

Let us look at a plot of acceptance against GPA, adding a bit of jitter to make overlapping points more visible.

```{r, fig.height=4.5}
medgpa.plot <- ggplot(data = MedGPA, aes(y = Acceptance, x = GPA)) +
               geom_jitter(width =0, height =0.01, alpha =0.5, colour ="#984ea3")
```

```{r, echo=FALSE, fig.height=4.5}
medgpa.plot <- medgpa.plot +
               theme(panel.background = element_rect(fill = "transparent", colour = NA),
               plot.background = element_rect(fill = "transparent", colour = NA),
               panel.border = element_rect(fill = NA, colour = "black", size = 1))
```

We can add the linear regression line for `Acceptance` as a function of `GPA` to the plot.


```{r, fig.height=4.5}
medgpa.plot + geom_smooth(method = "lm", se = FALSE,
                          fullrange = TRUE, colour = "#984ea3")
```


The R code for fitting the model and the model output is shown below.

```{r}
med.lm <- lm(Acceptance ~ GPA, data=MedGPA)
summary(med.lm)
```

In mathematical notation, the value of the independent response $Y_i$ is equal to 1 if the $i$th applicant is accepted and $Y_i=0$ otherwise, where $x_i$ refers to to the $i$th applicant's college GPA for $i=1,\dots,55$. The normal linear model assumes that the $Y_i$ are independent $N(\mu_i,\sigma^2)$ with $\mu_i=\beta_0+\beta_1 x_i$, with the fitted model equation given by $\hat{\mu}_i=-2.8240+0.9483x_i$.


One issue with this fit is that the predicted values of the response can take any real values, while acceptance can only take the value 0 or 1. And it is hard to argue that a variable taking values of 0 or 1 is normally distributed. Instead, we can use a logistic regression model for the *probability* of acceptance. Let's first write it down in mathematical notation by letting $p_i=P(Y_i=1)$ denote the probability of acceptance for the $i$th applicant. We assume that the $Y_i$ are independent random variables which follow the $\text{Bin}(1,p_i)$ (or Bernoulli($p_i)$) distribution with

$$\log \left(\dfrac{p_i}{1-p_i} \right)=\beta_0+\beta_1 x_i.$$
This is equivalent to:

$$p_i=\dfrac{\exp(\beta_0+\beta_1 x_i)}{1+\exp(\beta_0+\beta_1 x_i)}.$$

(Solve the first equation for $p_i$ to verify this!)

We can fit this model in R using the `glm()` function:

```{r, echo=TRUE}
med.glm <- glm(Acceptance ~ GPA, data = MedGPA, family = binomial)
```


The  argument `family=binomial` specifies that `Acceptance` follows a binomial distribution, with probability of success $p_i$ (i.e. the probability of the $i$th applicant being accepted is $p_i$). In addition, the probability $p_i$ is a function of $x_i$ (i.e. the probability of the $i$th applicant being accepted is a function of that applicant's GPA). The default link function, corresponding to the logit link $\log\left(\dfrac{p_i}{1-p_i}\right)$, is used here. That is, `family = binomial` implies `family = binomial(link="logit")`.

The model fit is shown below.

```{r, echo=TRUE}
summary(med.glm)
```

The regression equation for the fitted model is
$$\log \left(\dfrac{\hat{p}_i}{1-\hat{p}_i}\right)=-19.207+5.454 x_i,$$

or equivalently

$$\hat{p}_i=\dfrac{\exp(-19.207+5.454 x_i)}{1+\exp(-19.207+5.454 x_i)}.$$

The fitted curve for the probability of acceptance is shown in orange below.

```{r, echo=FALSE, fig.height=4.5}
medgpa.plot +
  geom_smooth(method = "lm",se = FALSE, fullrange = TRUE, colour = "#984ea3") +
  geom_smooth(method = "glm", color = "#ff7f00",se = FALSE, fullrange = TRUE,
              method.args = list(family = "binomial"))
```


We can see that that this curve fits the data better than the linear regression line, and that it gives predicted probabilities between 0 and 1, as desired. We could add predictors to the model to improve predictive performance -- we'll see more about that later.

The regression equation we have obtained allows us to predict the acceptance probability for a given GPA.

###[/example]

###[task]
Predict the acceptance probability for an applicant with a GPA of (i) 2.5, (ii) 3 (iii) 4. First do this "by hand" using the regression equation, then in R using the `predict()` function.

*Hint: The* `predict()` *function will return values on the linear predictor scale unless you specify* `type='response'` *which returns probabilities instead.*

####[answer]
In the *GPA and admission to medical school* example, we can write down the fitted model equation from the `summary(med.glm)`:
$$\log\left(\frac{p_i}{1-p_i}\right) =-19.207 + 5.454 \times \text{GPA}$$

From the fitted equation, we can obtain the acceptance probability by solving for $p_i$:

$$\hat{p}_i = \frac{\exp(-19.207 + 5.454 \times \text{GPA})} {1+ \exp(-19.207 + 5.454 \times \text{GPA})} $$

To predict the acceptance probability for an applicant we just need to substitute the specified GPA in the equation for $\hat{p}_i$:

(i) GPA = 2.5:
$$\hat{p}_i = \frac{\exp(-19.207 + 5.454 \times 2.5)} {1+ \exp(-19.207 + 5.454 \times 2.5)} \Rightarrow \hat{p}_i = 0.00378 $$

(ii) GPA = 3:
$$\hat{p}_i = \frac{\exp(-19.207 + 5.454 \times 3)} {1+ \exp(-19.207 + 5.454 \times 3)} \Rightarrow \hat{p}_i = 0.05494$$

(iii) GPA = 4:
$$\hat{p}_i = \frac{\exp(-19.207 + 5.454 \times 4)} {1+ \exp(-19.207 + 5.454 \times 4)} \Rightarrow \hat{p}_i = 0.93143$$


Alternatively, we can use the `predict()` function in `R` as follows:
```{r}
predict(med.glm, data.frame(GPA = c(2.5, 3, 4)), type = 'response')
```
####[/answer]
###[/task]

###[weblink,target="https://goo.gl/mZHxyN", icon=book]

 - Section 2.3 from *Mixed effects models and extensions in ecology with R -Zuur et al.* discusses the appropriateness of the assumptions of the linear model.

###[/weblink]

## What do the Bollywood box office and medical school admission examples have in common?

In both cases we have independent observations and we want to predict an outcome of interest (gross revenue/acceptance) based on an explanatory variable (budget/GPA). In both cases we have a regression equation allowing us to predict the response from a given value of the predictor. However, in one case the response is assumed to follow the normal distribution, in the other the binomial distribution. In both cases we fit a model to the *mean* of the response: in the normal linear model the mean $E(Y_i)=\mu_i$ is assumed to be a linear function of $x_i$: $\mu_i=\beta_0+\beta_1x_i$, and in the logistic regression model the mean $\mu_i=E(Y_i)=p_i$ is modelled through the *logit link function*. That is, in logistic regression $\log\left(\dfrac{p_i}{1-p_i}\right)=\beta_0+\beta_1x_i$. In slightly more general notation we have $g(\mu_i)=\boldsymbol{x}_i^{\intercal}\boldsymbol{\beta}$ where $\mu_i=E(Y_i)$ and $g(\mu_i)$ for each distribution is given in the following table.



**Model**                  **Random component**                                **Systematic component**                                    **Link function**
-------------------------- --------------------------------------------------- ----------------------------------------------------------- --------------------------------
Normal model               $y_i\overset{\text{indep}}\sim N(\mu_i,\sigma^2),$  $\boldsymbol{x}_i^T\boldsymbol{\beta}=\beta_0+\beta_1x_i$   Identity link $g(\mu_i)=\mu_i$
                           $E(Y_i)=\mu_i$
Logistic regression        $y_i\overset{\text{indep}}\sim Bin(1,p_i),$         $\boldsymbol{x}_{i}^T\boldsymbol{\beta} =\beta_0+\beta_1x_i$  Logit link: $g(\mu_i)=\log \left(\frac{\mu_i}{1-\mu_i}\right)= \log \left(\frac{p_i}{1-p_i}\right)$
model                      $E(Y_i)=p_i$


## Exponential family of distributions


It turns out that the normal and binomial distributions also have something else in common: they are both members of the *exponential family of distributions*. (And so is the Poisson, the negative binomial, gamma distribution and many others.)

###[definition] Exponential family of distributions
Consider a random variable $Y$ whose probability density function (p.d.f.) or probability mass function (p.m.f.) depends on parameter $\theta$. The distribution belongs to the exponential family if it can be written as $$f(y; \theta) = \exp \left[a(y) b(\theta)+c(\theta) + d(y) \right].$$ The term $b(\theta)$ is called the **natural parameter**. If $a(y)=y$ the distribution is said to be in **canonical form**.
###[/definition]

###[example] Normal distribution is a member of exponential family
Consider $Y \sim N(\theta, \sigma^2)$ with p.d.f. \begin{align} f(y; \theta) = \frac{1}{\sqrt{2 \pi \sigma^2}}\exp \left[ -\frac{1}{2\sigma^2} (y-\theta)^2\right], \hspace{0.5cm} -\infty <y<\infty.\end{align} If we are interested in estimating $\theta$, the variance, $\sigma^2$, can be regarded as a nuisance parameter. By rewriting the p.d.f. as \begin{align} f(y; \theta) = \exp \left[-\dfrac{y^2}{2\sigma^2}+\dfrac{y \theta}{\sigma^2} -\dfrac{\theta^2}{2\sigma^2}-\dfrac{1}{2}\log(2 \pi \sigma^2)\right]\end{align} we can see that this is of exponential family form with $a(y) = y$ (hence in canonical form) and natural parameter $b(\theta)= \theta/\sigma^2$.

###[/example]

###[task]
Show that the binomial distribution $\text{Bin}(n,p)$ is a member of the exponential family.

####[answer]
Consider $Y \sim \text{Bin}(n,\theta)$ with p.m.f. \begin{align} f(y; \theta) = \binom{n}{y} \theta^y (1-\theta)^{n-y}  \text{for } y=0,1,\dots,n. \end{align} By rewriting the p.m.f. as
 \begin{align} f(y; \theta) &= \exp \left[ y \log \theta - y \log (1-\theta) + n \log(1-\theta) +\log \binom{n}{y} \right] \\ &= \exp \left[ y \log \frac {\theta}{1-\theta} + n \log(1-\theta) +\log \binom{n}{y} \right]  \end{align}

we see that this is a member of the exponential family in canonical form, and its natural parameter is given by:
 $b(\theta)= \log \left(\dfrac{\theta}{1-\theta} \right)$.
####[/answer]
###[/task]

The exponential family of distributions has several interesting and useful properties. It can be shown that the expectation and variance for members of the exponential family can be expressed as $$E[a(Y)] = -\dfrac{c'(\theta)}{b'(\theta)}$$ and
$$\mathrm{Var}[a(Y)] =\dfrac{b''(\theta)c'(\theta)-c''(\theta)b'(\theta)}{[b'(\theta)]^3}.$$
Some further useful properties of exponential family distributions relate to the **score function**.

###[definition] Score statistic
\begin{align} U=\dfrac{dl(\theta;y)}{d\theta}\label{def:scoreuniv} \end{align}

is called the \textbf{score statistic}, and is equal to the derivative of the log-likelihood $l(\theta;y)$ with respect to the parameter $\theta$.
###[/definition]

For exponential family distributions with log-likelihood $l(\theta; y) = a(y)b(\theta)+c(\theta) + d(y)$, the score is
\begin{align} U(\theta;y)=\frac{dl(\theta;y)}{d\theta}=a(y)b'(\theta)+c'(\theta)\end{align}


<!-- Remember that in *Learning from Data* you used  -->

We use the score (derivative of the log-likelihood) to solve the likelihood equation $U=\dfrac{dl(\theta;y)}{d\theta}=0$ in order to obtain the maximum likelihood estimate $\hat{\theta}$ for a number of distributions.

We can think of the score statistic, $U=a(Y) b'(\theta)+c'(\theta)$, as a random variable in its own right, which means we can calculate its expectation
$$\mathrm{E}(U)= b'(\theta)E[a(Y)]+c'(\theta)=b'(\theta)\left[-\frac{c'(\theta)}{b'(\theta)} \right]+c'(\theta)=0,$$ and its variance $$\mathrm{Var}(U)=[b'(\theta)^2]\mathrm{Var}[a(Y)].$$

This leads us to a very important concept in statistical inference, called **Fisher information**, which we can use to obtain standard errors for maximum likelihood estimates of GLM coefficients.
###[definition] Fisher's information


\label{def:infuniv}

The **Fisher Information**, denoted as $\mathcal{I}$, is given by:
\begin{align} \mathcal{I}=\mathrm{Var}(U)=E(U^2)=E\left[ \left(\dfrac{dl(\theta;y)}{d\theta}\right)^2\right]=E\left[ \dfrac{d^2l(\theta;y)}{d\theta^2}\right].
\end{align}
###[/definition]

The variance of the maximum likelihood estimates tells us about the amount of *information* that an observed random variable carries about an unknown parameter in the model that is linked to a distribution.

As we will see shortly, the score and information play a key role in parameter estimation and in obtaining standard errors for the coefficient estimates of a GLM.

Having defined the exponential family of distributions, we are now ready to formally define a GLM.


###[weblink,target="https://goo.gl/mZHxyN", icon=book]

 - Chapter 8 from *Mixed effects models and extensions in ecology with R -Zuur et al.* contains a more in-depth discussion of the exponential family.

###[/weblink]

## Generalised Linear Models



###[definition] Generalised Linear Models
Let $Y_i$ be independent responses from an exponential family distribution in canonical form and $\mu_i=E(Y_i)$, $i=1, \dots, n$. A *generalised linear model* (GLM) is a model of the form $g(\mu_i) = \boldsymbol{x}_i^{\intercal} \boldsymbol{\beta}$ where $\boldsymbol{\beta}$ is a $p$-dimensional parameter vector,  $\boldsymbol{x}_i^{\intercal}$ is the $i$th row of the design matrix $\boldsymbol{X}$, and $g()$ is a monotonic, differentiable function called the *link function*.
###[/definition]

A GLM generalises the normal linear model by allowing:

1. a response variable with a distribution other than normal, but a member of the exponential family of distributions; and

2. a relationship between the response and the linear component of the form $g(\mu_i) = \boldsymbol{x}_i^{\intercal} \boldsymbol{\beta}$ where $g()$ is the *link function*.

### Components of a generalised linear model

1. The *random component*: Suppose $Y_1,\dots, Y_n$ are independent random variables which follow an exponential family distribution such that $f(y_i; \theta_i)=\exp[y_i b(\theta_i)+c(\theta_i)+d(y_i)]$ for $i=1,\dots n$. The joint p.d.f. (or p.m.f.) of the $Y_i$ is \begin{align} &f(y_1,\dots,Y_n; \theta_1,\dots, \theta_n) =
  \prod_{i=1}^n \exp[y _i b(\theta_i)+c(\theta_i)+d(y_i)] \nonumber \\
  & = \exp \left[ \sum_{i=1}^n y_i b(\theta_i)+ \sum_{i=1}^n c(\theta_i) +\sum_{i=1}^n d(y_i) \right] \label{eqn:joint} \end{align}
  The distribution of each $Y_i$ is in canonical form and depends on a single parameter $\theta_i$.

2. The *systematic component*: Associated with each $y_i$ is a vector $\boldsymbol{x}_i=(x_{i1},x_{i2},\dots,x_{ip})^\intercal$ of values of $p$ explanatory variables. The response, $Y_i$, depends on the explanatory variables through a linear component, $\eta_i=\boldsymbol{x}_i^{\intercal} \boldsymbol{\beta}=\beta_1x_{i1}+\dots + \beta_p x_{ip}$ for $i=1,\dots, n$ where $\boldsymbol{x}_i^{\intercal}$ is the $i$th row of the design matrix $\boldsymbol{X}$ and $\boldsymbol{\beta}=(\beta_1,\dots, \beta_p)^\intercal$ is the parameter vector. As in linear models, the design matrix is given by $$\boldsymbol{X} = \left[ \begin{array}{c} \boldsymbol{x}_1^\intercal \\ \vdots \\ \boldsymbol{x}_n^\intercal \end{array}\right] =  \left[ \begin{array}{ccc} x_{11} & \dots & x_{1p} \\ \vdots & & \vdots \\ x_{n1} & \dots & x_{np}  \end{array}\right].$$

3. The *link function*: The parameters $\theta_i$ in equation (\ref{eqn:joint}) are usually not of direct interest. Instead, we are interested in a smaller set of parameters $(\beta_1,\dots, \beta_p)$, and assume that $Y_i$ depends on these through the linear predictor $\eta_i$. The link between the distribution of the $Y_i$ and the linear predictor $\eta_i$ is provided by the link function $g()$, for which $g(\mu_i)=\eta_i=\boldsymbol{x}_i^{\intercal} \boldsymbol{\beta}.$ Here $\mu_i=E(Y_i)$ and $g()$ is a monotone, differentiable function. Although any one-to-one function could be used in principle, certain choices of link function can offer great simplification. In particular, the link function can be chosen so that the natural parameter, $b(\theta_i)$, is proportional to the linear component $\eta_i=\boldsymbol{x}_i^{\intercal} \boldsymbol{\beta}$. Such a link function is known as the *canonical link*. The following table shows the canonical link function for some of the most common distributions.


**Distribution**   **Natural parameter**                         **Canonical link**
------------------ --------------------------------------------- ----------------------
Normal             $\dfrac{\theta}{\sigma^2}$                    $g(\mu)=\mu$
Poisson            $\log \theta$                                 $g(\mu)=\log(\mu)$
Binomial           $\log\left(\dfrac{\theta}{1-\theta} \right)$  $g(\mu)=\log\left(\dfrac{\mu}{1-\mu} \right)$

###[weblink,target="http://encore.lib.gla.ac.uk/iii/encore/record/C__Rb2939999?lang=eng",icon=book]

 - Chapter 6 from *Extending linear models with R: generalized linear, mixed effects and nonparametric
regression models - Faraway* gives an overview of GLMs and their properties.

###[/weblink]

###[weblink,target="https://goo.gl/mZHxyN", icon=book]

 - Sections 9.1 and 9.2 from *Mixed effects models and extensions in ecology with R -Zuur et al.* cover the general formulation of GLMs.

###[/weblink]

Let us now look at some examples of generalised linear models and their components.

###[example] Normal linear model

Consider $E(Y_i)=\mu_i =  \boldsymbol{x}_i^{\intercal} \boldsymbol{\beta}$ where the $Y_i$ are independent  $N(\mu_i,\sigma^2)$  for $i=1,\dots,n$. Here $g(\mu_i)=\mu_i$, the \textbf{identity} link. You may be more familiar with this model written as $\boldsymbol{y}=\boldsymbol{X}\boldsymbol{\beta}+\boldsymbol{\epsilon}$ where $\boldsymbol{\epsilon}=\left[ \begin{array}{c} \epsilon_1\\ \vdots \\ \epsilon_n  \end{array} \right]$ with the $\epsilon_i$ independent, identically distributed $N(0,\sigma^2)$ random variables.
###[/example]

###[example] A model for historical linguistics

If two languages are separated by time $t$, the probability of having cognate words for a particular meaning can be modelled as $\exp(-\theta t)$. For a test list of $n$ meanings, a linguist judges whether the corresponding words in two languages are cognate or not. For the $i$th meaning, define $$Y_i = \left \lbrace \right.\begin{array}{ll} 1 & \text{if the two languages have cognate words} \\ 0 & \text{if words are not cognate.} \end{array}$$ Then $P(Y_i=1)=\exp(-\theta t) =p$ and  $P(Y_i=0)=1-p$
i.e. $Y_i \sim \text{Bernoulli}(p)$ (or equivalently Bin$(1,p)$) and $E(Y_i)=p$. The link function is $g(p)= \log p = -\theta t$ so that $g(p)$ is linear in the parameter of interest, $\theta$; also $\boldsymbol{x}_i = -t$; and $\boldsymbol{\beta}=\theta$.
###[/example]

###[example] A model for mortality rates

 The number of deaths, $Y$, in a population can be modelled by the Poisson distribution $f(y; \mu)=\frac{\mu^y e^{-\mu}}{y!}$ where $y=0,1,2,\dots$
 The expected number of deaths per year is $E(Y)=\mu$ and it can be modelled by $E(Y)=\mu = n \lambda (\boldsymbol{x}^\intercal \boldsymbol{\beta})$ where $n$ is the population size and $\lambda (\boldsymbol{x}^\intercal \boldsymbol{\beta})$ is the death rate per 100,000 people per year.
 Let $Y_1,\dots, Y_n$ be the numbers of deaths occurring in successive age groups. A possible model is $E(Y_i)=\mu_i = n_i \exp(\theta i)$ where $Y_i \sim \text{Poisson}(\mu_i)$ and

* $i=1$ for the age group 30-34,
* $i=2$ for age group 35-39,
* $\vdots$
* $i=8$ for age group 65-69.

This can be expressed as a generalised linear model as $g(\mu_i) = \log \mu_i = \log n_i +\theta i$ where $\boldsymbol{x}_i^{\intercal}= (\log n_i, i)$;  and $\boldsymbol{\beta}=(1, \theta)^\intercal$. The term $\log n_i$ is called the *offset*, and we will see more about it when we talk about models for counts.

###[/example]

###[task]

Formulate the model used in the medical school admissions example as a GLM.

####[answer]
Random component: Let $Y_i=1$ if the $i$th applicant is accepted to medical school and $Y_i=0$ if not. We assume that the $Y_i$ are independent responses from $\text{Bin}(1,p_i)$, with $E(Y_i)=p_i$ for i=1,\dots,55.

Systematic component: $\beta_0+\beta_1 x_i$ where $x_i$ is the $i$th applicant's GPA and $\beta_0$ and $\beta_1$ are parameters to be estimated.

Link function: $g(p_i)=\log \left(\dfrac{p_i}{1-p_i} \right)$ (logit link)

Equation of the GLM: $$\log \left(\dfrac{p_i}{1-p_i} \right)=\beta_0+\beta_1 x_i.$$
####[/answer]
###[/task]

<!-- # Summary  -->

<!-- Main concepts to be sure to be familiar with from this chapter: -->
<!-- \begin{itemize} -->
<!-- *  What are the two ways in which the GLM generalises the normal linear model? -->
<!-- *  What are the random and systematic component of a GLM and what is the role of the link function? -->
<!-- *  What is the form of the p.d.f. (or p.m.f.) of a distribution that is a member of the exponential family of distributions? What do the terms \textit{natural parameter}, \textit{canonical form} and \textit{canonical link} refer to? -->
<!-- *  Properties of exponential family distributions: can you derive/remember the expressions for the mean and variance of an exponential family distribution in canonical form? -->
<!-- \end{itemize} -->

## Maximum likelihood estimation of GLM coefficients

In a generalised linear model we are interested in the parameters $\beta_1,\dots,\beta_p$ that describe how the response depends on the explanatory variables. We use the observed $y_1,\dots,y_n$ to  maximise the log-likelihood function
\begin{align} l(\boldsymbol{\beta},\boldsymbol{y})=\sum_{i=1}^n y_i b(\theta_i)+ \sum_{i=1}^n c (\theta_i) +\sum_{i=1}^n d(y_i)  \label{eqn:loglik} \end{align}
obtained from equation (\ref{eqn:joint}). This depends on $\boldsymbol{\beta}$ through \begin{align*}
& \mu_i=E(Y_i)=-\frac{c'(\theta_i)}{b'(\theta_i)};    \\
& \mathrm{Var}(Y_i) = [b''(\theta_i)c'(\theta_i)-c''(\theta_i)b'(\theta_i)]/[b'(\theta_i)]^3;\\
& g(\mu_i)=\eta_i= \boldsymbol{x}_i^{\intercal} \boldsymbol{\beta}, \hspace{1cm} i=1,\dots,n.                                                                           \end{align*}
The maximisation procedure results in $p$ simultaneous equations for $\hat{\boldsymbol{\beta}}$, which are usually solved numerically using the **method of scoring** (also known as **Fisher's scoring method**) and an algorithm called **iteratively reweighted least squares**.

<!-- To illustrate this process we will first present an example with a single parameter, $\theta$, of interest, before proceeding to develop the general theory of estimation of $(\beta_1,\dots,\beta_p)$ for generalised linear models. -->

<!-- \begin{example}[Weibull distribution for failure times] -->

<!-- Suppose we wish to obtain the maximum likelihood estimate of $\theta$ for independent random variables $Y_1, \dots, Y_n$ that follow the Weibull distribution with p.d.f. \begin{align}f(y; \theta)= \exp[\log \lambda + (\lambda-1) \log y - \lambda \log \theta - (y/\theta)^\lambda]\end{align} where $\lambda$ is assumed to be known. This distribution belongs to the exponential family with \begin{align} &a(y)= y^\lambda, \nonumber\\ &b(\theta)=-\theta^{-\lambda}, \nonumber\\ &c(\theta)=\log \lambda - \lambda \log \theta,  \text{ and} \nonumber\\ &d(y)=(\lambda-1) \log(y). \label{eqn:weibullexpfam} \end{align} The joint p.d.f. of the $Y_i$ is \begin{align}f(y_1,\dots, y_n; \theta)= \prod_{i=1}^n \frac{\lambda y_i^{\lambda-1}}{\theta^\lambda} \exp\left[ - \left(\frac{y_i}{\theta} \right)^\lambda \right],\end{align} and the corresponding log-likelihood is $l(\theta; y_1,\dots, y_n)= \sum_{i=1}^n \left[ [(\lambda-1) \log y_i + \log \lambda - \lambda \log \theta]- \left(\frac{y_i}{\theta} \right)^\lambda \right]. $ -->


<!-- The score function is the derivative of the log-likelihood with respect to $\theta$: \begin{align} U(\theta)=\frac{dl}{d\theta}= \sum_{i=1}^n  \left[ -\frac{\lambda}{\theta}+\frac{\lambda y_i^\lambda}{\theta^{\lambda+1}} \right]. \label{eqn:u} \end{align} The maximum likelihood estimator, $\hat{\theta}$, is the solution of the equation $U(\theta)=0 \Rightarrow \sum_{i=1}^n  \left[ -\frac{\lambda}{\theta}+\frac{\lambda y_i^\lambda}{\theta^{\lambda+1}} \right]=0$ Although this equation can be easily solved for $\theta$ (assuming $\lambda$ is known -- you can verify that the solution is $\left(\frac{1}{n} \sum y_i^{\lambda}\right)^{1/\lambda}$, for illustration purposes consider solving it numerically using the Newton-Raphson algorithm. The iterative process uses the formula \begin{align} \theta^{(m)}= \theta^{(m-1)}-\frac{U^{(m-1)}}{U'^{(m-1)}}\label{eqn:newtonraphson} \end{align} starting with an initial guess $\theta^{(1)}$ and obtaining successive approximations until convergence. The derivative, $U'$, with respect to $\theta$ is  \begin{align} U'(\theta)= \frac{dU}{d \theta}= \sum_{i=1}^n \left[ \frac{\lambda}{\theta^2}-\frac{\lambda(\lambda+1)y_i^\lambda}{\theta^{\lambda+2}} \right] = \frac{\lambda n}{\theta^2}-\frac{\lambda(\lambda+1)\sum y_i^\lambda}{\theta^{\lambda+2}} \label{eqn:uprime} .\end{align}  Given a starting value, $\theta^{(1)}$, as well as $n$, $\sum y_i^\lambda$ and the expressions for $U$ and $U'$ in (\ref{eqn:u}) and (\ref{eqn:uprime}), we can use the Newton-Raphson formula (\ref{eqn:newtonraphson}) to obtain the maximum likelihood estimate of $\theta$. An alternative approach, known as the \textbf{method of scoring} or \textbf{Fisher scoring}, is to approximate $U'$ by its expectation $E(U')$ which, for exponential family distributions, is (see equation (\ref{eqn:informationtheta}))$E(U')=-\mathrm{Var}(U)=-\mathcal{I}=b''(\theta)\left[ -\frac{c'(\theta)}{b'(\theta)}\right]+c''(\theta) .$ Using the method of scoring we write the estimating equation as \begin{align} \theta^{(m)}= \theta^{(m-1)}+\frac{U^{(m-1)}}{\mathcal{I}^{(m-1)}}. \label{eqn:scoring} \end{align} In our example the information $\mathcal{I}$ is \begin{align}\mathcal{I}=E(-U')= E\left[- \sum_{i=1}^n U'_i \right]=\sum_{i=1}^n E(-U'_i) =\sum_{i=1}^n \left[ \frac{b''(\theta)c'(\theta)}{b'(\theta)} -c''(\theta)\right]=\dots=\frac{\lambda^2 n}{\theta^2}, \label{eqn:weibull} \end{align} -->
<!--   %\vspace{5cm} -->
<!--   where $U_i$ is the score for $Y_i$ and $b(\theta)$ and $c(\theta)$ are given in (\ref{eqn:weibullexpfam}). -->
<!-- \end{example} -->


###[supplement]

Here we present in some detail how the maximum likelihood estimates of the coefficients of a GLM are obtained. Suppose that we are interested in estimating the parameter vector $(\beta_1,\dots, \beta_p)^\intercal$ in a GLM. To find the maximum likelihood estimates $\hat{\beta}_j$ we need the scores (multivariate version of the score from Definition \ref{def:scoreuniv}) expressed as functions of the $\beta_j$:
  \begin{align} U_j=\frac{\partial l}{\partial \beta_j} &= \sum_{i=1}^n \left [   \frac{\partial l_i}{\partial \beta_j}  \right]= \sum_{i=1}^n \left [   \frac{\partial l_i}{\partial \theta_i} \cdot \frac{\partial \theta_i}{\partial \mu_i} \cdot \frac{\partial \mu_i} {\partial \beta_j}  \right] \label{eqn:chainrule}           \end{align}


For an exponential family distribution in canonical form, the components of (\ref{eqn:chainrule}) are:
  \begin{align} &\frac{\partial l_i}{\partial \theta_i} = y_i b'(\theta) +c'(\theta)=b'(\theta) \left[y_i -\left(-\frac{c'(\theta)}{b'(\theta)}\right) \right]=b' (\theta) (y_i - \mu_i) \label{eqn:dldtheta}\\
& \frac{\partial \mu_i}{\partial \theta_i} = -\frac{c''(\theta_i)}{b'(\theta_i)}+\frac{c'(\theta_i) b''(\theta_i)}{[b'(\theta_i)]^2} = b'(\theta_i) \mathrm{Var}(Y_i)  \nonumber \\ & \Rightarrow  \frac{\partial \theta_i}{\partial \mu_i}= \frac{1}{b'(\theta_i) \mathrm{Var}(Y_i)} \label{eqn:dmudtheta}\\
&\frac{\partial \mu_i}{\partial \beta_j}=\frac{\partial \mu_i}{\partial \eta_i}\cdot \frac{\partial \eta_i}{\partial \beta_j}=\frac{\partial \mu_i}{\partial \eta_i}x_{ij}=\frac{x_{ij}}{g'(\mu_i)} \label{eqn:dmudbeta}
\end{align}
Substituting (\ref{eqn:dldtheta}), (\ref{eqn:dmudtheta}) and (\ref{eqn:dmudbeta}) into (\ref{eqn:chainrule}) the expression for the scores becomes
 \begin{align}
 U_j=\sum_{i=1}^n \left[  \frac{(y_i-\mu_i)}{\mathrm{Var}(Y_i)}x_{ij} \frac{\partial \mu_i}{\partial \eta_i}\right]=\sum_{i=1}^n \left[  \frac{(y_i-\mu_i)}{\mathrm{Var}(Y_i)}\frac{x_{ij}} {g'(\mu_i)}\right]. \label{eqn:score}
 \end{align}
Note that the scores depend on $\boldsymbol{\beta}$ through $\mu_i=E(Y_i)$ and through $\mathrm{Var}(Y_i)$.
The variance-covariance matrix of the $U_j$ has terms $\mathcal{\mathcal{I}}_{jk}=E(U_jU_k)$ and is known as the \textbf{(Fisher) information matrix}. This is the multivariate version of Definition \ref{def:infuniv}. The elements of matrix $\mathcal{\mathcal{I}}$ can be obtained from (\ref{eqn:score}):
\begin{align}
 \mathcal{\mathcal{I}}_{jk}&=  E \left \lbrace \sum_{i=1}^n \left[  \frac{(y_i-\mu_i)}{\mathrm{Var}(Y_i)}x_{ij} \frac{\partial \mu_i}{\partial \eta_i}\right] \sum_{l=1}^n \left[  \frac{(y_l-\mu_l)}{\mathrm{Var}(Y_l)}x_{lk} \frac{\partial \mu_l}{\partial \eta_l}\right]  \right \rbrace \nonumber \\
 &=\sum_{i=1}^n \frac{E[(Y_i-\mu_i)^2]x_{ij}x_{ik}}{[\mathrm{Var}(Y_i)]^2} \left(\frac{\partial \mu_i}{\partial \eta_i} \right)^2  \nonumber\\
 &=\sum_{i=1}^n \frac{x_{ij}x_{ik}}{\mathrm{Var}(Y_i)} \left(\frac{\partial \mu_i}{\partial \eta_i} \right)^2 = \sum_{i=1}^n \frac{x_{ij}x_{ik}}{\mathrm{Var}(Y_i)\left(g'(\mu_i)\right)^2} \label{eqn:informationmatrix}
\end{align}
Here we have used the fact that $E[(Y_i-\mu_i)(Y_l-\mu_l)]=0$ by the independence of the $Y_i$.
Notice that the information matrix can be written as \begin{align}
\mathcal{I}=\mathcal{I}(\boldsymbol{\beta})=\boldsymbol{X}^\intercal \boldsymbol{W} \boldsymbol{X} \label{eqn:informationirwls} \end{align} where $\boldsymbol{X}=\left[ \begin{array}{c} \boldsymbol{x}_1^\intercal\\ \dots \\  \boldsymbol{x}_n^\intercal \end{array} \right]= \left[ \begin{array}{ccc}
x_{11} & \dots & x_{1p}\\
\vdots & \ddots & \vdots\\
x_{n1} & \dots & x_{np}                                                                                                                                                                                                                     \end{array} \right],$
 $\boldsymbol{W}=\text{diag}(\boldsymbol{w})=  \left[ \begin{array}{cccc}
w_1 & 0 &  \dots & 0\\
0 & w_2 & & \vdots\\
\vdots & & \ddots & 0\\
0 & \dots &0 & w_n                                                                                                                                                                                                                     \end{array} \right],$ and \begin{align}w_i=\frac{1}{\mathrm{Var}(Y_i)\left(g'(\mu_i)\right)^2}, \hspace{1cm} i=1,\dots, n.\end{align}
The information matrix $\mathcal{I}(\boldsymbol{\beta})$ depends on $\boldsymbol{\beta}$ through $\boldsymbol{\mu}$ and through $\mathrm{Var}(Y_i)$ for $i=1,\dots,n$.

Equation (\ref{eqn:score}) can be written as \begin{align}
U_j=\sum_{i=1}^n (y_i-\mu_i)x_{ij}w_i g'(\mu_i)=   \sum_{i=1}^n x_{ij}w_iz_i \label{eqn:scoreirwls}    \hspace{1cm} j=1,\dots,p                                   \end{align}

where $z_i=(y_i-\mu_i)g'(\mu_i)$, so the score can be expressed in vector-matrix form as \begin{align}
\boldsymbol{U}(\boldsymbol{\theta})= \boldsymbol{X}^\intercal \boldsymbol{W} \boldsymbol{z} \label{eqn:scoreirwlsmatrix}. \end{align}



Fisher's method of scoring is based on the estimating equation \begin{align}\hat{\boldsymbol{\beta}}^{(m)} = \hat{\boldsymbol{\beta}}^{(m-1)} +[\mathcal{\mathcal{I}}^{(m-1)}]^{-1} \boldsymbol{U}^{(m-1)} \label{eqn:scoringpdim}\end{align} where $\hat{\boldsymbol{\beta}}^{(m)}$ is the vector of estimates of $(\beta_1,\dots \beta_p)$ at the $m$th iteration, $[\mathcal{\mathcal{I}}^{(m-1)}]^{-1}$ is the inverse of the information matrix with elements $\mathcal{\mathcal{I}}_{jk}$ given by (\ref{eqn:informationmatrix}), and $\boldsymbol{U}^{(m-1)}$ is the vector of elements given by (\ref{eqn:score}), all evaluated at  $\hat{\boldsymbol{\beta}}^{(m-1)}$.
  Substituting (\ref{eqn:informationirwls}) and (\ref{eqn:scoreirwlsmatrix}) in (\ref{eqn:scoringpdim}) we obtain \begin{align} \hat{\boldsymbol{\beta}}^{(m)}&=   \hat{\boldsymbol{\beta}}^{(m-1)}+\left[\boldsymbol{X}^\intercal \boldsymbol{W}^{(m-1)} \boldsymbol{X}\right]^{-1}  \boldsymbol{X}^\intercal \boldsymbol{W}^{(m-1)} \boldsymbol{z}^{(m-1)}  \nonumber \\
  &=  \left[\boldsymbol{X}^\intercal \boldsymbol{W}^{(m-1)} \boldsymbol{X}\right]^{-1}  \boldsymbol{X}^\intercal \boldsymbol{W}^{(m-1)}\boldsymbol{X} \hat{\boldsymbol{\beta}}^{(m-1)}+\left[\boldsymbol{X}^\intercal \boldsymbol{W}^{(m-1)} \boldsymbol{X}\right]^{-1}  \boldsymbol{X}^\intercal \boldsymbol{W}^{(m-1)} \boldsymbol{z}^{(m-1)}  \nonumber \\
  &=\left[\boldsymbol{X}^\intercal \boldsymbol{W}^{(m-1)} \boldsymbol{X}\right]^{-1}  \boldsymbol{X}^\intercal \boldsymbol{W}^{(m-1)} (\boldsymbol{\eta}^{(m-1)}+\boldsymbol{z}^{(m-1)}) \label{eqn:irwls} \end{align}
 This is the same form as the normal equations for weighted least squares, except that it has to be solved numerically because $\boldsymbol{\eta}$, $\boldsymbol{z}$ and $\boldsymbol{W}$ depend on $\hat{\boldsymbol{\beta}}$.

This is why the method to obtain maximum likelihood estimators for GLMs is called \textbf{iteratively (re)weighted least squares (IRWLS)}.
The procedure begins by using an initial approximation $\hat{\boldsymbol{\beta}}^{(0)}$ to obtain estimates of $\boldsymbol{\eta}$, $\boldsymbol{z}$ and $\boldsymbol{W}$. Then $\hat{\boldsymbol{\beta}}^{(1)}$ is obtained from (\ref{eqn:irwls}) and is used to update $\boldsymbol{\eta}$, $\boldsymbol{z}$ and $\boldsymbol{W}$. The iterative process continues until the difference between successive approximations $\hat{\boldsymbol{\beta}}^{(m-1)}$ and $\hat{\boldsymbol{\beta}}^{(m)}$ is sufficiently small.
###[/supplement]

<!-- \begin{example}[Poisson regression] -->

<!-- Suppose we have the following data: -->
<!-- \begin{center} -->
<!-- \begin{tabular}{crrrrrrrrrr} -->
<!-- \hline -->
<!-- $y_i$ & 2 & 3 & 6 & 7 & 8 & 9 & 10& 12 & 15\\ -->
<!-- $x_i$ & -1 & -1 & 0 & 0 &0 & 0 & 1 & 1 & 1 \\ -->
<!-- \hline -->
<!-- \end{tabular} -->
<!-- \end{center} -->
<!-- Assume that the responses $Y_i$ are independent Poisson random variables with $E(Y_i)=\mu_i = \beta_1 + \beta_2 x_i= \boldsymbol{x}_i^{\intercal} \boldsymbol{\beta}=\eta_i$ where $\boldsymbol{\beta}= \left[ \begin{array}{c} \beta_1\\ \beta_2 \end{array}\right] \text{ and } \boldsymbol{x}_i =   \left[ \begin{array}{c} 1\\ x_i \end{array} \right] \text{ for } i=1,\dots, n.$ Note that here we are taking the link function to be the identity link: $g(\mu_i)= \mu_i= \boldsymbol{x}_i^{\intercal} \boldsymbol{\beta}=\eta_i$ -->
<!-- Then $g'(\mu_i)=\dfrac{\partial \mu_i}{\partial \eta_i}= 1$ so the weights in (\ref{eqn:informationirwls}) are  $w_i = \frac{1}{\mathrm{Var}(Y_i)} =\frac{1}{\mu_i}=\frac{1}{\beta_1+\beta_2 x_i}$ and $\eta_i+z_i$ in  (\ref{eqn:irwls}) simplifies to  $\eta_i+z_i = -->
<!--  \hat{\beta}_1+\hat{\beta}_2x_i+ (y_i-\mu_i)=\hat{\beta}_1 +\hat{\beta}_2 x_i +(y_i - \hat{\beta}_1 -\hat{\beta}_2 x_i) = y_i. $ Also $\mathcal{\mathcal{I}}=\boldsymbol{X}^\intercal \boldsymbol{W} \boldsymbol{X}= \left[ \begin{array}{cc} \sum \dfrac{1}{\hat{\beta}_1+\hat{\beta}_2 x_i }&  \sum \dfrac{x_i}{\hat{\beta}_1+\hat{\beta}_2 x_i} \\ -->
<!--  \sum \dfrac{x_i}{\hat{\beta}_1+\hat{\beta}_2 x_i}  &  \sum \dfrac{x_i^2}{\hat{\beta}_1+\hat{\beta}_2 x_i } \end{array}\right] $ -->
<!-- and $\boldsymbol{X}^\intercal \boldsymbol{W} (\boldsymbol{\eta}+\boldsymbol{z})= \left[ \begin{array}{c} \sum \dfrac{y_i}{\hat{\beta}_1+\hat{\beta}_2 x_i }\\ -->
<!--  \sum \dfrac{x_i y_i}{\hat{\beta}_1+\hat{\beta}_2 x_i}  \end{array}\right] $ -->
<!-- so that the MLEs can be obtained from  $\hat{\boldsymbol{\beta}}^{(m)}=\left[(\boldsymbol{X}^\intercal \boldsymbol{W} \boldsymbol{X})^{(m-1)}\right]^{-1} (\boldsymbol{X}^\intercal \boldsymbol{W} \boldsymbol{z}) ^{(m-1)}.$ -->
<!-- For the data in this example, $\boldsymbol{\eta}+ \boldsymbol{z}=\boldsymbol{y}=\left[\begin{array}{c} 2\\ 3\\ \vdots\\ 15 \end{array}\right] \text{ and } \boldsymbol{X}=\left[ \begin{array}{c} \boldsymbol{x}_1^\intercal \\ \boldsymbol{x}_2^\intercal \\ \vdots \\ \boldsymbol{x}_9^\intercal  \end{array} \right]=\left[\begin{array}{rr} 1 &  -1\\ 1 &  -1\\ \vdots & \vdots\\ 1 & 1 \end{array}\right]. $ -->
<!-- We also need some initial parameter estimates. Start with $\hat{\beta}_1^{(1)}=7$ and $\hat{\beta}_2^{(1)}=5$. -->
<!-- Then $(\boldsymbol{X}^\intercal \boldsymbol{W} \boldsymbol{X})^{(1)}= \left[ \begin{array}{rr} 1.821429 & -0.75  \\ -->
<!--  -0.75&  1.25 \end{array}\right], $ -->
<!--  $(\boldsymbol{X}^\intercal \boldsymbol{W} \boldsymbol{z})^{(1)}= \left[ \begin{array}{cc} 9.869048  \\ -->
<!-- 0.583333 \end{array}\right], $ -->
<!--  so \begin{align*} \hat{\boldsymbol{\beta}}^{(2)}= [(\boldsymbol{X}^\intercal \boldsymbol{W} \boldsymbol{X})^{(1)}]^{-1}(\boldsymbol{X}^\intercal \boldsymbol{W} \boldsymbol{z})^{(1)}= \left[ \begin{array}{cc} 7.4514  \\ -->
<!-- 4.9375 \end{array}\right] -->
<!-- \end{align*} -->
<!-- and so on until convergence.\\ -->
<!-- The MLEs are $\hat{\beta}_1=7.45163$ and $\hat{\beta}_2=4.93520$. -->
<!-- \end{example} -->

<!-- \begin{remark} Fisher scoring and the Newton-Raphson algorithm are equivalent when the canonical link is used in a generalised linear model. This is because for the canonical link function $\mathcal{I}=E(U')=-U'$. -->
<!-- \end{remark} -->

<!-- \begin{remark} The linear model is a generalised linear model with identity link, $\eta_i=g(\mu_i)=\mu_i$, and $\mathrm{Var}(Y_i)=\sigma^2$ for $i=1,\dots,n$. This results in weights $w_i=1/\sigma^2$ and $z_i=(y_i-\mu_i)$ that do not depend on $\boldsymbol{\beta}$. Hence $\boldsymbol{\eta}+\boldsymbol{z}=\boldsymbol{y}$ and $\boldsymbol{W}=1/\sigma^2 \boldsymbol{I}$, where $\boldsymbol{I}$ is the $n\times n$ identity matrix. Therefore the solution to the likelihood equations can be obtained in one step as we will see in more detail in Chapter 5. -->
<!-- \end{remark} -->

<!-- #Summary -->
<!-- Points to take away from this chapter: -->
<!-- \begin{itemize} -->
<!-- *  Multivariate versions of the score and information (score statistics and information matrix) -->
<!-- *  How to do maximum likelihood estimation -->
<!-- *  Finding the maximum likelihood estimator using numerical methods (Newton-Raphson method, Fisher's scoring method/iteratively reweighted least squares) -->
<!-- *  Think about estimation in the context of a GLM: if you are asked how the estimates for the model parameters were obtained, can you name the estimation method and briefly describe how it works? -->
<!-- \end{itemize} -->

## Inference for GLMs
We will now turn our attention to inference for GLMs, mainly through hypothesis tests and the construction of confidence intervals for the parameters of interest. For that we need some sampling distribution results.
<!-- We can think of this process in the context of two models which have the same probability distribution and the same link function, but different number of parameters in the linear component. The simpler model (model under the null hypothesis $H_0$) must be a special case of the more general model. Then hypothesis testing takes the steps: -->
<!--  \begin{enumerate} -->
<!--  *  Specify model $M_0$ to correspond to $H_0$ and a more general model $M_1$ with $M_0$ a special case of $M_1$. -->
<!--    *  Fit $M_0$ and calculate a ``goodness of fit statistic" $G_0$. Fit $M_1$ and the corresponding statistic $G_1$. -->
<!--    *   Calculate the improvement in fit, usually $G_1-G_0$ (or sometimes $G_1/G_0$). -->
<!--     *  Use the sampling distribution of $G_1-G_0$ to test the null hypothesis that $G_1=G_0$ against $G_1 \neq G_0$. -->
<!--         *  If the hypothesis $G_1=G_0$ is not rejected, then $H_0$ is not rejected and $M_0$ is the preferred model. If $G_1=G_0$ is rejected, then $M_1$ is regarded as the better model. -->
<!--  \end{enumerate} -->
<!-- To obtain confidence intervals and test statistics we need the sampling distribution of the estimator. Exact sampling distributions can be obtained if the response follows the normal distribution. Otherwise, we use asymptotic (large sample) results, usually based on the Central Limit Theorem. For these results to be valid, certain conditions need to hold, but we won't go into them in detail because these are satisfied for the exponential family distributions used in the GLMs we will be covering. -->

<!-- #Large sample approximations  -->

<!-- To come up with the approximate distribution of a statistic of interest, we rely on the following large sample distribution results: If $S$ is a statistic of interest, under appropriate conditions $\dfrac{S-E(S)}{\sqrt{\mathrm{Var}(S)}}  \overset{\text{approx}}{\sim} N(0,1)$, or equivalently  $\dfrac{[S-E(S)]^2}{\mathrm{Var}(S)}  \overset{\text{approx}}{\sim} \chi^2(1)$. -->
<!--  For a vector of statistics of interest $\boldsymbol{s}=(S_1, S_2, \dots, S_p)^\intercal$ with mean $E(\boldsymbol{s})$ and variance-covariance matrix $\boldsymbol{V}$, we have the asymptotic result -->
<!--  \begin{align} [\boldsymbol{s}-E(\boldsymbol{s})]^\intercal \boldsymbol{V}^{-1}  [\boldsymbol{s}-E(\boldsymbol{s})]  \overset{\text{approx}}{\sim} \chi^2(p), \label{eqn:approxnormgeneral}\end{align} provided that $\boldsymbol{V}$ is not singular. -->

<!-- #Sampling distribution for score statistics  -->
<!--  Suppose $Y_1, \dots, Y_n$ are independent random variables in a GLM with parameters $\boldsymbol{\beta}$, with $E(Y_i)=\mu_i$ and $g(\mu_i)=\boldsymbol{x}_i^{\intercal} \boldsymbol{\beta}= \eta_i$. -->

<!-- Recall equation (\ref{eqn:score}), which gives the score statistics as \begin{align*}U_j= \frac{\partial l}{\partial \beta_j} = \sum_{i=1}^n \left[ \frac{(Y_i-\mu_i)}{\mathrm{Var}(Y_i)}\frac{x_{ij}}{g'(\mu_i)} \right]  \hspace{1cm}  \text{ for } j=1, \dots, p. \end{align*} -->
<!-- Recall also that since $E(Y_i)=\mu_i$, we have that $E(U_j)=0$ for $j=1,\dots, p$ and that the variance-covariance matrix of the score statistics has elements -->
<!-- \begin{align}\mathcal{I}_{jk}= E(U_jU_k).\label{eqn:scorevar}\end{align} -->

<!-- For univariate $\beta$, the score statistic has asymptotic sampling distribution \begin{align}\frac{U}{\sqrt{\mathcal{I}}} \overset{\text{approx}}{\sim}N(0,1)\label{eqn:scoreuniv}\end{align} or equivalently  \begin{align}\frac{U^2}{\mathcal{I}} \overset{\text{approx}}{\sim} \chi^2(1).\end{align} -->
<!-- For $\boldsymbol{\beta}=(\beta_1, \dots, \beta_p)^\intercal$, the score vector \begin{align}\boldsymbol{U}=(U_1,\dots, U_p)^\intercal \overset{\text{approx}}{\sim} MVN(\boldsymbol{0}, \mathcal{I})  \end{align} and so \begin{align} \boldsymbol{U}^\intercal \mathcal{I}^{-1} \boldsymbol{U} \overset{\text{approx}}{\sim} \chi^2(p).\label{eqn:scoremultiv}\end{align} -->

<!-- \begin{example}[Normal distribution] -->
<!-- Let $Y_1, \dots, Y_n$ be independent $N(\mu, \sigma^2)$ random variables where $\sigma^2$ is assumed to be known. The log-likelihood function is -->
<!--  \begin{align}l = -\frac{1}{2\sigma^2}\sum_{i=1}^n(y_i -\mu)^2 -n \log(\sqrt{2\pi \sigma^2}).\label{eqn:infnormal} -->
<!--  \end{align} -->
<!--  The score statistic is \begin{align}U=\frac{dl}{d\mu}= \frac{1}{\sigma^2}\sum (Y_i -\mu) = \frac{n}{\sigma^2} (\bar{Y}-\mu).\label{eqn:scorenormal}\end{align} -->
<!-- It can be seen that the expectation of $U$ is $0$ since $E(Y_i)=\mu$. -->
<!-- The variance of $U$ is -->
<!--  \begin{align} \mathcal{I} = \mathrm{Var}(U)= \frac{1}{\sigma^4}\sum \mathrm{Var}(Y_i)=\frac{n}{\sigma^2}. \label{eqn:varnormal}\end{align} -->
<!--  Therefore \begin{align}\frac{U}{\sqrt{\mathcal{I}}}= \frac{\bar{Y}-\mu}{\sigma / \sqrt{n}} \overset{\text{approx}}{\sim}N(0,1).\label{eqn:scoredistnormaluniv} \end{align} In fact, this is an exact result since $\bar{Y} \sim N(\mu, \sigma^2/n)$. -->
<!-- Also, \begin{align} \boldsymbol{U}^\intercal \mathcal{I}^{-1} \boldsymbol{U}= \frac{U^2}{\mathcal{I}} = \frac{(\bar{Y}-\mu)^2}{\sigma^2/n} \sim \chi^2(1).\label{eqn:scoredistnormalmultiv}\end{align} -->
<!-- This sampling distribution can be used obtain interval estimates for $\mu$ of the form $\bar{y} \pm 1.96 \sigma /\sqrt{n} $ where $\sigma^2$ is assumed to be known. -->
<!-- \end{example} -->

<!-- \begin{example}[Binomial distribution] -->
<!-- For $Y \sim Bin(n,p)$, the log-likelihood function is -->
<!--  \begin{align}l(p; y) = y \log p+(n-y) \log(1-p)+ \log \binom{n}{y}.\label{eqn:binomialloglik}\end{align} -->
<!--  The score statistic is \begin{align}U=\frac{dl}{dp}= \frac{Y}{p}-\frac{n-Y}{1-p} = \frac{Y-np}{p(1-p)},\label{eqn:scorebinomial} -->
<!--  \end{align} -->
<!--    and the expectation of $U$ is $0$ since $E(Y)=np$. -->
<!-- Also, $\mathrm{Var}(Y)=np(1-p)$, so \begin{align}\mathcal{I}=\mathrm{Var}(U) = \frac{1}{p^2(1-p)^2} \mathrm{Var}(Y)=\frac{n}{p(1-p)}.\label{eqn:infbinomial}\end{align} -->
<!-- Therefore a large sample approximation for $U$ is \begin{align}\frac{U}{\sqrt{\mathcal{I}}} = \frac{Y-np}{\sqrt{np(1-p)}} \overset{\text{approx}}{\sim} N(0,1).\label{eqn:scoredistbinom}\end{align} -->
<!-- This is the normal approximation to the binomial distribution (without a continuity correction). It can be used to obtain approximate confidence intervals for $p$. -->
<!-- \end{example} -->

### Sampling distribution of the MLE
<!-- To obtain an asymptotic distribution of the MLE in a generalised linear model, we need to consider some Taylor series expansions of the log-likelihood and its derivatives.\\ -->

<!-- \textbf{Reminder:} The \textbf{Taylor series expansion} of a function $f(x)$ about a value $t$ is given by $f(x)= f(t)+ (x-t) \left. \frac{df}{dx} \right|_{x=t}+ \frac{1}{2} (x-t)^2 \left. \frac{d^2f}{dx^2} \right|_{x=t} + \dots$ provided that $x$ is near $t$.\\ -->


<!-- Let's begin with an approximation for the log-likelihood: -->
<!-- Consider the first three terms of the expansion \begin{align*}l(\beta) & \approx l(\hat{\beta})+ (\beta-\hat{\beta}) \left. \frac{dl}{d\beta} \right|_{\beta=\hat{\beta}}+ \frac{1}{2} (\beta-\hat{\beta})^2 \left. \frac{d^2l}{d\beta^2} \right|_{\beta=\hat{\beta}} \\ & \approx l(\hat{\beta}) + (\beta-\hat{\beta}) U(\hat{\beta}) +  \frac{1}{2} (\beta-\hat{\beta})^2 U'(\hat{\beta})\end{align*} -->

<!-- Replacing $U'=d^2l/ d\beta^2$ by its expected value, $E(U')=-\mathcal{I}$, the approximation becomes  $l(\beta) \approx l(\hat{\beta}) + (\beta-\hat{\beta}) U(\hat{\beta}) -  \frac{1}{2} (\beta-\hat{\beta})^2 \mathcal{I}(\hat{\beta}).$ -->

<!--  For a vector $\boldsymbol{\beta}$ of parameters this generalizes to \begin{align} l(\boldsymbol{\beta}) \approx l(\hat{\boldsymbol{\beta}}) + (\boldsymbol{\beta}-\hat{\boldsymbol{\beta}})^\intercal \boldsymbol{U}(\hat{\boldsymbol{\beta}}) -  \frac{1}{2} (\boldsymbol{\beta}-\hat{\boldsymbol{\beta}})^\intercal \mathcal{I}(\hat{\boldsymbol{\beta}})(\boldsymbol{\beta}-\hat{\boldsymbol{\beta}}). \label{eqn:likelihoodexp} \end{align} -->

<!-- Similarly for the score function with a single parameter $\beta$ the Taylor series expansion is \begin{align*}U(\beta) & \approx U(\hat{\beta})+ (\beta-\hat{\beta})U'(\hat{\beta}).\end{align*} -->
<!-- Approximating $U'$ by its expectation $E(U')=-\mathcal{I}$ this becomes -->
<!-- \begin{align*}U(\beta) & \approx U(\hat{\beta})- (\beta-\hat{\beta})\mathcal{I}(\hat{\beta}).\end{align*} -->
<!-- For a vector $\boldsymbol{\beta}$ of parameters this generalizes to \begin{align} U(\boldsymbol{\beta}) \approx U(\hat{\boldsymbol{\beta}}) -\mathcal{I}(\hat{\boldsymbol{\beta}}) (\boldsymbol{\beta}-\hat{\boldsymbol{\beta}}).\label{eqn:scoreexp}\end{align} -->

<!-- Equation (\ref{eqn:scoreexp}) can be used to obtain the sampling distribution of the MLE $\hat{\boldsymbol{\beta}}$.  Since $U(\hat{\boldsymbol{\beta}})=\boldsymbol{0}$, we have  $ U(\boldsymbol{\beta}) \approx  -\mathcal{I}(\hat{\boldsymbol{\beta}}) (\boldsymbol{\beta}-\hat{\boldsymbol{\beta}}) \Rightarrow (\hat{\boldsymbol{\beta}} - \boldsymbol{\beta}) = \mathcal{I}^{-1} \boldsymbol{U} $ assuming $\mathcal{I}$ is not singular. Taking the expectation (regarding $\mathcal{I}$ as a constant), we have that asymptotically $\hat{\boldsymbol{\beta}}- \boldsymbol{\beta}=\boldsymbol{0}$ since $E(\boldsymbol{U})=0$. Thus $E(\hat{\boldsymbol{\beta}})=\boldsymbol{\beta}$ (asymptotically) and hence the MLE is asymptotically unbiased. -->
<!--  The variance-covariance matrix for $\hat{\boldsymbol{\beta}}$ is $E\left[(\hat{\boldsymbol{\beta}} - \boldsymbol{\beta})(\hat{\boldsymbol{\beta}} - \boldsymbol{\beta})^\intercal  \right]= \mathcal{I}^{-1} E(\boldsymbol{U}\boldsymbol{U}^\intercal) \mathcal{I}^{-1} = \mathcal{I}^{-1}$ since $E(\boldsymbol{U}\boldsymbol{U}^\intercal)=\mathcal{I}$ and $(\mathcal{I}^{-1})^\intercal =\mathcal{I}^{-1}$ since $\mathcal{I}$ is symmetric. -->
<!-- Then by equation (\ref{eqn:approxnormgeneral}), -->

The asymptotic (large sample) distribution for $\hat{\boldsymbol{\beta}}$ is
  \begin{align}
   (\hat{\boldsymbol{\beta}} - \boldsymbol{\beta})^\intercal \mathcal{I}(\hat{\boldsymbol{\beta}}) (\hat{\boldsymbol{\beta}} - \boldsymbol{\beta}) \overset{\text{approx}}{\sim} \chi^2(p) \label{eqn:waldchi}   \end{align} or equivalently \begin{align}\hat{\boldsymbol{\beta}} \overset{\text{approx}}{\sim} N_p(\boldsymbol{\beta}, \mathcal{I}^{-1}),\label{eqn:waldnormal}\end{align} where $\chi^2(p)$ refers to the chi-squared distribution with $p$ degrees of freedom and $N_p(\boldsymbol{\beta},\mathcal{I}^{-1})$ refers to a multivariate ($p$-dimensional) normal distribution with vector mean $\boldsymbol{\beta}$ and $p \times p$-dimensional variance-covariance matrix $\mathcal{I}^{-1}$.

In the one-dimensional case, what this says is that the MLE $\hat{\beta}$ is approximately normally distributed with mean $\beta$ and variance $\mathcal{I}^{-1}$.

This allows us to perform hypothesis tests and construct confidence intervals for the model parameterss.

###[definition] Wald statistic
The *Wald statistic*, also known as the *z-statistic*, for each of the model parameters $\left \lbrace \beta_j\right \rbrace,~j=1,\dots,p$, is equal to the coefficient estimate, $\hat{\beta}_j$, over its  standard error, $\text{se}(\hat{\beta}_j)$.
###[/definition]

Under the null hypothesis $H_0: \beta_j=0$, and using the asymptotic normality result for the MLE, the Wald statistic is approximately distributed as standard normal. In other words, we have that \[z=\frac{\hat{\beta}_j}{\text{se}(\hat{\beta}_j)} \overset{\text{approx}}{\sim} N(0,1)\]This allows us to perform the *Wald test*, which compares the z-statistic to the upper percentile of a standard normal distribution.

Also using this asymptotic normality result, we can construct an approximate 95\% confidence interval for $\beta_j$ by taking \[\hat{\beta}_j\pm 1.96\text{se}(\hat{\beta}_j).\] Here 1.96 is the 97.5th percentile of the standard normal distribution.

###[example] Hypothesis test and confidence interval for the GPA coefficient in the model for medical school admissions
Recall the logistic regression model for the medical school admissions data. In the output we see the MLEs of $\beta_0$ and $\beta_1$ in the `Estimate` column. These are obtained by solving the likelihood equations numerically using Fisher's scoring method (notice the `Number of Fisher Scoring iterations` information towards the end.)
```{r echo=c(4:6)}
library(Stat2Data)
data(MedGPA)
med.glm <- glm(Acceptance ~ GPA, data=MedGPA, family=binomial)
summary(med.glm)
```
We can also see the standard errors for $\hat{\beta_0}$ `(Intercept)` and $\hat{\beta_1}$ (GPA coefficient). These are obtained from the observed information matrix, which is computed during the iterative estimation procedure. Remember that the variance of $\boldsymbol{\beta}$ is estimated by $\mathcal{I}^{-1}$, the inverse of the information matrix. The function `vcov()`returns this estimated variance-covariance matrix of the model coefficients:
```{r}
vcov(med.glm)
```
The diagonal entries of this matrix are the estimated variances for $\hat{\beta_0}$ and $\hat{\beta_1}$ and the off-diagonal entries give the estimated covariance between $\hat{\beta_0}$ and $\hat{\beta_1}$. The standard errors shown in the output (`Std. Error` column) are the square roots of the estimated variances:
```{r}
sqrt(diag(vcov(med.glm)))
```
The `z value` column gives the Wald statistics that test the hypotheses $H_0: \beta_0=0$ and $H_0: \beta_1=0$. Usually we are not as interested in testing whether the intercept term is zero or not and we focus instead on the coefficients of the explanatory variables in the model. So for $H_0: \beta_1=0$ the $z$ value equals 3.454 and is obtained by taking the coefficient estimate and dividing by its standard error $$z=\frac{\hat{\beta_1}}{\text{se}(\hat{\beta}_1)}=\frac{5.454}{1.579}=3.454.$$ Under $H_0$, $z$ should approximately follow the standard normal distribution, $N(0,1)$. The $p$-value shown as `Pr(>|z|)` in the output is the probability of obtaining a value as extreme as $z$ or larger in absolute value, assuming the null hypothesis is true. A small $p$-value indicates that the $z$ value is unlikely to come from a $N(0,1)$ distribution and leads to rejecting $H_0$. As the $p$-value for the GPA coefficient is small (0.000553) we can therefore conclude that the GPA coefficient differs significantly from zero, ie. that the GPA term is worth keeping in the model.

To obtain an approximate 95\% confidence interval for the GPA coefficient, we take $\hat{\beta}_j\pm 1.96\text{se}(\hat{\beta}_j)$:

```{r}
5.454-1.96*1.579
5.454+1.96*1.579
```
The resulting interval is $(2.36,8.55)$, and since it does not include zero, we conclude that the GPA coefficient is significant.

###[/example]
<!-- \begin{example}[MLE for the normal linear model] -->
<!-- Consider the model $E(Y_i)= \mu_i = \boldsymbol{x}_i^{\intercal} \boldsymbol{\beta}$ where $Y_i \sim N(\mu_i, \sigma^2)$  for $i =1, \dots, n$ and $\boldsymbol{\beta}$ is a $p$-dimensional vector of parameters. This is a GLM with the identity link. Since $\eta_i =g(\mu_i)=\mu_i$ we have $g'(\mu_i)=1$. The elements of the information matrix therefore are $\mathcal{I}_{jk}=\sum_{i=1}^n \frac{x_{ij}x_{ik}}{\mathrm{Var}(Y_i) (g'(\mu_i))^2} =\sum_{i=1}^n \frac{x_{ij}x_{ik}}{\sigma^2}$ Hence \begin{align} -->
<!--               \mathcal{I} =\frac{1}{\sigma^2} \boldsymbol{X}^\intercal \boldsymbol{X} \label{eqn:informationnormal} -->
<!--              \end{align} -->
<!-- Similarly -->
<!--  $\eta_i+z_i = \mu_i+ (y_i-\mu_i) g'(\mu_i)=y_i$ -->
<!--  The estimating equation  $\boldsymbol{X}^\intercal \boldsymbol{W} \boldsymbol{X} \hat{\boldsymbol{\beta}}^{(m)}= \boldsymbol{X}^\intercal \boldsymbol{W} (\boldsymbol{\eta}+\boldsymbol{z}) $ thus simplifies to  $\frac{1}{\sigma^2}\boldsymbol{X}^\intercal \boldsymbol{X} \hat{\boldsymbol{\beta}}=\frac{1}{\sigma^2} \boldsymbol{X}^\intercal \boldsymbol{y} \Rightarrow \hat{\boldsymbol{\beta}}=(\boldsymbol{X}^\intercal \boldsymbol{X})^{-1} \boldsymbol{X}^\intercal \boldsymbol{y}. $ -->
<!--  If we write the normal linear model as -->
<!--   $\boldsymbol{y} \sim MVN (\boldsymbol{X}\boldsymbol{\beta}, \sigma^2 \boldsymbol{I})$ where $\boldsymbol{I}$ is the $n\times n$ identity matrix, we have that $E(\hat{\boldsymbol{\beta}}) = E[(\boldsymbol{X}^\intercal \boldsymbol{X})^{-1} \boldsymbol{X}^\intercal \boldsymbol{y}]=  (\boldsymbol{X}^\intercal \boldsymbol{X})^{-1} \boldsymbol{X}^\intercal \boldsymbol{X}\boldsymbol{\beta} = \boldsymbol{\beta}, $ -->
<!--  so $\hat{\boldsymbol{\beta}}$ is an unbiased estimator of $\boldsymbol{\beta}$. -->

<!--  To obtain the variance-covariance matrix for $\hat{\boldsymbol{\beta}}$ we use $$(\hat{\boldsymbol{\beta}}-\boldsymbol{\beta}) &= (\boldsymbol{X}^\intercal \boldsymbol{X})^{-1} \boldsymbol{X}^\intercal \boldsymbol{y}-\boldsymbol{\beta} \\                                                                       &=    (\boldsymbol{X}^\intercal \boldsymbol{X})^{-1} \boldsymbol{X}^\intercal (\boldsymbol{y}-\boldsymbol{X}\boldsymbol{\beta}) $$  -->

<!--  Hence, -->
<!--  \begin{align*} -->
<!--  & E[(\hat{\boldsymbol{\beta}}-\boldsymbol{\beta})  (\hat{\boldsymbol{\beta}}-\boldsymbol{\beta})\intercal] \\ -->
<!--  & = (\boldsymbol{X}^\intercal \boldsymbol{X})^{-1} \boldsymbol{X}^\intercal E[(\boldsymbol{y}-\boldsymbol{X}\boldsymbol{\beta})(\boldsymbol{y}-\boldsymbol{X}\boldsymbol{\beta})\intercal]\boldsymbol{X}  (\boldsymbol{X}^\intercal \boldsymbol{X})^{-1}\\ & = (\boldsymbol{X}^\intercal \boldsymbol{X})^{-1} \boldsymbol{X}^\intercal \mathrm{Var}(\boldsymbol{y})  \boldsymbol{X}  (\boldsymbol{X}^\intercal \boldsymbol{X})^{-1}  \\   &= \sigma^2      (\boldsymbol{X}^\intercal \boldsymbol{X})^{-1}\\ -->
<!--  &= \mathcal{I}^{-1} \hspace{5cm} \text{ by (\ref{eqn:informationnormal})} \end{align*} -->
<!-- In this case, the exact sampling distribution of $\hat{\boldsymbol{\beta}}$ is $N(\boldsymbol{\beta}, \mathcal{I}^{-1})$ or equivalently $(\hat{\boldsymbol{\beta}}-\boldsymbol{\beta})^\intercal \mathcal{I}(\hat{\boldsymbol{\beta}}-\boldsymbol{\beta}) \sim \chi^2(p)$. -->
<!-- \end{example} -->

###Deviance

To assess the adequacy of a model of interest, we compare it with the \textbf{saturated} (or \textbf{full}) model, which has the maximum number of parameters that can be estimated. For data with $n$ observations, $y_1,\dots, y_n$, each with a different parameter in $\boldsymbol{X}^\intercal \boldsymbol{\beta}$, the saturated  model can be specified with $n$ parameters. If we have replicates, the maximum number of parameters in the saturated model can be less than $n$. Let $m$ be the maximum number of parameters that can be estimated, and $\boldsymbol{\beta}_{\max}$ and $\hat{\boldsymbol{\beta}}_{\max}$ be the corresponding parameter vector and MLE.

Let $L(\hat{\boldsymbol{\beta}}_{\max};\boldsymbol{y})$ be the likelihood evaluated at $\hat{\boldsymbol{\beta}}_{\max}$, that is the likelihood for the full model. Let $L(\hat{\boldsymbol{\beta}};\boldsymbol{y})$ be the maximum value of the likelihood for a model of interest.
The **likelihood ratio** $$\lambda=\frac{L(\hat{\boldsymbol{\beta}}_{\max};\boldsymbol{y})}{L(\hat{\boldsymbol{\beta}};\boldsymbol{y})}$$ provides a measure of how well the model of interest fits compared with the full model.
In practice, we often use the logarithm of the likelihood ratio:
 $\log \lambda = l(\hat{\boldsymbol{\beta}}_{\max};\boldsymbol{y})-l(\hat{\boldsymbol{\beta}};\boldsymbol{y})$. Large values of $\log \lambda$ suggest that the model of interest is a poor description of the data relative to the full model. How large a value of $\log \lambda$? To answer this question we need to obtain a critical region using the sampling distribution of $\log \lambda$. In fact, we will work with the quantity $2\log \lambda$, which is called the \textbf{deviance}.
###[definition] Deviance
The deviance, $D$, is defined as $D=2\log \lambda =2[ l(\hat{\boldsymbol{\beta}}_{\max};\boldsymbol{y})-l(\hat{\boldsymbol{\beta}};\boldsymbol{y})]$ where $l(\hat{\boldsymbol{\beta}}_{\max};\boldsymbol{y})$ is the maximised log-likelihood for the saturated model and $l(\hat{\boldsymbol{\beta}};\boldsymbol{y})$ is the maximised log-likelihood for the model of interest.
###[/definition]

<!-- Recall the Taylor series expansion for the log-likelihood (\ref{eqn:likelihoodexp}): $(\boldsymbol{\beta}) \approx l(\hat{\boldsymbol{\beta}}) + (\boldsymbol{\beta}-\hat{\boldsymbol{\beta}})^\intercal U(\hat{\boldsymbol{\beta}}) -  \frac{1}{2} (\boldsymbol{\beta}-\hat{\boldsymbol{\beta}})^\intercal \mathcal{I}(\hat{\boldsymbol{\beta}})(\boldsymbol{\beta}-\hat{\boldsymbol{\beta}}) $ -->
<!-- Since $\hat{\boldsymbol{\beta}}$ is the MLE, $U(\hat{\boldsymbol{\beta}})=\boldsymbol{0}$ and hence this becomes  $l(\boldsymbol{\beta})- l(\hat{\boldsymbol{\beta}}) \approx  -  \frac{1}{2} (\boldsymbol{\beta}-\hat{\boldsymbol{\beta}})^\intercal \mathcal{I}(\hat{\boldsymbol{\beta}}) (\boldsymbol{\beta}-\hat{\boldsymbol{\beta}}).$ -->
<!-- Therefore, the statistic $2[ l(\hat{\boldsymbol{\beta}};\boldsymbol{y})-l(\boldsymbol{\beta};\boldsymbol{y})] \approx  (\boldsymbol{\beta}-\hat{\boldsymbol{\beta}})^\intercal \mathcal{I}(\hat{\boldsymbol{\beta}}) (\boldsymbol{\beta}-\hat{\boldsymbol{\beta}})$ will be approximately $\chi^2(p)$. -->
<!-- For the deviance, write \begin{align*}D&=2[ l(\hat{\boldsymbol{\beta}}_{\max};\boldsymbol{y})-l(\hat{\boldsymbol{\beta}};\boldsymbol{y})]\\ -->
<!--  &=2[ l(\hat{\boldsymbol{\beta}}_{\max};\boldsymbol{y})-l(\boldsymbol{\beta}_{\max};\boldsymbol{y})]\\ -->
<!--  & \hspace{1cm} -2[ l(\hat{\boldsymbol{\beta}};\boldsymbol{y})-l(\boldsymbol{\beta};\boldsymbol{y})]+2 \left[ l(\boldsymbol{\beta}_{\max};\boldsymbol{y})-l(\boldsymbol{\beta};\boldsymbol{y}) \right]    .                    \end{align*} -->
<!--  $2[ l(\hat{\boldsymbol{\beta}}_{\max};\boldsymbol{y})-l(\boldsymbol{\beta}_{\max};\boldsymbol{y})]$ has the $\chi^2(m)$ distribution, where $m$ is the number of parameters in the full (saturated) model. -->
<!--  $2[ l(\hat{\boldsymbol{\beta}};\boldsymbol{y})-l(\boldsymbol{\beta};\boldsymbol{y})]$ is $\chi^2(p)$, where $p$ is the number of parameters in the model of interest. -->
<!--  $v=2 \left[ l(\boldsymbol{\beta}_{\max};\boldsymbol{y})-l(\boldsymbol{\beta};\boldsymbol{y}) \right]$ is a constant which should be close to zero if the model of interest fits the data almost as well as the full model. -->
For a GLM that fits the data well, the approximate distribution of the deviance, $D$, is $\chi^2(m-p)$ where $m$ is the number of parameters in the saturated model and $p$ is the number of parameters in the model of interest.

###[example] Deviance for a binomial model
Suppose $Y_1, \dots, Y_n$ are independent with $Y_i \sim Bin(n_i, p_i)$ for $i=1,\dots, n$.  The log-likelihood is  \begin{align}l(\boldsymbol{\beta}; \boldsymbol{y}) = \sum_{i=1}^n \left[y_i \log p_i-y_i \log(1-p_i)+ n_i \log(1-p_i) +\log \binom{n_i}{y_i} \right] \label{eqn:binomloglik} \end{align}
For the full model the $p_i$'s are all different, so $\boldsymbol{\beta}=(p_1, \dots, p_n)^\intercal$ and we can show that $\hat{p}_i=y_i/n_i$. This gives
  \begin{align}l(\hat{\boldsymbol{\beta}}_{\max}; \boldsymbol{y}) &= \sum \left[y_i \log \left(\frac{y_i}{n_i}\right)-y_i \log\left(\frac{n_i-y_i}{n_i}\right)  + n_i \log\left(\frac{n_i-y_i}{n_i}\right) +\log \binom{n_i}{y_i} \right].\label{eqn:binomloglikfullmodel}\end{align}
For any model with $p<n$ parameters, the MLE of $p_i$ is $\hat{p}_i$ and the fitted values are $\hat{y_i}=n_i \hat{p_i}$.  This gives
  \begin{align}l(\hat{\boldsymbol{\beta}}; \boldsymbol{y}) &= \sum_{i=1}^n \left[y_i \log \left(\frac{\hat{y}_i}{n_i}\right)-y_i \log\left(\frac{n_i-\hat{y}_i}{n_i}\right)+n_i \log\left(\frac{n_i-\hat{y}_i}{n_i}\right) +\log \binom{n_i}{y_i} \right].\end{align}
Thus, the deviance is \begin{align}
               D&=2[ l(\hat{\boldsymbol{\beta}}_{\max};\boldsymbol{y})-l(\hat{\boldsymbol{\beta}};\boldsymbol{y})]= 2\sum_{i=1}^n \left[ y_i \log\left(\frac{y_i}{\hat{y}_i}\right)+ (n_i - y_i)  \log\left(\frac{n_i-y_i}{n_i-\hat{y}_i}\right) \right]. \label{eqn:binomdev}
              \end{align}
###[/example]

###[example] Deviance for a normal linear model
Suppose $Y_1, \dots, Y_n$ are independent with $Y_i \sim N(\mu_i, \sigma^2)$ and $E(Y_i)= \mu_i=\boldsymbol{X}^\intercal_i  \boldsymbol{\beta}$ for $i=1,\dots, n$ .
 The log-likelihood function is
 \begin{align}l(\boldsymbol{\beta}; \boldsymbol{y}) = -\frac{1}{2\sigma^2}\sum_{i=1}^n(y_i -\mu_i)^2 -\frac{1}{2}n \log(2\pi \sigma^2).\label{eqn:normalloglik}\end{align}
For the saturated model, we have $n$ parameters $\mu_1, \dots, \mu_n$. The MLEs are $\hat{\mu}_i=y_i$ and so the maximum value of the log-likelihood becomes
 \begin{align}l(\hat{\boldsymbol{\beta}}_{\max}; \boldsymbol{y}) =  -\frac{1}{2}n \log(2\pi \sigma^2). \label{eqn:normloglikfullmodel}\end{align}
 For any other model with $p<n$ parameters, the MLE of $\boldsymbol{\beta}$ is $\hat{\boldsymbol{\beta}} = (\boldsymbol{X}^\intercal \boldsymbol{X})^{-1} \boldsymbol{X}^\intercal \boldsymbol{y}.$
The corresponding maximised log-likelihood function is
 \begin{align}l(\hat{\boldsymbol{\beta}}; \boldsymbol{y}) = -\frac{1}{2\sigma^2}\sum_{i=1}^n(y_i -\boldsymbol{x}_i^{\intercal} \hat{\boldsymbol{\beta}})^2 -\frac{1}{2}n \log(2\pi \sigma^2).\end{align}
 The deviance then is \begin{align}
               D&=2[ l(\hat{\boldsymbol{\beta}}_{\max};\boldsymbol{y})-l(\hat{\boldsymbol{\beta}};\boldsymbol{y})]= \frac{1}{\sigma^2}\sum_{i=1}^n \left(y_i-\boldsymbol{x}_i^{\intercal} \hat{\boldsymbol{\beta}}\right)^2
                =\frac{1}{\sigma^2} \sum_{i=1}^n \left(y_i-\hat{\mu}_i\right)^2. \label{eqn:normaldev}
              \end{align}
<!-- In the case when $E(Y_i)=\mu$ (null model), we have $\hat{\mu}_i=\hat{\mu}=\bar{y}$ and -->
<!-- \begin{align*} -->
<!--   D =\frac{1}{\sigma^2} \sum_{i=1}^n \left(y_i-\bar{y}\right)^2. -->
<!-- \end{align*} -->
<!-- Note the relationship to the sample variance $S^2$: -->
<!-- $S^2 = \frac{1}{n-1}\sum_{i=1}^n \left(y_i-\bar{y}_i\right)^2=\frac{\sigma^2 D}{n-1}.$ -->

<!-- The distribution of the sample variance is given by $(n-1)S^2/\sigma^2 \sim \chi^2(n-1),$ so $D \sim \chi^2(n-1).$ -->

<!-- More generally, when $\mu_i=\boldsymbol{x}_i^{\intercal} \hat{\boldsymbol{\beta}}$, we have -->
<!-- \begin{align*} -->
<!--   D =\frac{1}{\sigma^2} \sum_{i=1}^n \left(y_i-\boldsymbol{x}_i^{\intercal} \hat{\boldsymbol{\beta}}\right)^2=\frac{1}{\sigma^2} (\boldsymbol{y}-\boldsymbol{X} \hat{\boldsymbol{\beta}})^\intercal (\boldsymbol{y}-\boldsymbol{X} \hat{\boldsymbol{\beta}}). -->
<!-- \end{align*} -->
<!-- Since the MLE is $\hat{\boldsymbol{\beta}}= (\boldsymbol{X}^\intercal \boldsymbol{X})^{-1} \boldsymbol{X}^\intercal \boldsymbol{y}$, the term $(\boldsymbol{y}-\boldsymbol{X} \hat{\boldsymbol{\beta}})$ can be written as -->
<!-- \begin{align*}\boldsymbol{y}-\boldsymbol{X} \hat{\boldsymbol{\beta}} &= \boldsymbol{y}-\boldsymbol{X} (\boldsymbol{X}^\intercal \boldsymbol{X})^{-1} \boldsymbol{X}^\intercal \boldsymbol{y}\\ -->
<!--  &= [\boldsymbol{I} - \boldsymbol{X} (\boldsymbol{X}^\intercal \boldsymbol{X})^{-1} \boldsymbol{X}^\intercal]\boldsymbol{y}=[\boldsymbol{I}-\boldsymbol{H}]\boldsymbol{y} -->
<!-- \end{align*} where $\boldsymbol{I}$ is the identity matrix and $\boldsymbol{H}= \boldsymbol{X}(\boldsymbol{X}^\intercal \boldsymbol{X})^{-1} \boldsymbol{X}^\intercal$ is the \textbf{hat} matrix. -->
<!-- Therefore -->
<!-- \begin{align*} -->
<!--   D &=\frac{1}{\sigma^2} (\boldsymbol{y}-\boldsymbol{X} \hat{\boldsymbol{\beta}})^\intercal (\boldsymbol{y}-\boldsymbol{X} \hat{\boldsymbol{\beta}})\\ -->
<!--   &=\frac{1}{\sigma^2}\left([\boldsymbol{I}-\boldsymbol{H}]\boldsymbol{y}\right)^\intercal [\boldsymbol{I}-\boldsymbol{H}]\boldsymbol{y}\\ -->
<!--   &= \frac{1}{\sigma^2} \boldsymbol{y}^\intercal (\boldsymbol{I}-\boldsymbol{H})\boldsymbol{y} -->
<!-- \end{align*} -->
<!-- since $\boldsymbol{H}$ is idempotent; that is $\boldsymbol{H}^\intercal = \boldsymbol{H}$ and $\boldsymbol{H}\boldsymbol{H}=\boldsymbol{H}$. -->
<!-- $D  = \dfrac{1}{\sigma^2} \boldsymbol{y}^\intercal (\boldsymbol{I}-\boldsymbol{H}) \boldsymbol{y}$ is a quadratic form, with rank equal to the rank of the matrix $\boldsymbol{I}-\boldsymbol{H}$. -->
<!-- The rank of $\boldsymbol{I}$ is $n$ and that of $\boldsymbol{H}$ is $p$, so the rank of $\boldsymbol{I}-\boldsymbol{H}$  is $n-p$. (For more details consult your Predictive Modelling notes.) -->

It turns out that the (exact) distribution of $D$ is $\chi^2(n-p)$.
If the model fits the data well, then $D \sim \chi^2(n-p)$, and the expected value of $D$ will be $n-p$, since the expectation of a chi-squared random variable is equal to its degrees of freedom.

However, we are not able to use this chi-squared distribution directly, because the expression for the deviance contains the nuisance parameter $\sigma^2$. As you may remember from your linear regression courses, we end up using F tests instead.

<!-- From the expression $D =\frac{1}{\sigma^2} \sum_{i=1}^n \left( y_i-\hat{\mu}_i\right)^2,$ we can obtain an estimate of $\sigma^2$: -->
<!--  $\tilde{\sigma}^2=\frac{ \sum_{i=1}^n \left( y_i-\hat{\mu}_i\right)^2}{n-p}.$ -->
###[/example]

###[example] Deviance for a Poisson model
Let $Y_1, \dots, Y_n$ be independent random variables with $Y_i \sim Po(\mu_i)$.
Then the log-likelihood function is \begin{align}l(\boldsymbol{\beta}; \boldsymbol{y}) = \sum y_i \log \mu_i - \sum \mu_i -\sum \log (y_i!).\label{en:poissonloglik}\end{align}
For the full model $\boldsymbol{\beta}_{\max}= (\mu_1,\dots, \mu_n)^\intercal$, $\hat{\mu}_i=y_i$, and the maximum value of the log-likelihood is
  \begin{align}l(\hat{\boldsymbol{\beta}}_{\max}; \boldsymbol{y}) = \sum y_i \log y_i - \sum y_i -\sum \log (y_i!).\label{eqn:poissonloglikfullmodel}\end{align}
Suppose that for the model of interest with $p<n$ parameters the MLE, $\hat{\boldsymbol{\beta}}$, can be used to obtain $\hat{\mu}_i$ and hence fitted values $\hat{y}_i=\hat{\mu}_i$ (because $E(Y_i)=\mu_i$).
 For the model of interest the maximum value of the log-likelihood is
  \begin{align}l(\hat{\boldsymbol{\beta}}; \boldsymbol{y}) = \sum y_i \log \hat{y}_i - \sum \hat{y}_i -\sum \log (y_i!).\end{align}
  Hence, the deviance is
  \begin{align}D= 2 [l(\hat{\boldsymbol{\beta}}_{\max}; \boldsymbol{y}) -l(\hat{\boldsymbol{\beta}}; \boldsymbol{y}) ]= 2 \left[\sum y_i \log\left(\frac{y_i} {\hat{y}_i}\right)-\sum (y_i-\hat{y_i}) \right] \label{eqn:devpoisson}\end{align}
 For most models $\sum y_i=\sum \hat{y_i}$, so the deviance can be written as
  \begin{align}D=2 \sum y_i \log\left(\frac{y_i} {\hat{y}_i}\right)= 2 \sum o_i \log\left(\frac{o_i} {e_i}\right),\end{align}
  where $o_i$ denotes the observed value and $e_i$ the expected value of $y_i$.
 The deviance can be computed from the data and compared with the $\chi^2(n-p)$ distribution.


Consider the data below for which the $Y_i$ are assumed to be independent observations from a Poisson distribution.
 \begin{center}
\begin{tabular}{crrrrrrrrrr}
\hline
$y_i$ & 2 & 3 & 6 & 7 & 8 & 9 & 10& 12 & 15\\
$x_i$ & -1 & -1 & 0 & 0 &0 & 0 & 1 & 1 & 1 \\
\hline
\end{tabular}
\end{center}
We fit a model of the form $\mu_i=\beta_1 + \beta_2 x_i$.
  The fitted values are $\hat{y}_i=\hat{\beta}_1 + \hat{\beta}_2 x_i$ where $\hat{\beta}_1 =7.45163$ and $\hat{\beta}_2= 4.93530$.
 The deviance is $1.8947$, which is small compared with $n-p=7$, indicating no lack of fit.
###[/example]

###[task]

Verify the value 1.8947 for the deviance given in the above example.

####[answer]
We need to get the fitted values $\hat{y}_i$ for $i=1,\dots,9$ and then substitute into the expression for the deviance.
\begin{center}
\begin{tabular}{crrrrrrrrrr}
\hline
$y_i$ & 2 & 3 & 6 & 7 & 8 & 9 & 10& 12 & 15\\
$\hat{y}_i$ & 2.51633 & 2.51633 &7.45163 & 7.45163 &7.45163 & 7.45163& 12.38693 & 12.38693 & 12.38693 \\
$y_i \log\left(\frac{y_i}{\hat{y}_i} \right)$ & -0.45931 & 0.52743 & -1.30004 & -0.43766 & 0.56807 & 1.69913 & -2.14057 & -0.38082 & 2.87115\\
\hline
\end{tabular}
\end{center}
So $\sum_{i=1}^9 y_i \log\left(\frac{y_i} {\hat{y}_i}\right)=0.94738$ and $D=2\sum_{i=1}^9 y_i \log\left(\frac{y_i} {\hat{y}_i}\right)=1.89476$.
####[/answer]
###[/task]


### Hypothesis testing using the deviance

As we've already seen, we can test hypotheses about the $p$-dimensional parameter vector $\boldsymbol{\beta}$ by using the Wald statistic and the asymptotic distribution of the MLE.

Alternatively we can compare **nested** models $M_0$ and $M_1$ using the difference of their deviances.


Consider $H_0: \boldsymbol{\beta}= \boldsymbol{\beta}_0 = (\beta_1, \dots, \beta_q)^\intercal$ corresponding to $M_0$ and $H_1: \boldsymbol{\beta} = \boldsymbol{\beta}_1=(\beta_1, \dots, \beta_p)^\intercal$ corresponding to $M_1$ with $q<p<n$.
 Test $H_0$ against $H_1$ by considering \begin{align*} D_0-D_1& =2[l(\hat{\boldsymbol{\beta}}_{\max}; \boldsymbol{y}) -l(\hat{\boldsymbol{\beta}}_0; \boldsymbol{y})]-2[l(\hat{\boldsymbol{\beta}}_{\max}; \boldsymbol{y}) -l(\hat{\boldsymbol{\beta}}_1; \boldsymbol{y})] \\ &=2[ l(\hat{\boldsymbol{\beta}}_1; \boldsymbol{y}) -l(\hat{\boldsymbol{\beta}}_0; \boldsymbol{y})]\end{align*}
 If both models describe the data well then $D_0 \sim \chi^2(n-q)$, $D_1 \sim \chi^2(n-p)$ and $D_0-D_1 \sim \chi^2(p-q)$.
 If $M_1$ describes the data well but $M_0$ does not, then $D_0-D_1$ will be larger than expected for a value from $\chi^2(p-q)$.
 So, reject $H_0$ if $D_0-D_1> \chi^2(1-\alpha; p-q)$ that is, if the difference in deviances exceeds the upper $100\times \alpha \%$ point of the $\chi^2(p-q)$ distribution.


###[example] Hypothesis test for the GPA coefficient in the model for medical school admissions, this time using deviances

Suppose that we want to test $H_0: \beta_1=0$ in the medical school admissions example. We can perform this test using the deviances given in the output.
```{r echo=FALSE}
summary(med.glm)
```
Here $D_0$ is the null deviance, that is the deviance in the model that includes only the intercept (and no other predictors). $D_1$ is the residual deviance, that is the deviance of the model of interest (the model with GPA included as a predictor). Under $H_0$ $D_0-D_1$ should be approximately distributed as $\chi^2(1)$. The 95th percentile of the $\chi^2(1)$ distribution is $\chi^2(0.95; 1)=3.84$, and as $D_0-D_1=75.791-56.839=18.952>3.84$ we can reject the null hypothesis. Again we conclude that GPA is a significant term in the model.

###[/example]

<!-- \begin{example}[Normal linear model] -->
<!-- Consider the model $E(Y_i)=\mu_i= \boldsymbol{x}_i^{\intercal} \boldsymbol{\beta}, \hspace{1cm} Y_i \sim N(\mu_i, \sigma^2)$ where the $Y_i, i=1,\dots, n$ are independent. -->
<!-- Suppose that model $M_1$ has $p$ parameters and model $M_0$ has $q$ parameters and that the fitted values from each model are denoted by $\hat{\mu}_i(1)$ and  $\hat{\mu}_i(0)$ respectively. -->
<!--  Then -->
<!--  $ D_0 =\frac{1}{\sigma^2} \sum_{i=1}^n \left[ y_i-\hat{\mu}_i(0)\right]^2$ and -->
<!--  $ D_1 =\frac{1}{\sigma^2} \sum_{i=1}^n \left[ y_i-\hat{\mu}_i(1)\right]^2.$ -->
<!--  Assuming that model $M_1$ fits the data well, we have $D_1 \sim \chi^2(n-p)$. If also $M_0$ fits the data well, then $D_0 \sim \chi^2(n-q)$ and $D_0-D_1 \sim \chi^2(p-q)$. -->
<!--  Since the deviance for the normal model involves the unknown parameter $\sigma^2$, use the ratio -->
<!--   \begin{align*} -->
<!--    F&=\frac{(D_0-D_1)/(p-q)}{D_1/(n-p)}\\ &= \frac{\left\lbrace \sum_{i=1}^n \left[ y_i-\hat{\mu}_i(0)\right]^2-\sum_{i=1}^n \left[ y_i-\hat{\mu}_i(1)\right]^2 \right\rbrace/(p-q)}{\sum_{i=1}^n \left[ y_i-\hat{\mu}_i(1)\right]^2/(n-p)} -->
<!--   \end{align*} -->
<!-- which should be distributed as $F(p-q,n-p)$ if $H_0$ holds. -->
<!-- \end{example} -->

<!-- \begin{example}[Inference for birthweight v. gestational age data] -->
<!-- Recall Example 1.3 on birthweight as a function of gestational age. -->
<!-- \begin{figure}[hbtp] -->
<!-- \centering -->
<!-- \includegraphics[totalheight=0.8\textwidth,angle=0]{birthweight} -->
<!-- \caption{Birthweight against gestational age for boys (solid circles) and girls (open circles)} \label{fig:birthweight} -->
<!-- \end{figure} -->

<!-- $M_0$ is the model under $H_0$, specified as -->
<!-- $E(Y_{jk}) = \mu_{jk}=\alpha_j + \beta x_{jk} ;  \hspace{1cm} Y_{jk} \sim N(\mu_{jk},\sigma^2)$ -->
<!-- $M_1$ is the model under $H_1$, specified as: -->
<!-- $E(Y_{jk}) = \mu_{jk}=\alpha_j + \beta_j x_{jk}; \hspace{1cm}  Y_{jk} \sim N(\mu_{jk},\sigma^2)$ -->

<!-- where the $Y_{jk}$ are independent for $j=1,2$ and $k=1,\dots, 12$. -->
<!-- \begin{center} -->
<!-- \begin{tabular}{cccc} -->
<!-- \hline -->
<!--  Model  & Min. Sum of Squares & No. parameters & DF\\ -->
<!--  \hline -->
<!--  $M_0$ & $\hat{S}_0=658770.8$ & 3& 24-3=21\\ -->
<!--   $M_1$& $\hat{S}_1=652424.5$ &4 & 24-4=20\\ -->
<!--  \hline -->
<!-- \end{tabular} -->
<!-- \end{center} -->

<!-- The deviances are related to the sums of squares: -->
<!--   $\hat{S}_0= \sigma^2 D_0 \Rightarrow D_0=\hat{S}_0/\sigma^2 \text{ and similarly } D_1=\hat{S}_1/\sigma^2$ -->
<!-- Therefore $ F=\frac{(658770.8-652424.5)/1}{652424.5/20}=0.19 $ which is not significant when compared to the $F(1,20)$ distribution. -->
<!-- \end{example} -->

<!-- # Summary -->
<!-- Key points from this chapter: -->

<!-- *  Approximate (asymptotic/large sample) distribution for the score -->
<!-- *  Approximate (asymptotic/large sample) distribution for the MLE -->
<!-- *  Terminology: full (saturated, maximal) model; null (minimal) model; model of interest (something in-between) -->
<!-- *  maximised log-likelihood for full model, model of interest, null model -->
<!-- *  Definition of deviance, deviance for model of interest, deviance for null model -->
<!-- *  Approximate (asymptotic/large sample) distribution for the deviance of a model of interest under the hypothesis that the model is a good fit -->
<!-- *  What is different about the normal linear model and why don't we use the deviance directly for goodness of fit/hypothesis tests? -->
<!-- *  Hypothesis tests about model parameters: Wald tests and tests based on deviances: what are the relevant null hypotheses, test statistics and distributions involved? Can you perform both types of tests for a given example? -->


## Week 2 learning outcomes

* Know the scope of generalised linear models (GLMs): what do they have in common with the normal linear model and in which ways do they generalise it?

* Be familiar with the properties of the exponential family of distributions and how they relate to GLMs; recognise common distributions that are members of this family.

* Have a basic understanding of how coefficient estimates and standard errors are obtained in a GLM; be familiar with the concepts of likelihood, maximum likelihood estimation, testing, confidence intervals, and goodness of fit statistics in the context of GLMs.

* Be familiar with the main ways of doing inference on the parameters of a GLM -- these are based on large sample distribution results for the maximum likelihood estimator and the deviance. Be able to test the significance of a term in a GLM by performing a Wald test or by comparing deviances between nested models.

