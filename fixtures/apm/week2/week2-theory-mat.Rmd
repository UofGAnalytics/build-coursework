---
output:
  pdf_document: default
  html_document: default
---
$\newcommand{\Var}{\mathrm{Var}}$

# Generalized linear models: what are they, estimation and inference

## Generalized Linear Models 

###[definition] Generalized Linear Model 


Consider the \textbf{linear} model $E(Y_i) = \mu_i = mathbf {x}_i ^\intercal \boldsymbol{\beta}$ with $Y_i \sim N(\mu_i, \sigma^2)$,
where $Y_i$ are independent for $i=1, \dots, n$, and where $mathbf {x}_i ^\intercal$ is the $i$th row of the design matrix $mathbf{X}$. We can generalize this to include: 

 *  a response variable with a distribution other than normal, but a member of the class known as the **exponential family of distributions**; and
 *  a relationship between the response and the linear component of the form $$g(\mu_i) = \mathbf{x}_i ^\intercal \boldsymbol{\beta}$$ where $g$ is the **link function**.
 
###[/definition] 


### The exponential family

Most of the commonly used statistical distributions, e.g. Normal, Binomial and Poisson, are members of the exponential family of distributions, which means they share some interesting properties (discussed later in this section). But first, let us define the general form of a distribution belonging to this family. 


###[definition] Exponential family


Consider a random variable $Y$ whose p.d.f. depends on parameter $\theta$. The distribution belongs to the **exponential family** if it can be written as:

$$ f(y; \theta) = s(y) t(\theta) e^{a(y) b(\theta)}$$ 

This is equivalent to \begin{align} f(y; \theta) = \exp \left[a(y) b(\theta)+c(\theta) + d(y) \right] \end{align} where $s(y) = \exp[d(\theta)]$ and $t(\theta)=\exp[c(\theta)]$. 

The term $b(\theta)$ is called the **natural parameter**. 
If $a(y)=y$ the distribution is said to be in **canonical form**.

###[/definition]

Checking if a distribution belongs to the **exponential family** is as simple as re-arranging the terms to appear as in the definition in (1). We demonstrate this with a couple of simple examples for the Normal and Poisson distributions.

###[example] Normal distribution


Consider $Y \sim N(\theta, \sigma^2)$ with p.d.f. \begin{align} f(y; \theta) = \frac{1}{\sqrt{2 \pi \sigma^2}}\exp \left[ -\frac{1}{2\sigma^2} (y-\theta)^2\right], \hspace{0.5cm} -\infty <y<\infty.\end{align} If we are interested in estimating $\theta$, the variance, $\sigma^2$, can be regarded as a nuisance parameter. By rewriting the p.d.f. as \begin{align} f(y; \theta) = \exp \left[-\dfrac{y^2}{2\sigma^2}+\dfrac{y \theta}{\sigma^2} -\dfrac{\theta^2}{2\sigma^2}-\dfrac{1}{2}\log(2 \pi \sigma^2)\right]\end{align} we can see that this is of exponential family form with $a(y) = y$ ( hence in canonical form) and natural parameter $b(\theta)= \theta/\sigma^2$.

###[/example]


###[example] Poisson distribution


Consider $Y \sim \text{Poisson}(\theta)$ with p.m.f. \begin{align} f(y; \theta) = \frac{\theta^y e^{-\theta}}{y!} \hspace{1cm} \text{for } y=0,1,2,\dots \end{align} This is a member of the exponential family since the probability distribution can be rewritten as \begin{align} f(y; \theta) = \exp \left[ y \log \theta - \theta - \log (y!) \right].\end{align} 
Note this is in canonical form since $a(y) = y$, and has natural parameter $b(\theta)= \log \theta$.

###[/example]

###[task]
Using a similar idea as for the normal and Poisson distributions, prove that the **binomial distribution** is also part of the exponential family and identify its natural parameter.
###[/task]



Table \ref{tab:expfam} shows a summary of the three distributions above.

\begin{table}
\begin{tabular}{lccc}
\hline
 Distribution & Natural parameter & $c(\theta)$ & $d(y)$\\
 \hline
  & & & \\
 Normal & $\dfrac{\theta}{\sigma^2}$ & $-\dfrac{\theta^2}{2\sigma^2}-\dfrac{1}{2}\log(2 \pi \sigma^2)$ & $-\dfrac{y^2}{2\sigma^2}$ \\
 & & & \\
  Poisson & $\log \theta$ & $-\theta$ & $-\log(y!)$ \\
 & & & \\
 Binomial & $\log\left(\dfrac{\theta}{1-\theta} \right)$ & $n \log(1-\theta)$ &
 $\log \binom{n}{y}$\\
  & & & \\
 \hline
\end{tabular}
\caption{Summary of some common distributions as members of the exponential family}
\label{tab:expfam}
\end{table}


Here are some properties that make the exponential family interesting.

* It can be proven that the expectation and variance for members of the exponential family can be obtained using the following formulae:  
\begin{align} E[a(Y)] &= -\frac{c'(\theta)}{b'(\theta)} \\
\mathrm{Var}[a(Y)] &= \frac{b''(\theta)c'(\theta)-c''(\theta)b'(\theta)}{[b'(\theta)]^3} \end{align}

* Some further useful properties of exponential family distributions relate to the **score function**. This is defined as the derivative of the log-likelihood with respect to $\theta$. The log-likelihood for exponential family distributions is:
\begin{align}l(\theta; y) = a(y)b(\theta)+c(\theta) + d(y), \end{align} 
so its derivative with respect to $\theta$ is given by:
\begin{align} U(\theta;y)=\frac{dl(\theta;y)}{d\theta}=a(y)b'(\theta)+c'(\theta)\end{align}
 
 
 

###[definition] Score statistic


\label{def:scoreuniv}$U=\dfrac{dl(\theta;y)}{d\theta}= a(y)b'(\theta)+c'(\theta)$ is called the \textbf{score statistic}, and is used for inference about parameters in generalized linear models.

###[/definition]

Remember that in *Learning from data* and *Predictive modelling* you solved the score statistic equations, $U=\dfrac{dl(\theta;y)}{d\theta}=0$ to obtain maximum likelihood estimates for linear models.

We can think of the score statistic, $U=a(Y) b'(\theta)+c'(\theta)$ as a random variable in its own right, which means we can calculate its expectation:
\begin{align} \mathrm{E}(U)= b'(\theta)E[a(Y)]+c'(\theta)=b'(\theta)\left[-\frac{c'(\theta)}{b'(\theta)} \right]+c'(\theta)=0, \end{align}

and its variance:
\begin{align}\mathrm{Var}(U)=[b'(\theta)^2]\mathrm{Var}[a(Y)]. \label{eqn:varU}\end{align}

This leads us to a very important concept in statistical inference, called **Fisher information**, which we can use to obtain covariance matrices for maximum-likelihood estimates. More specifically, the variance of the maximum likelihood estimates tells us about the amount of *information* that an observed random variable carries about an unknown parameter in the the model, that is linked to a distribution. 


###[definition] Fisher's information 


The **Fisher Information**, denoted as $\mathcal{I}$, is given by:
\begin{align} \mathcal{I}=\mathrm{Var}[U]=E(U^2)=E\left[ \left(\dfrac{dl(\theta;y)}{d\theta}\right)^2\right].
\end{align}
###[/definition] 

<!--  Combining equation (\ref{eqn:varU}) with equation (\ref{eqn:variance}) for the variance $\mathrm{Var}[a(Y)]$ we obtain the following expression for the information of exponential family distributions: \begin{align}\mathcal{I}=\mathrm{Var}[U]=\frac{b''(\theta)c'(\theta)}{b'(\theta)}-c''(\theta). \label{eqn:informationtheta}\end{align} -->

<!--  Another general property of the information is that \begin{align} \mathcal{I}=\mathrm{Var}[U]=E(U^2)=-E(U'). \label{eqn:informationvar}\end{align} -->
<!--   To prove this note that $E(U)=0$ so $\mathrm{Var}[U]=E(U^2)$, and that -->
<!--  \begin{align*} &U'=\frac{dU}{d\theta}= a(Y)b''(\theta)+c''(\theta)\\ -->
<!--   & \Rightarrow -E(U')=-[b''(\theta)E[a(Y)]+c''(\theta)] \\ -->
<!-- &   \Rightarrow -E(U')=-b''(\theta)\left[ -\frac{c'(\theta)}{b'(\theta)}\right]-c''(\theta)=\mathrm{Var}[U]. -->
<!--  \end{align*} -->








###[answers]

## Answers to Tasks


### Answer to Task 1 (Binomial distribution)


Consider $Y \sim Bin(n,\theta)$ with p.m.f. \begin{align} f(y; \theta) = \binom{n}{y} \theta^y (1-\theta)^{n-y}  \text{for } y=0,1,\dots,n. \end{align} By rewriting the p.m.f. as
 \begin{align} f(y; \theta) &= \exp \left[ y \log \theta - y \log (1-\theta) + n \log(1-\theta) +\log \binom{n}{y} \right] \\ &= \exp \left[ y \log \frac {\theta}{1-\theta} + n \log(1-\theta) +\log \binom{n}{y} \right]  \end{align} 
 
 we see that this is a member of the exponential family in canonical form, and its natural parameter is given by:
 $b(\theta)= \log \left(\dfrac{\theta}{1-\theta} \right)$.
 
###[/answers]
