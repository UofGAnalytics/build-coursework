Let us now look at some examples of generalised linear models and their components.

###[example] Normal linear model

Consider $E(Y_i)=\mu_i =  \boldsymbol{x}_i^T \boldsymbol{\beta}$ where the $Y_i$ are independent  $N(\mu_i,\sigma^2)$  for $i=1,\dots,n$. Here $g(\mu_i)=\mu_i$, the \textbf{identity} link. You may be more familiar with this model written as $\boldsymbol{y}=\boldsymbol{X}\boldsymbol{\beta}+\boldsymbol{e}$ where $\boldsymbol{e}=\left[ \begin{array}{c} e_1\\ \vdots \\ e_n  \end{array} \right]$ with the $e_i$ independent, identically distributed $N(0,\sigma^2)$.
###[/example]

###[example] A model for historical linguistics

 If two languages are separated by time $t$, the probability of having cognate words for a particular meaning can be modelled as $e^{-\theta t}$. For a test list of $n$ meanings, a linguist judges whether the corresponding words in two languages are cognate or not. For the $i$th meaning, define $$Y_i = \left \lbrace \right.\begin{array}{ll} 1 & \text{if the two languages have cognate words} \\ 0 & \text{if words are not cognate.} \end{array}$$ Then $P(Y_i=1)=e^{-\theta t} =p \text{ and } P(Y_i=0)=1-p$
i.e. $Y_i \sim \text{Bernoulli}(p)$ (or equivalently Bin$(1,p)$) and $E(Y_i)=p$.The link function is $g(p)= \log p = -\theta t$ so that $g(p)$ is linear in the parameter of interest, $\theta$; $\boldsymbol{x}_i = -t$; and $\boldsymbol{\beta}=\theta$.
###[/example]

###[example] A model for mortality rates

 The number of deaths, $Y$, in a population can be modelled by the Poisson distribution $f(y; \mu)=\frac{\mu^y e^{-\mu}}{y!}$ where $y=0,1,2,\dots$
 The expected number of deaths per year is $E(Y)=\mu$ and it can be modelled by $E(Y)=\mu = n \lambda (\boldsymbol{x}^T \boldsymbol{\beta})$ where $n$ is the population size and $\lambda (\boldsymbol{x}^T \boldsymbol{\beta})$ is the death rate per 100,000 people per year.
 Let $Y_1,\dots, Y_n$ be the numbers of deaths occurring in successive age groups. A possible model is $E(Y_i)=\mu_i = n_i e^{\theta i}$ where $Y_i \sim \text{Poisson}(\mu_i)$ and $i =1$ for the age group 30-34, $i=2$ for age group 35-39, $\dots, i=8$ for age group 65-69. This can be expressed as a generalised linear model as $g(\mu_i) = \log \mu_i = \log n_i +\theta i$ where $\boldsymbol{x}_i^T= (\log n_i, i)$;  and $\boldsymbol{\beta}=(1, \theta)^T$. The term $\log n_i$ is called the *offset*, and we will see more about it when we talk about models for counts.

###[/example]

###[task]

Formulate the model used in the medical school admissions example as a GLM.

####[answer]
Random component: Let $Y_i=1$ if the $i$th applicant is accepted to medical school and $Y_i=0$ if not. We assume that the $Y_i$ are independent responses from $Bin(1,p_i)$, with $E(Y_i)=p_i$ for i=1,\dots,55.

Systematic component: $\beta_0+\beta_1 x_i$ where $x_i$ is the $i$th applicant's GPA and $\beta_0$ and $\beta_1$ are parameters to be estimated.

Link function: $g(p_i)=\log \left(\dfrac{p_i}{1-p_i} \right)$ (logit link)

Equation of the GLM: $$\log \left(\dfrac{p_i}{1-p_i} \right)=\beta_0+\beta_1 x_i.$$
####[/answer]
###[/task]

<!-- # Summary  -->

<!-- Main concepts to be sure to be familiar with from this chapter: -->
<!-- \begin{itemize} -->
<!-- *  What are the two ways in which the GLM generalises the normal linear model? -->
<!-- *  What are the random and systematic component of a GLM and what is the role of the link function? -->
<!-- *  What is the form of the p.d.f. (or p.m.f.) of a distribution that is a member of the exponential family of distributions? What do the terms \textit{natural parameter}, \textit{canonical form} and \textit{canonical link} refer to? -->
<!-- *  Properties of exponential family distributions: can you derive/remember the expressions for the mean and variance of an exponential family distribution in canonical form? -->
<!-- \end{itemize} -->

## Maximum likelihood estimation of GLM coefficients

In a generalised linear model we are interested in the parameters $\beta_1,\dots,\beta_p$ that describe how the response depends on the explanatory variables. We use the observed $y_1,\dots,y_n$ to  maximise the log-likelihood function
\begin{align} l(\boldsymbol{\beta},\boldsymbol{y})=\sum_{i=1}^n y_i b(\theta_i)+ \sum_{i=1}^n c (\theta_i) +\sum_{i=1}^n d(y_i)  \label{eqn:loglik} \end{align}
obtained from equation (\ref{eqn:joint}). This depends on $\boldsymbol{\beta}$ through \begin{align*}
& \mu_i=E(Y_i)=-\frac{c'(\theta_i)}{b'(\theta_i)};    \\
& \mathrm{Var}(Y_i) = [b''(\theta_i)c'(\theta_i)-c''(\theta_i)b'(\theta_i)]/[b'(\theta_i)]^3;\\
& g(\mu_i)=\eta_i= \boldsymbol{x}_i^T \boldsymbol{\beta}, \hspace{1cm} i=1,\dots,n.                                                                           \end{align*}
The maximisation procedure results in $p$ simultaneous equations for $\hat{\boldsymbol{\beta}}$, which are usually solved numerically using the *method of scoring* (also known as *Fisher's scoring method*) and an algorithm called *iteratively reweighted least squares*.

<!-- To illustrate this process we will first present an example with a single parameter, $\theta$, of interest, before proceeding to develop the general theory of estimation of $(\beta_1,\dots,\beta_p)$ for generalised linear models. -->

<!-- \begin{example}[Weibull distribution for failure times] -->

<!-- Suppose we wish to obtain the maximum likelihood estimate of $\theta$ for independent random variables $Y_1, \dots, Y_n$ that follow the Weibull distribution with p.d.f. \begin{align}f(y; \theta)= \exp[\log \lambda + (\lambda-1) \log y - \lambda \log \theta - (y/\theta)^\lambda]\end{align} where $\lambda$ is assumed to be known. This distribution belongs to the exponential family with \begin{align} &a(y)= y^\lambda, \nonumber\\ &b(\theta)=-\theta^{-\lambda}, \nonumber\\ &c(\theta)=\log \lambda - \lambda \log \theta,  \text{ and} \nonumber\\ &d(y)=(\lambda-1) \log(y). \label{eqn:weibullexpfam} \end{align} The joint p.d.f. of the $Y_i$ is \begin{align}f(y_1,\dots, y_n; \theta)= \prod_{i=1}^n \frac{\lambda y_i^{\lambda-1}}{\theta^\lambda} \exp\left[ - \left( \frac{y_i}{\theta} \right)^\lambda \right],\end{align} and the corresponding log-likelihood is $l(\theta; y_1,\dots, y_n)= \sum_{i=1}^n \left[ [(\lambda-1) \log y_i + \log \lambda - \lambda \log \theta]- \left( \frac{y_i}{\theta} \right)^\lambda \right]. $ -->


<!-- The score function is the derivative of the log-likelihood with respect to $\theta$: \begin{align} U(\theta)=\frac{dl}{d\theta}= \sum_{i=1}^n  \left[ -\frac{\lambda}{\theta}+\frac{\lambda y_i^\lambda}{\theta^{\lambda+1}} \right]. \label{eqn:u} \end{align} The maximum likelihood estimator, $\hat{\theta}$, is the solution of the equation $U(\theta)=0 \Rightarrow \sum_{i=1}^n  \left[ -\frac{\lambda}{\theta}+\frac{\lambda y_i^\lambda}{\theta^{\lambda+1}} \right]=0$ Although this equation can be easily solved for $\theta$ (assuming $\lambda$ is known -- you can verify that the solution is $\left(\frac{1}{n} \sum y_i^{\lambda}\right)^{1/\lambda}$, for illustration purposes consider solving it numerically using the Newton-Raphson algorithm. The iterative process uses the formula \begin{align} \theta^{(m)}= \theta^{(m-1)}-\frac{U^{(m-1)}}{U'^{(m-1)}}\label{eqn:newtonraphson} \end{align} starting with an initial guess $\theta^{(1)}$ and obtaining successive approximations until convergence. The derivative, $U'$, with respect to $\theta$ is  \begin{align} U'(\theta)= \frac{dU}{d \theta}= \sum_{i=1}^n \left[ \frac{\lambda}{\theta^2}-\frac{\lambda(\lambda+1)y_i^\lambda}{\theta^{\lambda+2}} \right] = \frac{\lambda n}{\theta^2}-\frac{\lambda(\lambda+1)\sum y_i^\lambda}{\theta^{\lambda+2}} \label{eqn:uprime} .\end{align}  Given a starting value, $\theta^{(1)}$, as well as $n$, $\sum y_i^\lambda$ and the expressions for $U$ and $U'$ in (\ref{eqn:u}) and (\ref{eqn:uprime}), we can use the Newton-Raphson formula (\ref{eqn:newtonraphson}) to obtain the maximum likelihood estimate of $\theta$. An alternative approach, known as the \textbf{method of scoring} or \textbf{Fisher scoring}, is to approximate $U'$ by its expectation $E(U')$ which, for exponential family distributions, is (see equation (\ref{eqn:informationtheta}))$E(U')=-\mathrm{Var}(U)=-\mathcal{I}=b''(\theta)\left[ -\frac{c'(\theta)}{b'(\theta)}\right]+c''(\theta) .$ Using the method of scoring we write the estimating equation as \begin{align} \theta^{(m)}= \theta^{(m-1)}+\frac{U^{(m-1)}}{\mathcal{I}^{(m-1)}}. \label{eqn:scoring} \end{align} In our example the information $\mathcal{I}$ is \begin{align}\mathcal{I}=E(-U')= E\left[- \sum_{i=1}^n U'_i \right]=\sum_{i=1}^n E(-U'_i) =\sum_{i=1}^n \left[ \frac{b''(\theta)c'(\theta)}{b'(\theta)} -c''(\theta)\right]=\dots=\frac{\lambda^2 n}{\theta^2}, \label{eqn:weibull} \end{align} -->
<!--   %\vspace{5cm} -->
<!--   where $U_i$ is the score for $Y_i$ and $b(\theta)$ and $c(\theta)$ are given in (\ref{eqn:weibullexpfam}). -->
<!-- \end{example} -->


###[supplement]

Here we present in some detail how the maximum likelihood estimates of the coefficients of a GLM are obtained. Suppose that we are interested in estimating the parameter vector $(\beta_1,\dots, \beta_p)^T$ in a GLM. To find the maximum likelihood estimates $\hat{\beta}_j$ we need the scores (multivariate version of the score from Definition \ref{def:scoreuniv}) expressed as functions of the $\beta_j$:
  \begin{align} U_j=\frac{\partial l}{\partial \beta_j} &= \sum_{i=1}^n \left [   \frac{\partial l_i}{\partial \beta_j}  \right]= \sum_{i=1}^n \left [   \frac{\partial l_i}{\partial \theta_i} \cdot \frac{\partial \theta_i}{\partial \mu_i} \cdot \frac{\partial \mu_i} {\partial \beta_j}  \right] \label{eqn:chainrule}           \end{align}


For an exponential family distribution in canonical form, the components of (\ref{eqn:chainrule}) are:
  \begin{align} &\frac{\partial l_i}{\partial \theta_i} = y_i b'(\theta) +c'(\theta)=b'(\theta) \left[y_i -\left(-\frac{c'(\theta)}{b'(\theta)}\right) \right]=b' (\theta) (y_i - \mu_i) \label{eqn:dldtheta}\\
& \frac{\partial \mu_i}{\partial \theta_i} = -\frac{c''(\theta_i)}{b'(\theta_i)}+\frac{c'(\theta_i) b''(\theta_i)}{[b'(\theta_i)]^2} = b'(\theta_i) \mathrm{Var}(Y_i)  \nonumber \\ & \Rightarrow  \frac{\partial \theta_i}{\partial \mu_i}= \frac{1}{b'(\theta_i) \mathrm{Var}(Y_i)} \label{eqn:dmudtheta}\\
&\frac{\partial \mu_i}{\partial \beta_j}=\frac{\partial \mu_i}{\partial \eta_i}\cdot \frac{\partial \eta_i}{\partial \beta_j}=\frac{\partial \mu_i}{\partial \eta_i}x_{ij}=\frac{x_{ij}}{g'(\mu_i)} \label{eqn:dmudbeta}
\end{align}
Substituting (\ref{eqn:dldtheta}), (\ref{eqn:dmudtheta}) and (\ref{eqn:dmudbeta}) into (\ref{eqn:chainrule}) the expression for the scores becomes
 \begin{align}
 U_j=\sum_{i=1}^n \left[  \frac{(y_i-\mu_i)}{\mathrm{Var}(Y_i)}x_{ij} \frac{\partial \mu_i}{\partial \eta_i}\right]=\sum_{i=1}^n \left[  \frac{(y_i-\mu_i)}{\mathrm{Var}(Y_i)}\frac{x_{ij}} {g'(\mu_i)}\right]. \label{eqn:score}
 \end{align}
Note that the scores depend on $\boldsymbol{\beta}$ through $\mu_i=E(Y_i)$ and through $\mathrm{Var}(Y_i)$.
The variance-covariance matrix of the $U_j$ has terms $\mathcal{\mathcal{I}}_{jk}=E(U_jU_k)$ and is known as the \textbf{(Fisher) information matrix}. This is the multivariate version of Definition \ref{def:infuniv}. The elements of matrix $\mathcal{\mathcal{I}}$ can be obtained from (\ref{eqn:score}):
\begin{align}
 \mathcal{\mathcal{I}}_{jk}&=  E \left \lbrace \sum_{i=1}^n \left[  \frac{(y_i-\mu_i)}{\mathrm{Var}(Y_i)}x_{ij} \frac{\partial \mu_i}{\partial \eta_i}\right] \sum_{l=1}^n \left[  \frac{(y_l-\mu_l)}{\mathrm{Var}(Y_l)}x_{lk} \frac{\partial \mu_l}{\partial \eta_l}\right]  \right \rbrace \nonumber \\
 &=\sum_{i=1}^n \frac{E[(Y_i-\mu_i)^2]x_{ij}x_{ik}}{[\mathrm{Var}(Y_i)]^2} \left( \frac{\partial \mu_i}{\partial \eta_i} \right)^2  \nonumber\\
 &=\sum_{i=1}^n \frac{x_{ij}x_{ik}}{\mathrm{Var}(Y_i)} \left( \frac{\partial \mu_i}{\partial \eta_i} \right)^2 = \sum_{i=1}^n \frac{x_{ij}x_{ik}}{\mathrm{Var}(Y_i)\left(g'(\mu_i)\right)^2} \label{eqn:informationmatrix}
\end{align}
Here we have used the fact that $E[(Y_i-\mu_i)(Y_l-\mu_l)]=0$ by the independence of the $Y_i$.
Notice that the information matrix can be written as \begin{align}
\mathcal{I}=\mathcal{I}(\boldsymbol{\beta})=\boldsymbol{X}^T \boldsymbol{W} \boldsymbol{X} \label{eqn:informationirwls} \end{align} where $\boldsymbol{X}=\left[ \begin{array}{c} \boldsymbol{x}_1^T\\ \dots \\  \boldsymbol{x}_n^T \end{array} \right]= \left[ \begin{array}{ccc}
x_{11} & \dots & x_{1p}\\
\vdots & \ddots & \vdots\\
x_{n1} & \dots & x_{np}                                                                                                                                                                                                                     \end{array} \right],$
 $\boldsymbol{W}=\text{diag}(\boldsymbol{w})=  \left[ \begin{array}{cccc}
w_1 & 0 &  \dots & 0\\
0 & w_2 & & \vdots\\
\vdots & & \ddots & 0\\
0 & \dots &0 & w_n                                                                                                                                                                                                                     \end{array} \right],$ and \begin{align}w_i=\frac{1}{\mathrm{Var}(Y_i)\left(g'(\mu_i)\right)^2}, \hspace{1cm} i=1,\dots, n.\end{align}
The information matrix $\mathcal{I}(\boldsymbol{\beta})$ depends on $\boldsymbol{\beta}$ through $\boldsymbol{\mu}$ and through $\mathrm{Var}(Y_i)$ for $i=1,\dots,n$.

Equation (\ref{eqn:score}) can be written as \begin{align}
U_j=\sum_{i=1}^n (y_i-\mu_i)x_{ij}w_i g'(\mu_i)=   \sum_{i=1}^n x_{ij}w_iz_i \label{eqn:scoreirwls}    \hspace{1cm} j=1,\dots,p                                   \end{align}

where $z_i=(y_i-\mu_i)g'(\mu_i)$, so the score can be expressed in vector-matrix form as \begin{align}
\boldsymbol{U}(\boldsymbol{\theta})= \boldsymbol{X}^T \boldsymbol{W} \boldsymbol{z} \label{eqn:scoreirwlsmatrix}. \end{align}



Fisher's method of scoring is based on the estimating equation \begin{align}\hat{\boldsymbol{\beta}}^{(m)} = \hat{\boldsymbol{\beta}}^{(m-1)} +[\mathcal{\mathcal{I}}^{(m-1)}]^{-1} \boldsymbol{U}^{(m-1)} \label{eqn:scoringpdim}\end{align} where $\hat{\boldsymbol{\beta}}^{(m)}$ is the vector of estimates of $(\beta_1,\dots \beta_p)$ at the $m$th iteration, $[\mathcal{\mathcal{I}}^{(m-1)}]^{-1}$ is the inverse of the information matrix with elements $\mathcal{\mathcal{I}}_{jk}$ given by (\ref{eqn:informationmatrix}), and $\boldsymbol{U}^{(m-1)}$ is the vector of elements given by (\ref{eqn:score}), all evaluated at  $\hat{\boldsymbol{\beta}}^{(m-1)}$.
  Substituting (\ref{eqn:informationirwls}) and (\ref{eqn:scoreirwlsmatrix}) in (\ref{eqn:scoringpdim}) we obtain \begin{align} \hat{\boldsymbol{\beta}}^{(m)}&=   \hat{\boldsymbol{\beta}}^{(m-1)}+\left[\boldsymbol{X}^T \boldsymbol{W}^{(m-1)} \boldsymbol{X}\right]^{-1}  \boldsymbol{X}^T \boldsymbol{W}^{(m-1)} \boldsymbol{z}^{(m-1)}  \nonumber \\
  &=  \left[\boldsymbol{X}^T \boldsymbol{W}^{(m-1)} \boldsymbol{X}\right]^{-1}  \boldsymbol{X}^T \boldsymbol{W}^{(m-1)}\boldsymbol{X} \hat{\boldsymbol{\beta}}^{(m-1)}+\left[\boldsymbol{X}^T \boldsymbol{W}^{(m-1)} \boldsymbol{X}\right]^{-1}  \boldsymbol{X}^T \boldsymbol{W}^{(m-1)} \boldsymbol{z}^{(m-1)}  \nonumber \\
  &=\left[\boldsymbol{X}^T \boldsymbol{W}^{(m-1)} \boldsymbol{X}\right]^{-1}  \boldsymbol{X}^T \boldsymbol{W}^{(m-1)} (\boldsymbol{\eta}^{(m-1)}+\boldsymbol{z}^{(m-1)}) \label{eqn:irwls} \end{align}
 This is the same form as the normal equations for weighted least squares, except that it has to be solved numerically because $\boldsymbol{\eta}$, $\boldsymbol{z}$ and $\boldsymbol{W}$ depend on $\hat{\boldsymbol{\beta}}$.

This is why the method to obtain maximum likelihood estimators for GLMs is called \textbf{iteratively (re)weighted least squares (IRWLS)}.
The procedure begins by using an initial approximation $\hat{\boldsymbol{\beta}}^{(0)}$ to obtain estimates of $\boldsymbol{\eta}$, $\boldsymbol{z}$ and $\boldsymbol{W}$. Then $\hat{\boldsymbol{\beta}}^{(1)}$ is obtained from (\ref{eqn:irwls}) and is used to update $\boldsymbol{\eta}$, $\boldsymbol{z}$ and $\boldsymbol{W}$. The iterative process continues until the difference between successive approximations $\hat{\boldsymbol{\beta}}^{(m-1)}$ and $\hat{\boldsymbol{\beta}}^{(m)}$ is sufficiently small.
###[/supplement]

<!-- \begin{example}[Poisson regression] -->

<!-- Suppose we have the following data: -->
<!-- \begin{center} -->
<!-- \begin{tabular}{crrrrrrrrrr} -->
<!-- \hline -->
<!-- $y_i$ & 2 & 3 & 6 & 7 & 8 & 9 & 10& 12 & 15\\ -->
<!-- $x_i$ & -1 & -1 & 0 & 0 &0 & 0 & 1 & 1 & 1 \\ -->
<!-- \hline -->
<!-- \end{tabular} -->
<!-- \end{center} -->
<!-- Assume that the responses $Y_i$ are independent Poisson random variables with $E(Y_i)=\mu_i = \beta_1 + \beta_2 x_i= \boldsymbol{x}_i^T \boldsymbol{\beta}=\eta_i$ where $\boldsymbol{\beta}= \left[ \begin{array}{c} \beta_1\\ \beta_2 \end{array}\right] \text{ and } \boldsymbol{x}_i =   \left[ \begin{array}{c} 1\\ x_i \end{array} \right] \text{ for } i=1,\dots, n.$ Note that here we are taking the link function to be the identity link: $g(\mu_i)= \mu_i= \boldsymbol{x}_i^T \boldsymbol{\beta}=\eta_i$ -->
<!-- Then $g'(\mu_i)=\dfrac{\partial \mu_i}{\partial \eta_i}= 1$ so the weights in (\ref{eqn:informationirwls}) are  $w_i = \frac{1}{\mathrm{Var}(Y_i)} =\frac{1}{\mu_i}=\frac{1}{\beta_1+\beta_2 x_i}$ and $\eta_i+z_i$ in  (\ref{eqn:irwls}) simplifies to  $\eta_i+z_i = -->
<!--  \hat{\beta}_1+\hat{\beta}_2x_i+ (y_i-\mu_i)=\hat{\beta}_1 +\hat{\beta}_2 x_i +(y_i - \hat{\beta}_1 -\hat{\beta}_2 x_i) = y_i. $ Also $\mathcal{\mathcal{I}}=\boldsymbol{X}^T \boldsymbol{W} \boldsymbol{X}= \left[ \begin{array}{cc} \sum \dfrac{1}{\hat{\beta}_1+\hat{\beta}_2 x_i }&  \sum \dfrac{x_i}{\hat{\beta}_1+\hat{\beta}_2 x_i} \\ -->
<!--  \sum \dfrac{x_i}{\hat{\beta}_1+\hat{\beta}_2 x_i}  &  \sum \dfrac{x_i^2}{\hat{\beta}_1+\hat{\beta}_2 x_i } \end{array}\right] $ -->
<!-- and $\boldsymbol{X}^T \boldsymbol{W} ( \boldsymbol{\eta}+\boldsymbol{z})= \left[ \begin{array}{c} \sum \dfrac{y_i}{\hat{\beta}_1+\hat{\beta}_2 x_i }\\ -->
<!--  \sum \dfrac{x_i y_i}{\hat{\beta}_1+\hat{\beta}_2 x_i}  \end{array}\right] $ -->
<!-- so that the MLEs can be obtained from  $\hat{\boldsymbol{\beta}}^{(m)}=\left[(\boldsymbol{X}^T \boldsymbol{W} \boldsymbol{X})^{(m-1)}\right]^{-1} (\boldsymbol{X}^T \boldsymbol{W} \boldsymbol{z}) ^{(m-1)}.$ -->
<!-- For the data in this example, $\boldsymbol{\eta}+ \boldsymbol{z}=\boldsymbol{y}=\left[\begin{array}{c} 2\\ 3\\ \vdots\\ 15 \end{array}\right] \text{ and } \boldsymbol{X}=\left[ \begin{array}{c} \boldsymbol{x}_1^T \\ \boldsymbol{x}_2^T \\ \vdots \\ \boldsymbol{x}_9^T  \end{array} \right]=\left[\begin{array}{rr} 1 &  -1\\ 1 &  -1\\ \vdots & \vdots\\ 1 & 1 \end{array}\right]. $ -->
<!-- We also need some initial parameter estimates. Start with $\hat{\beta}_1^{(1)}=7$ and $\hat{\beta}_2^{(1)}=5$. -->
<!-- Then $(\boldsymbol{X}^T \boldsymbol{W} \boldsymbol{X})^{(1)}= \left[ \begin{array}{rr} 1.821429 & -0.75  \\ -->
<!--  -0.75&  1.25 \end{array}\right], $ -->
<!--  $(\boldsymbol{X}^T \boldsymbol{W} \boldsymbol{z})^{(1)}= \left[ \begin{array}{cc} 9.869048  \\ -->
<!-- 0.583333 \end{array}\right], $ -->
<!--  so \begin{align*} \hat{\boldsymbol{\beta}}^{(2)}= [(\boldsymbol{X}^T \boldsymbol{W} \boldsymbol{X})^{(1)}]^{-1}(\boldsymbol{X}^T \boldsymbol{W} \boldsymbol{z})^{(1)}= \left[ \begin{array}{cc} 7.4514  \\ -->
<!-- 4.9375 \end{array}\right] -->
<!-- \end{align*} -->
<!-- and so on until convergence.\\ -->
<!-- The MLEs are $\hat{\beta}_1=7.45163$ and $\hat{\beta}_2=4.93520$. -->
<!-- \end{example} -->

<!-- \begin{remark} Fisher scoring and the Newton-Raphson algorithm are equivalent when the canonical link is used in a generalised linear model. This is because for the canonical link function $\mathcal{I}=E(U')=-U'$. -->
<!-- \end{remark} -->

<!-- \begin{remark} The linear model is a generalised linear model with identity link, $\eta_i=g(\mu_i)=\mu_i$, and $\mathrm{Var}(Y_i)=\sigma^2$ for $i=1,\dots,n$. This results in weights $w_i=1/\sigma^2$ and $z_i=(y_i-\mu_i)$ that do not depend on $\boldsymbol{\beta}$. Hence $\boldsymbol{\eta}+\boldsymbol{z}=\boldsymbol{y}$ and $\boldsymbol{W}=1/\sigma^2 \boldsymbol{I}$, where $\boldsymbol{I}$ is the $n\times n$ identity matrix. Therefore the solution to the likelihood equations can be obtained in one step as we will see in more detail in Chapter 5. -->
<!-- \end{remark} -->

<!-- #Summary -->
<!-- Points to take away from this chapter: -->
<!-- \begin{itemize} -->
<!-- *  Multivariate versions of the score and information (score statistics and information matrix) -->
<!-- *  How to do maximum likelihood estimation -->
<!-- *  Finding the maximum likelihood estimator using numerical methods (Newton-Raphson method, Fisher's scoring method/iteratively reweighted least squares) -->
<!-- *  Think about estimation in the context of a GLM: if you are asked how the estimates for the model parameters were obtained, can you name the estimation method and briefly describe how it works? -->
<!-- \end{itemize} -->

## Inference for GLMs
We will now turn our attention to inference for GLMs, mainly through hypothesis tests and the construction of confidence intervals for the parameters of interest. For that we need some sampling distribution results.
<!-- We can think of this process in the context of two models which have the same probability distribution and the same link function, but different number of parameters in the linear component. The simpler model (model under the null hypothesis $H_0$) must be a special case of the more general model. Then hypothesis testing takes the steps: -->
<!--  \begin{enumerate} -->
<!--  *  Specify model $M_0$ to correspond to $H_0$ and a more general model $M_1$ with $M_0$ a special case of $M_1$. -->
<!--    *  Fit $M_0$ and calculate a ``goodness of fit statistic" $G_0$. Fit $M_1$ and the corresponding statistic $G_1$. -->
<!--    *   Calculate the improvement in fit, usually $G_1-G_0$ (or sometimes $G_1/G_0$). -->
<!--     *  Use the sampling distribution of $G_1-G_0$ to test the null hypothesis that $G_1=G_0$ against $G_1 \neq G_0$. -->
<!--         *  If the hypothesis $G_1=G_0$ is not rejected, then $H_0$ is not rejected and $M_0$ is the preferred model. If $G_1=G_0$ is rejected, then $M_1$ is regarded as the better model. -->
<!--  \end{enumerate} -->
<!-- To obtain confidence intervals and test statistics we need the sampling distribution of the estimator. Exact sampling distributions can be obtained if the response follows the normal distribution. Otherwise, we use asymptotic (large sample) results, usually based on the Central Limit Theorem. For these results to be valid, certain conditions need to hold, but we won't go into them in detail because these are satisfied for the exponential family distributions used in the GLMs we will be covering. -->

<!-- #Large sample approximations  -->

<!-- To come up with the approximate distribution of a statistic of interest, we rely on the following large sample distribution results: If $S$ is a statistic of interest, under appropriate conditions $\dfrac{S-E(S)}{\sqrt{\mathrm{Var}(S)}}  \overset{\text{approx}}{\sim} N(0,1)$, or equivalently  $\dfrac{[S-E(S)]^2}{\mathrm{Var}(S)}  \overset{\text{approx}}{\sim} \chi^2(1)$. -->
<!--  For a vector of statistics of interest $\boldsymbol{s}=(S_1, S_2, \dots, S_p)^T$ with mean $E(\boldsymbol{s})$ and variance-covariance matrix $\boldsymbol{V}$, we have the asymptotic result -->
<!--  \begin{align} [\boldsymbol{s}-E(\boldsymbol{s})]^T \boldsymbol{V}^{-1}  [\boldsymbol{s}-E(\boldsymbol{s})]  \overset{\text{approx}}{\sim} \chi^2(p), \label{eqn:approxnormgeneral}\end{align} provided that $\boldsymbol{V}$ is not singular. -->

<!-- #Sampling distribution for score statistics  -->
<!--  Suppose $Y_1, \dots, Y_n$ are independent random variables in a GLM with parameters $\boldsymbol{\beta}$, with $E(Y_i)=\mu_i$ and $g(\mu_i)=\boldsymbol{x}_i^T \boldsymbol{\beta}= \eta_i$. -->

<!-- Recall equation (\ref{eqn:score}), which gives the score statistics as \begin{align*}U_j= \frac{\partial l}{\partial \beta_j} = \sum_{i=1}^n \left[ \frac{(Y_i-\mu_i)}{\mathrm{Var}(Y_i)}\frac{x_{ij}}{g'(\mu_i)} \right]  \hspace{1cm}  \text{ for } j=1, \dots, p. \end{align*} -->
<!-- Recall also that since $E(Y_i)=\mu_i$, we have that $E(U_j)=0$ for $j=1,\dots, p$ and that the variance-covariance matrix of the score statistics has elements -->
<!-- \begin{align}\mathcal{I}_{jk}= E(U_jU_k).\label{eqn:scorevar}\end{align} -->

<!-- For univariate $\beta$, the score statistic has asymptotic sampling distribution \begin{align}\frac{U}{\sqrt{\mathcal{I}}} \overset{\text{approx}}{\sim}N(0,1)\label{eqn:scoreuniv}\end{align} or equivalently  \begin{align}\frac{U^2}{\mathcal{I}} \overset{\text{approx}}{\sim} \chi^2(1).\end{align} -->
<!-- For $\boldsymbol{\beta}=(\beta_1, \dots, \beta_p)^T$, the score vector \begin{align}\boldsymbol{U}=(U_1,\dots, U_p)^T \overset{\text{approx}}{\sim} MVN(\boldsymbol{0}, \mathcal{I})  \end{align} and so \begin{align} \boldsymbol{U}^T \mathcal{I}^{-1} \boldsymbol{U} \overset{\text{approx}}{\sim} \chi^2(p).\label{eqn:scoremultiv}\end{align} -->

<!-- \begin{example}[Normal distribution] -->
<!-- Let $Y_1, \dots, Y_n$ be independent $N(\mu, \sigma^2)$ random variables where $\sigma^2$ is assumed to be known. The log-likelihood function is -->
<!--  \begin{align}l = -\frac{1}{2\sigma^2}\sum_{i=1}^n(y_i -\mu)^2 -n \log(\sqrt{2\pi \sigma^2}).\label{eqn:infnormal} -->
<!--  \end{align} -->
<!--  The score statistic is \begin{align}U=\frac{dl}{d\mu}= \frac{1}{\sigma^2}\sum (Y_i -\mu) = \frac{n}{\sigma^2} (\bar{Y}-\mu).\label{eqn:scorenormal}\end{align} -->
<!-- It can be seen that the expectation of $U$ is $0$ since $E(Y_i)=\mu$. -->
<!-- The variance of $U$ is -->
<!--  \begin{align} \mathcal{I} = \mathrm{Var}(U)= \frac{1}{\sigma^4}\sum \mathrm{Var}(Y_i)=\frac{n}{\sigma^2}. \label{eqn:varnormal}\end{align} -->
<!--  Therefore \begin{align}\frac{U}{\sqrt{\mathcal{I}}}= \frac{\bar{Y}-\mu}{\sigma / \sqrt{n}} \overset{\text{approx}}{\sim}N(0,1).\label{eqn:scoredistnormaluniv} \end{align} In fact, this is an exact result since $\bar{Y} \sim N(\mu, \sigma^2/n)$. -->
<!-- Also, \begin{align} \boldsymbol{U}^T \mathcal{I}^{-1} \boldsymbol{U}= \frac{U^2}{\mathcal{I}} = \frac{(\bar{Y}-\mu)^2}{\sigma^2/n} \sim \chi^2(1).\label{eqn:scoredistnormalmultiv}\end{align} -->
<!-- This sampling distribution can be used obtain interval estimates for $\mu$ of the form $\bar{y} \pm 1.96 \sigma /\sqrt{n} $ where $\sigma^2$ is assumed to be known. -->
<!-- \end{example} -->

<!-- \begin{example}[Binomial distribution] -->
<!-- For $Y \sim Bin(n,p)$, the log-likelihood function is -->
<!--  \begin{align}l(p; y) = y \log p+(n-y) \log(1-p)+ \log \binom{n}{y}.\label{eqn:binomialloglik}\end{align} -->
<!--  The score statistic is \begin{align}U=\frac{dl}{dp}= \frac{Y}{p}-\frac{n-Y}{1-p} = \frac{Y-np}{p(1-p)},\label{eqn:scorebinomial} -->
<!--  \end{align} -->
<!--    and the expectation of $U$ is $0$ since $E(Y)=np$. -->
<!-- Also, $\mathrm{Var}(Y)=np(1-p)$, so \begin{align}\mathcal{I}=\mathrm{Var}(U) = \frac{1}{p^2(1-p)^2} \mathrm{Var}(Y)=\frac{n}{p(1-p)}.\label{eqn:infbinomial}\end{align} -->
<!-- Therefore a large sample approximation for $U$ is \begin{align}\frac{U}{\sqrt{\mathcal{I}}} = \frac{Y-np}{\sqrt{np(1-p)}} \overset{\text{approx}}{\sim} N(0,1).\label{eqn:scoredistbinom}\end{align} -->
<!-- This is the normal approximation to the binomial distribution (without a continuity correction). It can be used to obtain approximate confidence intervals for $p$. -->
<!-- \end{example} -->

### Sampling distribution of the MLE
<!-- To obtain an asymptotic distribution of the MLE in a generalised linear model, we need to consider some Taylor series expansions of the log-likelihood and its derivatives.\\ -->

<!-- \textbf{Reminder:} The \textbf{Taylor series expansion} of a function $f(x)$ about a value $t$ is given by $f(x)= f(t)+ (x-t) \left. \frac{df}{dx} \right|_{x=t}+ \frac{1}{2} (x-t)^2 \left. \frac{d^2f}{dx^2} \right|_{x=t} + \dots$ provided that $x$ is near $t$.\\ -->


<!-- Let's begin with an approximation for the log-likelihood: -->
<!-- Consider the first three terms of the expansion \begin{align*}l(\beta) & \approx l(\hat{\beta})+ (\beta-\hat{\beta}) \left. \frac{dl}{d\beta} \right|_{\beta=\hat{\beta}}+ \frac{1}{2} (\beta-\hat{\beta})^2 \left. \frac{d^2l}{d\beta^2} \right|_{\beta=\hat{\beta}} \\ & \approx l(\hat{\beta}) + (\beta-\hat{\beta}) U(\hat{\beta}) +  \frac{1}{2} (\beta-\hat{\beta})^2 U'(\hat{\beta})\end{align*} -->

<!-- Replacing $U'=d^2l/ d\beta^2$ by its expected value, $E(U')=-\mathcal{I}$, the approximation becomes  $l(\beta) \approx l(\hat{\beta}) + (\beta-\hat{\beta}) U(\hat{\beta}) -  \frac{1}{2} (\beta-\hat{\beta})^2 \mathcal{I}(\hat{\beta}).$ -->

<!--  For a vector $\boldsymbol{\beta}$ of parameters this generalizes to \begin{align} l(\boldsymbol{\beta}) \approx l(\hat{\boldsymbol{\beta}}) + (\boldsymbol{\beta}-\hat{\boldsymbol{\beta}})^T \boldsymbol{U}(\hat{\boldsymbol{\beta}}) -  \frac{1}{2} (\boldsymbol{\beta}-\hat{\boldsymbol{\beta}})^T \mathcal{I}(\hat{\boldsymbol{\beta}})(\boldsymbol{\beta}-\hat{\boldsymbol{\beta}}). \label{eqn:likelihoodexp} \end{align} -->

<!-- Similarly for the score function with a single parameter $\beta$ the Taylor series expansion is \begin{align*}U(\beta) & \approx U(\hat{\beta})+ (\beta-\hat{\beta})U'(\hat{\beta}).\end{align*} -->
<!-- Approximating $U'$ by its expectation $E(U')=-\mathcal{I}$ this becomes -->
<!-- \begin{align*}U(\beta) & \approx U(\hat{\beta})- (\beta-\hat{\beta})\mathcal{I}(\hat{\beta}).\end{align*} -->
<!-- For a vector $\boldsymbol{\beta}$ of parameters this generalizes to \begin{align} U(\boldsymbol{\beta}) \approx U(\hat{\boldsymbol{\beta}}) -\mathcal{I}(\hat{\boldsymbol{\beta}}) (\boldsymbol{\beta}-\hat{\boldsymbol{\beta}}).\label{eqn:scoreexp}\end{align} -->

<!-- Equation (\ref{eqn:scoreexp}) can be used to obtain the sampling distribution of the MLE $\hat{\boldsymbol{\beta}}$.  Since $U(\hat{\boldsymbol{\beta}})=\boldsymbol{0}$, we have  $ U(\boldsymbol{\beta}) \approx  -\mathcal{I}(\hat{\boldsymbol{\beta}}) (\boldsymbol{\beta}-\hat{\boldsymbol{\beta}}) \Rightarrow (\hat{\boldsymbol{\beta}} - \boldsymbol{\beta}) = \mathcal{I}^{-1} \boldsymbol{U} $ assuming $\mathcal{I}$ is not singular. Taking the expectation (regarding $\mathcal{I}$ as a constant), we have that asymptotically $\hat{\boldsymbol{\beta}}- \boldsymbol{\beta}=\boldsymbol{0}$ since $E(\boldsymbol{U})=0$. Thus $E(\hat{\boldsymbol{\beta}})=\boldsymbol{\beta}$ (asymptotically) and hence the MLE is asymptotically unbiased. -->
<!--  The variance-covariance matrix for $\hat{\boldsymbol{\beta}}$ is $E\left[(\hat{\boldsymbol{\beta}} - \boldsymbol{\beta})(\hat{\boldsymbol{\beta}} - \boldsymbol{\beta})^T  \right]= \mathcal{I}^{-1} E(\boldsymbol{U}\boldsymbol{U}^T) \mathcal{I}^{-1} = \mathcal{I}^{-1}$ since $E(\boldsymbol{U}\boldsymbol{U}^T)=\mathcal{I}$ and $(\mathcal{I}^{-1})^T =\mathcal{I}^{-1}$ since $\mathcal{I}$ is symmetric. -->
<!-- Then by equation (\ref{eqn:approxnormgeneral}), -->

The asymptotic (large sample) distribution for $\hat{\boldsymbol{\beta}}$ is
  \begin{align}
   (\hat{\boldsymbol{\beta}} - \boldsymbol{\beta})^T \mathcal{I}(\hat{\boldsymbol{\beta}}) (\hat{\boldsymbol{\beta}} - \boldsymbol{\beta}) \overset{\text{approx}}{\sim} \chi^2(p) \label{eqn:waldchi}   \end{align} or equivalently \begin{align}\hat{\boldsymbol{\beta}} \overset{\text{approx}}{\sim} MVN(\boldsymbol{\beta}, \mathcal{I}^{-1}).\label{eqn:waldnormal}\end{align}

In the one-dimensional case, what this says is that the MLE $\hat{\beta}$ is approximately normally distributed with mean $\beta$ and variance $\mathcal{I}^{-1})$.

This allows us to perform hypothesis tests and construct confidence intervals for the model parameterss.

###[definition] Wald statistic
The *Wald statistic*, also known as the *z-statistic*, for each of the model parameters $\left \lbrace \beta_j\right \rbrace,~j=1,\dots,p$, is equal to the coefficient estimate, $\hat{\beta}_j$, over its  standard error, $se(\hat{\beta})_j$.
###[/definition]

Under the null hypothesis $H_0: \beta_j=0$, and using the asymptotic normality result for the MLE, we can see that under $H_0$ the Wald statistic is approximately distributed as standard normal. This allows us to perform the *Wald test*, which compares the z-statistic to the upper percentile of a standard normal distribution.

###[example] Hypothesis test for the GPA coefficient in the model for medical school admissions
Recall the logistic regression model for the medical school admissions data. In the output we see the MLEs of $\beta_0$ and $\beta_1$ in the `Estimate` column. These are obtained by solving the likelihood equations numerically using Fisher's scoring method (notice the `Number of Fisher Scoring iterations` information towards the end.)
```{r, echo=c(4:6)}
library(Stat2Data)
data(MedGPA)
med.glm <- glm(Acceptance ~ GPA, data=MedGPA, family=binomial)
summary(med.glm)
```
We can also see the standard errors for $\hat{\beta_0}$ `(Intercept)` and $\hat{\beta_1}$ (GPA coefficient). These are obtained from the observed information matrix, which is computed during the iterative estimation procedure. Remember that the variance of $\boldsymbol{\beta}$ is estimated by $\mathcal{I}$, the inverse of the information matrix. The function 'vcov' returns this estimated variance-covariance matrix of the model coefficients:
```{r}
vcov(med.glm)
```
The diagonal entries of this matrix are the estimated variances for $\hat{\beta_0}$ and $\hat{\beta_1}$ and the off-diagonal entries give the estimated covariance between $\hat{\beta_0}$ and $\hat{\beta_1}$. The standard errors shown in the output (`Std. Error` column) are the square roots of the estimated variances:
```{r}
sqrt(diag(vcov(med.glm)))
```
The `z value` column gives the Wald statistics that test the hypotheses $H_0: \beta_0=0$ and $H_0: \beta_1=0$. Usually we are not as interested in testing whether the intercept term should be zero or not and we focus instead on the coefficients of the explanatory variables in the model. So for $H_0: \beta_1=0$ the $z$ value equals 3.454 and is obtained by taking the coefficient estimate and dividing by its standard error $$z=\frac{\hat{\beta_1}}{se(\hat{\beta}_1)}=\frac{5.454}{1.579}.$$ Under $H_0$, $z$ should approximately follow the standard normal distribution, $N(0,1)$. The $p$-value shown as `Pr(>|z|)` in the output is the probability of obtaining a value as extreme as $z$ or larger in absolute value, assuming the null hypothesis is true. A small $p$-value indicates that the $z$ value is unlikely to come from a $N(0,1)$ distribution and leads to rejecting $H_0$. As the $p$-value for the GPA coefficient is small (0.000553) we can therefore conclude that the GPA term is significant and that it is worth keeping it in the model.
###[/example]
<!-- \begin{example}[MLE for the normal linear model] -->
<!-- Consider the model $E(Y_i)= \mu_i = \boldsymbol{x}_i^T \boldsymbol{\beta}$ where $Y_i \sim N(\mu_i, \sigma^2)$  for $i =1, \dots, n$ and $\boldsymbol{\beta}$ is a $p$-dimensional vector of parameters. This is a GLM with the identity link. Since $\eta_i =g(\mu_i)=\mu_i$ we have $g'(\mu_i)=1$. The elements of the information matrix therefore are $\mathcal{I}_{jk}=\sum_{i=1}^n \frac{x_{ij}x_{ik}}{\mathrm{Var}(Y_i) (g'(\mu_i))^2} =\sum_{i=1}^n \frac{x_{ij}x_{ik}}{\sigma^2}$ Hence \begin{align} -->
<!--               \mathcal{I} =\frac{1}{\sigma^2} \boldsymbol{X}^T \boldsymbol{X} \label{eqn:informationnormal} -->
<!--              \end{align} -->
<!-- Similarly -->
<!--  $\eta_i+z_i = \mu_i+ (y_i-\mu_i) g'(\mu_i)=y_i$ -->
<!--  The estimating equation  $\boldsymbol{X}^T \boldsymbol{W} \boldsymbol{X} \hat{\boldsymbol{\beta}}^{(m)}= \boldsymbol{X}^T \boldsymbol{W} ( \boldsymbol{\eta}+\boldsymbol{z}) $ thus simplifies to  $\frac{1}{\sigma^2}\boldsymbol{X}^T \boldsymbol{X} \hat{\boldsymbol{\beta}}=\frac{1}{\sigma^2} \boldsymbol{X}^T \boldsymbol{y} \Rightarrow \hat{\boldsymbol{\beta}}=(\boldsymbol{X}^T \boldsymbol{X})^{-1} \boldsymbol{X}^T \boldsymbol{y}. $ -->
<!--  If we write the normal linear model as -->
<!--   $\boldsymbol{y} \sim MVN ( \boldsymbol{X}\boldsymbol{\beta}, \sigma^2 \boldsymbol{I})$ where $\boldsymbol{I}$ is the $n\times n$ identity matrix, we have that $E(\hat{\boldsymbol{\beta}}) = E[(\boldsymbol{X}^T \boldsymbol{X})^{-1} \boldsymbol{X}^T \boldsymbol{y}]=  (\boldsymbol{X}^T \boldsymbol{X})^{-1} \boldsymbol{X}^T \boldsymbol{X}\boldsymbol{\beta} = \boldsymbol{\beta}, $ -->
<!--  so $\hat{\boldsymbol{\beta}}$ is an unbiased estimator of $\boldsymbol{\beta}$. -->

<!--  To obtain the variance-covariance matrix for $\hat{\boldsymbol{\beta}}$ we use $$(\hat{\boldsymbol{\beta}}-\boldsymbol{\beta}) &= (\boldsymbol{X}^T \boldsymbol{X})^{-1} \boldsymbol{X}^T \boldsymbol{y}-\boldsymbol{\beta} \\                                                                       &=    (\boldsymbol{X}^T \boldsymbol{X})^{-1} \boldsymbol{X}^T (\boldsymbol{y}-\boldsymbol{X}\boldsymbol{\beta}) $$  -->

<!--  Hence, -->
<!--  \begin{align*} -->
<!--  & E[(\hat{\boldsymbol{\beta}}-\boldsymbol{\beta})  (\hat{\boldsymbol{\beta}}-\boldsymbol{\beta})^T] \\ -->
<!--  & = (\boldsymbol{X}^T \boldsymbol{X})^{-1} \boldsymbol{X}^T E[(\boldsymbol{y}-\boldsymbol{X}\boldsymbol{\beta})(\boldsymbol{y}-\boldsymbol{X}\boldsymbol{\beta})^T]\boldsymbol{X}  (\boldsymbol{X}^T \boldsymbol{X})^{-1}\\ & = (\boldsymbol{X}^T \boldsymbol{X})^{-1} \boldsymbol{X}^T \mathrm{Var}(\boldsymbol{y})  \boldsymbol{X}  (\boldsymbol{X}^T \boldsymbol{X})^{-1}  \\   &= \sigma^2      (\boldsymbol{X}^T \boldsymbol{X})^{-1}\\ -->
<!--  &= \mathcal{I}^{-1} \hspace{5cm} \text{ by (\ref{eqn:informationnormal})} \end{align*} -->
<!-- In this case, the exact sampling distribution of $\hat{\boldsymbol{\beta}}$ is $N(\boldsymbol{\beta}, \mathcal{I}^{-1})$ or equivalently $(\hat{\boldsymbol{\beta}}-\boldsymbol{\beta})^T \mathcal{I}(\hat{\boldsymbol{\beta}}-\boldsymbol{\beta}) \sim \chi^2(p)$. -->
<!-- \end{example} -->

###Deviance

To assess the adequacy of a model of interest, we compare it with the \textbf{saturated} (or \textbf{full}) model, which has the maximum number of parameters that can be estimated. For data with $n$ observations, $y_1,\dots, y_n$, each with a different parameter in $\boldsymbol{X}^T \boldsymbol{\beta}$, the saturated  model can be specified with $n$ parameters. If we have replicates, the maximum number of parameters in the saturated model can be less than $n$. Let $m$ be the maximum number of parameters that can be estimated, and $\boldsymbol{\beta}_{\max}$ and $\hat{\boldsymbol{\beta}}_{\max}$ be the corresponding parameter vector and MLE. Let $L(\hat{\boldsymbol{\beta}}_{\max};\boldsymbol{y})$ be the likelihood evaluated at $\hat{\boldsymbol{\beta}}_{\max}$, that is the likelihood for the full model. Let $L(\hat{\boldsymbol{\beta}};\boldsymbol{y})$ be the maximum value of the likelihood for a model of interest. The \textbf{likelihood ratio} $\lambda=\frac{L(\hat{\boldsymbol{\beta}}_{\max};\boldsymbol{y})}{L(\hat{\boldsymbol{\beta}};\boldsymbol{y})}$ provides a measure of how well the model of interest fits compared with the full model.
In practice, we often use the logarithm of the likelihood ratio:
 $\log \lambda = l(\hat{\boldsymbol{\beta}}_{\max};\boldsymbol{y})-l(\hat{\boldsymbol{\beta}};\boldsymbol{y})$
 Large values of $\log \lambda$ suggest that the model of interest is a poor description of the data relative to the full model. How large a value of $\log \lambda$? To answer this question we need to obtain a critical region using the sampling distribution of $\log \lambda$. In fact, we will work with the quantity $2\log \lambda$, which is called the \textbf{deviance}.
###[definition] Deviance
The deviance, $D$, is defined as $D=2\log \lambda =2[ l(\hat{\boldsymbol{\beta}}_{\max};\boldsymbol{y})-l(\hat{\boldsymbol{\beta}};\boldsymbol{y})]$ where $l(\hat{\boldsymbol{\beta}}_{\max};\boldsymbol{y})$ is the maximised log-likelihood for the saturated model and $l(\hat{\boldsymbol{\beta}};\boldsymbol{y})$ is the maximised log-likelihood for the model of interest.
###[/definition]

<!-- Recall the Taylor series expansion for the log-likelihood (\ref{eqn:likelihoodexp}): $(\boldsymbol{\beta}) \approx l(\hat{\boldsymbol{\beta}}) + (\boldsymbol{\beta}-\hat{\boldsymbol{\beta}})^T U(\hat{\boldsymbol{\beta}}) -  \frac{1}{2} (\boldsymbol{\beta}-\hat{\boldsymbol{\beta}})^T \mathcal{I}(\hat{\boldsymbol{\beta}})(\boldsymbol{\beta}-\hat{\boldsymbol{\beta}}) $ -->
<!-- Since $\hat{\boldsymbol{\beta}}$ is the MLE, $U(\hat{\boldsymbol{\beta}})=\boldsymbol{0}$ and hence this becomes  $l(\boldsymbol{\beta})- l(\hat{\boldsymbol{\beta}}) \approx  -  \frac{1}{2} (\boldsymbol{\beta}-\hat{\boldsymbol{\beta}})^T \mathcal{I}(\hat{\boldsymbol{\beta}}) (\boldsymbol{\beta}-\hat{\boldsymbol{\beta}}).$ -->
<!-- Therefore, the statistic $2[ l(\hat{\boldsymbol{\beta}};\boldsymbol{y})-l(\boldsymbol{\beta};\boldsymbol{y})] \approx  (\boldsymbol{\beta}-\hat{\boldsymbol{\beta}})^T \mathcal{I}(\hat{\boldsymbol{\beta}}) (\boldsymbol{\beta}-\hat{\boldsymbol{\beta}})$ will be approximately $\chi^2(p)$. -->
<!-- For the deviance, write \begin{align*}D&=2[ l(\hat{\boldsymbol{\beta}}_{\max};\boldsymbol{y})-l(\hat{\boldsymbol{\beta}};\boldsymbol{y})]\\ -->
<!--  &=2[ l(\hat{\boldsymbol{\beta}}_{\max};\boldsymbol{y})-l(\boldsymbol{\beta}_{\max};\boldsymbol{y})]\\ -->
<!--  & \hspace{1cm} -2[ l(\hat{\boldsymbol{\beta}};\boldsymbol{y})-l(\boldsymbol{\beta};\boldsymbol{y})]+2 \left[ l(\boldsymbol{\beta}_{\max};\boldsymbol{y})-l(\boldsymbol{\beta};\boldsymbol{y}) \right]    .                    \end{align*} -->
<!--  $2[ l(\hat{\boldsymbol{\beta}}_{\max};\boldsymbol{y})-l(\boldsymbol{\beta}_{\max};\boldsymbol{y})]$ has the $\chi^2(m)$ distribution, where $m$ is the number of parameters in the full (saturated) model. -->
<!--  $2[ l(\hat{\boldsymbol{\beta}};\boldsymbol{y})-l(\boldsymbol{\beta};\boldsymbol{y})]$ is $\chi^2(p)$, where $p$ is the number of parameters in the model of interest. -->
<!--  $v=2 \left[ l(\boldsymbol{\beta}_{\max};\boldsymbol{y})-l(\boldsymbol{\beta};\boldsymbol{y}) \right]$ is a constant which should be close to zero if the model of interest fits the data almost as well as the full model. -->
For a GLM that fits the data well, the approximate distribution of the deviance, $D$, is $\chi^2(m-p)$.

###[example] Deviance for a binomial model
Suppose $Y_1, \dots, Y_n$ are independent with $Y_i \sim Bin(n_i, p_i)$ for $i=1,\dots, n$.  The log-likelihood is  \begin{align}l(\boldsymbol{\beta}; \boldsymbol{y}) = \sum_{i=1}^n \left[y_i \log p_i-y_i \log(1-p_i)+ n_i \log(1-p_i) +\log \binom{n_i}{y_i} \right] \label{eqn:binomloglik} \end{align}
For the full model the $p_i$'s are all different, so $\boldsymbol{\beta}=(p_1, \dots, p_n)^T$. Since $\hat{p}_i=y_i/n_i$, this gives
  \begin{align}l(\hat{\boldsymbol{\beta}}_{\max}; \boldsymbol{y}) &= \sum \left[y_i \log \left(\frac{y_i}{n_i}\right)-y_i \log\left(\frac{n_i-y_i}{n_i}\right)  + n_i \log\left(\frac{n_i-y_i}{n_i}\right) +\log \binom{n_i}{y_i} \right].\label{eqn:binomloglikfullmodel}\end{align}
For any model with $p<n$ parameters, the MLE of $p_i$ is $\hat{p}_i$ and the fitted values are $\hat{y_i}=n_i \hat{p_i}$.  This gives
  \begin{align}l(\hat{\boldsymbol{\beta}}; \boldsymbol{y}) &= \sum_{i=1}^n \left[y_i \log \left(\frac{\hat{y}_i}{n_i}\right)-y_i \log\left(\frac{n_i-\hat{y}_i}{n_i}\right)+n_i \log\left(\frac{n_i-\hat{y}_i}{n_i}\right) +\log \binom{n_i}{y_i} \right].\end{align}
Thus, the deviance is \begin{align}
               D&=2[ l(\hat{\boldsymbol{\beta}}_{\max};\boldsymbol{y})-l(\hat{\boldsymbol{\beta}};\boldsymbol{y})]= 2\sum_{i=1}^n \left[ y_i \log\left( \frac{y_i}{\hat{y}_i}\right)+ (n_i - y_i)  \log\left( \frac{n_i-y_i}{n_i-\hat{y}_i}\right) \right]. \label{eqn:binomdev}
              \end{align}
###[/example]

###[example] Deviance for a normal linear model
Suppose $Y_1, \dots, Y_n$ are independent with $Y_i \sim N(\mu_i, \sigma^2)$ and $E(Y_i)= \mu_i=\boldsymbol{x}^T_i  \boldsymbol{\beta}$ for $i=1,\dots, n$ .
 The log-likelihood function is
 \begin{align}l(\boldsymbol{\beta}; \boldsymbol{y}) = -\frac{1}{2\sigma^2}\sum_{i=1}^n(y_i -\mu_i)^2 -\frac{1}{2}n \log(2\pi \sigma^2).\label{eqn:normalloglik}\end{align}
For the saturated model, we have $n$ parameters $\mu_1, \dots, \mu_n$ and the maximum value of the log-likelihood becomes
 \begin{align}l(\hat{\boldsymbol{\beta}}_{\max}; \boldsymbol{y}) =  -\frac{1}{2}n \log(2\pi \sigma^2). \label{eqn:normloglikfullmodel}\end{align}
 For any other model with $p<n$ parameters, the MLE of $\boldsymbol{\beta}$ is $\hat{\boldsymbol{\beta}} = (\boldsymbol{X}^T \boldsymbol{X})^{-1} \boldsymbol{X}^T \boldsymbol{y}.$
The corresponding maximised log-likelihood function is
 \begin{align}l(\hat{\boldsymbol{\beta}}; \boldsymbol{y}) = -\frac{1}{2\sigma^2}\sum_{i=1}^n(y_i -\boldsymbol{x}_i^T \hat{\boldsymbol{\beta}})^2 -\frac{1}{2}n \log(2\pi \sigma^2).\end{align}
 The deviance then is \begin{align}
               D&=2[ l(\hat{\boldsymbol{\beta}}_{\max};\boldsymbol{y})-l(\hat{\boldsymbol{\beta}};\boldsymbol{y})]= \frac{1}{\sigma^2}\sum_{i=1}^n \left( y_i-\boldsymbol{x}_i^T \hat{\boldsymbol{\beta}}\right)^2
                =\frac{1}{\sigma^2} \sum_{i=1}^n \left( y_i-\hat{\mu}_i\right)^2. \label{eqn:normaldev}
              \end{align}
In the case when $E(Y_i)=\mu$ (null model), we have $\hat{\mu}_i=\hat{\mu}=\bar{y}$ and
\begin{align*}
  D =\frac{1}{\sigma^2} \sum_{i=1}^n \left( y_i-\bar{y}\right)^2.
\end{align*}
Note the relationship to the sample variance $S^2$:
$S^2 = \frac{1}{n-1}\sum_{i=1}^n \left( y_i-\bar{y}_i\right)^2=\frac{\sigma^2 D}{n-1}.$

The distribution of the sample variance is given by $(n-1)S^2/\sigma^2 \sim \chi^2(n-1),$ so $D \sim \chi^2(n-1).$

<!-- More generally, when $\mu_i=\boldsymbol{x}_i^T \hat{\boldsymbol{\beta}}$, we have -->
<!-- \begin{align*} -->
<!--   D =\frac{1}{\sigma^2} \sum_{i=1}^n \left( y_i-\boldsymbol{x}_i^T \hat{\boldsymbol{\beta}}\right)^2=\frac{1}{\sigma^2} (\boldsymbol{y}-\boldsymbol{X} \hat{\boldsymbol{\beta}})^T (\boldsymbol{y}-\boldsymbol{X} \hat{\boldsymbol{\beta}}). -->
<!-- \end{align*} -->
<!-- Since the MLE is $\hat{\boldsymbol{\beta}}= (\boldsymbol{X}^T \boldsymbol{X})^{-1} \boldsymbol{X}^T \boldsymbol{y}$, the term $(\boldsymbol{y}-\boldsymbol{X} \hat{\boldsymbol{\beta}})$ can be written as -->
<!-- \begin{align*}\boldsymbol{y}-\boldsymbol{X} \hat{\boldsymbol{\beta}} &= \boldsymbol{y}-\boldsymbol{X} (\boldsymbol{X}^T \boldsymbol{X})^{-1} \boldsymbol{X}^T \boldsymbol{y}\\ -->
<!--  &= [\boldsymbol{I} - \boldsymbol{X} (\boldsymbol{X}^T \boldsymbol{X})^{-1} \boldsymbol{X}^T]\boldsymbol{y}=[\boldsymbol{I}-\boldsymbol{H}]\boldsymbol{y} -->
<!-- \end{align*} where $\boldsymbol{I}$ is the identity matrix and $\boldsymbol{H}= \boldsymbol{X}(\boldsymbol{X}^T \boldsymbol{X})^{-1} \boldsymbol{X}^T$ is the \textbf{hat} matrix. -->
<!-- Therefore -->
<!-- \begin{align*} -->
<!--   D &=\frac{1}{\sigma^2} (\boldsymbol{y}-\boldsymbol{X} \hat{\boldsymbol{\beta}})^T (\boldsymbol{y}-\boldsymbol{X} \hat{\boldsymbol{\beta}})\\ -->
<!--   &=\frac{1}{\sigma^2}\left([\boldsymbol{I}-\boldsymbol{H}]\boldsymbol{y}\right)^T [\boldsymbol{I}-\boldsymbol{H}]\boldsymbol{y}\\ -->
<!--   &= \frac{1}{\sigma^2} \boldsymbol{y}^T (\boldsymbol{I}-\boldsymbol{H})\boldsymbol{y} -->
<!-- \end{align*} -->
<!-- since $\boldsymbol{H}$ is idempotent; that is $\boldsymbol{H}^T = \boldsymbol{H}$ and $\boldsymbol{H}\boldsymbol{H}=\boldsymbol{H}$. -->
<!-- $D  = \dfrac{1}{\sigma^2} \boldsymbol{y}^T (\boldsymbol{I}-\boldsymbol{H}) \boldsymbol{y}$ is a quadratic form, with rank equal to the rank of the matrix $\boldsymbol{I}-\boldsymbol{H}$. -->
<!-- The rank of $\boldsymbol{I}$ is $n$ and that of $\boldsymbol{H}$ is $p$, so the rank of $\boldsymbol{I}-\boldsymbol{H}$  is $n-p$. (For more details consult your Predictive Modelling notes.) -->

It turns out that the (exact) distribution of $D$ is $\chi^2(n-p)$.
If the model fits the data well, then $D \sim \chi^2(n-p)$, and the expected value of $D$ will be $n-p$, since the expectation of a chi-squared random variable is equal to its degrees of freedom.

However, we are not able to use this chi-squared distribution directly, because the expression for the deviance contains the nuisance parameter $\sigma^2$. Instead, we end up using F tests for the normal linear model, as you may remember from Predictive Modelling.

<!-- From the expression $D =\frac{1}{\sigma^2} \sum_{i=1}^n \left( y_i-\hat{\mu}_i\right)^2,$ we can obtain an estimate of $\sigma^2$: -->
<!--  $\tilde{\sigma}^2=\frac{ \sum_{i=1}^n \left( y_i-\hat{\mu}_i\right)^2}{n-p}.$ -->
###[/example]

###[example] Deviance for a Poisson model
Let $Y_1, \dots, Y_n$ be independent random variables with $Y_i \sim Po(\mu_i)$.
Then the log-likelihood function is \begin{align}l(\boldsymbol{\beta}; \boldsymbol{y}) = \sum y_i \log \mu_i - \sum \mu_i -\sum \log (y_i!).\label{en:poissonloglik}\end{align}
For the full model $\boldsymbol{\beta}_{\max}= (\lambda_1,\dots, \lambda_n)^T$, $\hat{\lambda}_i=y_i$, and the maximum value of the log-likelihood is
  \begin{align}l(\hat{\boldsymbol{\beta}}_{\max}; \boldsymbol{y}) = \sum y_i \log y_i - \sum y_i -\sum \log (y_i!).\label{eqn:poissonloglikfullmodel}\end{align}
Suppose that for the model of interest with $p<n$ parameters the MLE, $\hat{\boldsymbol{\beta}}$, can be used to obtain $\hat{\lambda}_i$ and hence fitted values $\hat{y}_i=\hat{\lambda}_i$ (because $E(Y_i)=\mu_i$).
 For the model of interest the maximum value of the log-likelihood is
  \begin{align}l(\hat{\boldsymbol{\beta}}; \boldsymbol{y}) = \sum y_i \log \hat{y}_i - \sum \hat{y}_i -\sum \log (y_i!).\end{align}
  Hence, the deviance is
  \begin{align}D= 2 [l(\hat{\boldsymbol{\beta}}_{\max}; \boldsymbol{y}) -l(\hat{\boldsymbol{\beta}}; \boldsymbol{y}) ]= 2 \left[\sum y_i \log\left(\frac{y_i} {\hat{y}_i}\right)-\sum (y_i-\hat{y_i}) \right] \label{eqn:devpoisson}\end{align}
 For most models $\sum y_i=\sum \hat{y_i}$, so the deviance can be written as
  \begin{align}D=2 \sum y_i \log\left(\frac{y_i} {\hat{y}_i}\right)= 2 \sum o_i \log\left(\frac{o_i} {e_i}\right),\end{align}
  where $o_i$ denotes the observed value and $e_i$ the expected value of $y_i$.
 The deviance can be computed from the data and compared with the $\chi^2(n-p)$ distribution.
Consider the following data which are assumed to be independent observations from a Poisson distribution.
 \begin{center}
\begin{tabular}{crrrrrrrrrr}
\hline
$y_i$ & 2 & 3 & 6 & 7 & 8 & 9 & 10& 12 & 15\\
$x_i$ & -1 & -1 & 0 & 0 &0 & 0 & 1 & 1 & 1 \\
\hline
\end{tabular}
\end{center}
We fit a model of the form $\mu_i=\beta_1 + \beta_2 x_i$.
  The fitted values are $\hat{y}_i=\hat{\beta}_1 + \hat{\beta}_2 x_i$ where $\hat{\beta}_1 =7.45163$ and $\hat{\beta}_2= 4.93530$.
 The deviance is $1.8947$, which is small compared with $n-p=7$, indicating a good fit.
###[/example]

<!-- ## Hypothesis testing -->

As we've already seen, we can test hypotheses about the $p$-dimensional parameter vector $\boldsymbol{\beta}$ by using the Wald statistic and the asymptotic distribution of the MLE. Alternatively we can compare \textbf{nested} models $M_0$ and $M_1$ using the difference of their deviances.
Consider $H_0: \boldsymbol{\beta}= \boldsymbol{\beta}_0 = (\beta_1, \dots, \beta_q)^T$ corresponding to $M_0$ and $H_1: \boldsymbol{\beta}= \boldsymbol{\beta}_0 = (\beta_1, \dots, \beta_p)^T$ corresponding to $M_1$ with $q<p<n$.
 Test $H_0$ against $H_1$ by considering \begin{align*} D_0-D_1& =2[l(\hat{\boldsymbol{\beta}}_{\max}; \boldsymbol{y}) -l(\hat{\boldsymbol{\beta}}_0; \boldsymbol{y})]-2[l(\hat{\boldsymbol{\beta}}_{\max}; \boldsymbol{y}) -l(\hat{\boldsymbol{\beta}}_1; \boldsymbol{y})] \\ &=2[ l(\hat{\boldsymbol{\beta}}_1; \boldsymbol{y}) -l(\hat{\boldsymbol{\beta}}_0; \boldsymbol{y})]\end{align*}
 If both models describe the data well then $D_0 \sim \chi^2(n-q)$, $D_1 \sim \chi^2(n-p)$ and $D_0-D_1 \sim \chi^2(p-q)$.
 If $M_1$ describes the data well but $M_0$ does not, then $D_0-D_1$ will be larger than expected for a value from $\chi^2(p-q)$.
 So, reject $H_0$ if $D_0-D_1> \chi^2(1-\alpha; p-q)$ that is, if the difference in deviances exceeds the upper $100\times \alpha \%$ point of the $\chi^2(p-q)$ distribution.


###[example] Hypothesis test for the GPA coefficient in the model for medical school admissions, this time using deviances

Suppose that we want to test $H_0: \beta_1=0$ in the medical school admissions example. We can perform this test using the deviances given in the output.
```{r, echo=FALSE}
summary(med.glm)
```
Here $D_0$ is the null deviance, that is the deviance in the model with intercept only and no other predictors. $D_1$ is the residual deviance, that is the deviance of the model of interest (the model with GPA included as a predictor). Under $H_0$ $D_0-D_1$ should be approximately distributed as $\chi^2(1)$. The 95th percentile of the $\chi^2(1)$ distribution is $\chi^2(0.95; 1)=3.84$, and as $D_0-D_1=75.791-56.839=18.952>3.84$ we can reject the null hypothesis. Again, we conclude that GPA is a significant term in the model.

###[/example]

<!-- \begin{example}[Normal linear model] -->
<!-- Consider the model $E(Y_i)=\mu_i= \boldsymbol{x}_i^T \boldsymbol{\beta}, \hspace{1cm} Y_i \sim N(\mu_i, \sigma^2)$ where the $Y_i, i=1,\dots, n$ are independent. -->
<!-- Suppose that model $M_1$ has $p$ parameters and model $M_0$ has $q$ parameters and that the fitted values from each model are denoted by $\hat{\mu}_i(1)$ and  $\hat{\mu}_i(0)$ respectively. -->
<!--  Then -->
<!--  $ D_0 =\frac{1}{\sigma^2} \sum_{i=1}^n \left[ y_i-\hat{\mu}_i(0)\right]^2$ and -->
<!--  $ D_1 =\frac{1}{\sigma^2} \sum_{i=1}^n \left[ y_i-\hat{\mu}_i(1)\right]^2.$ -->
<!--  Assuming that model $M_1$ fits the data well, we have $D_1 \sim \chi^2(n-p)$. If also $M_0$ fits the data well, then $D_0 \sim \chi^2(n-q)$ and $D_0-D_1 \sim \chi^2(p-q)$. -->
<!--  Since the deviance for the normal model involves the unknown parameter $\sigma^2$, use the ratio -->
<!--   \begin{align*} -->
<!--    F&=\frac{(D_0-D_1)/(p-q)}{D_1/(n-p)}\\ &= \frac{\left\lbrace \sum_{i=1}^n \left[ y_i-\hat{\mu}_i(0)\right]^2-\sum_{i=1}^n \left[ y_i-\hat{\mu}_i(1)\right]^2 \right\rbrace/(p-q)}{\sum_{i=1}^n \left[ y_i-\hat{\mu}_i(1)\right]^2/(n-p)} -->
<!--   \end{align*} -->
<!-- which should be distributed as $F(p-q,n-p)$ if $H_0$ holds. -->
<!-- \end{example} -->

<!-- \begin{example}[Inference for birthweight v. gestational age data] -->
<!-- Recall Example 1.3 on birthweight as a function of gestational age. -->
<!-- \begin{figure}[hbtp] -->
<!-- \centering -->
<!-- \includegraphics[totalheight=0.8\textwidth,angle=0]{birthweight} -->
<!-- \caption{Birthweight against gestational age for boys (solid circles) and girls (open circles)} \label{fig:birthweight} -->
<!-- \end{figure} -->

<!-- $M_0$ is the model under $H_0$, specified as -->
<!-- $E(Y_{jk}) = \mu_{jk}=\alpha_j + \beta x_{jk} ;  \hspace{1cm} Y_{jk} \sim N(\mu_{jk},\sigma^2)$ -->
<!-- $M_1$ is the model under $H_1$, specified as: -->
<!-- $E(Y_{jk}) = \mu_{jk}=\alpha_j + \beta_j x_{jk}; \hspace{1cm}  Y_{jk} \sim N(\mu_{jk},\sigma^2)$ -->

<!-- where the $Y_{jk}$ are independent for $j=1,2$ and $k=1,\dots, 12$. -->
<!-- \begin{center} -->
<!-- \begin{tabular}{cccc} -->
<!-- \hline -->
<!--  Model  & Min. Sum of Squares & No. parameters & DF\\ -->
<!--  \hline -->
<!--  $M_0$ & $\hat{S}_0=658770.8$ & 3& 24-3=21\\ -->
<!--   $M_1$& $\hat{S}_1=652424.5$ &4 & 24-4=20\\ -->
<!--  \hline -->
<!-- \end{tabular} -->
<!-- \end{center} -->

<!-- The deviances are related to the sums of squares: -->
<!--   $\hat{S}_0= \sigma^2 D_0 \Rightarrow D_0=\hat{S}_0/\sigma^2 \text{ and similarly } D_1=\hat{S}_1/\sigma^2$ -->
<!-- Therefore $ F=\frac{(658770.8-652424.5)/1}{652424.5/20}=0.19 $ which is not significant when compared to the $F(1,20)$ distribution. -->
<!-- \end{example} -->

<!-- # Summary -->
<!-- Key points from this chapter: -->

<!-- *  Approximate (asymptotic/large sample) distribution for the score -->
<!-- *  Approximate (asymptotic/large sample) distribution for the MLE -->
<!-- *  Terminology: full (saturated, maximal) model; null (minimal) model; model of interest (something in-between) -->
<!-- *  maximised log-likelihood for full model, model of interest, null model -->
<!-- *  Definition of deviance, deviance for model of interest, deviance for null model -->
<!-- *  Approximate (asymptotic/large sample) distribution for the deviance of a model of interest under the hypothesis that the model is a good fit -->
<!-- *  What is different about the normal linear model and why don't we use the deviance directly for goodness of fit/hypothesis tests? -->
<!-- *  Hypothesis tests about model parameters: Wald tests and tests based on deviances: what are the relevant null hypotheses, test statistics and distributions involved? Can you perform both types of tests for a given example? -->


## Week 2 learning outcomes

* Know the scope of generalised linear models (GLMs): what do they have in common with, and how do they generalise, the normal linear model?

* Be familiar with the properties of the exponential family of distributions and how they relate to GLMs; recognise common distributions that are members of this family.

* Have a basic understanding of how coefficient estimates and standard errors are obtained in a GLM; be familiar with the concepts of likelihood, maximum likelihood estimation, testing, confidence intervals, and goodness of fit statistics in the context of GLMs.

* Be familiar with the main ways of doing inference on the parameters of a GLM -- these are mainly based on large sample distribution results for the maximum likelihood estimator and the deviance.

