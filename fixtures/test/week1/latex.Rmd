Consider the bivariate function $f: [-1, 2] \times [-1, 2]\rightarrow\mathbb{R}$
$$
f(\mathbf{x})=f(x_1,x_2)=2 x_1(x_1-1) - 0.25 \cos(9x_1 + 4x_2) + x_2 (x_2-1)
$$

(a) Write a function `f` that takes a vector `x` of length 2 as argument and that returns $f(\mathbf{x})$.
(b) Create a plot of $f(\mathbf{x})$.
(c) Use the BFGS method or Nelder \& Mead's method in `optim` to find a local minimum of $f$.
(d) Using the plot from part (b) try to find the global minimum of $f(\mathbf{x})$.
(e) The derivatives of $f(\mathbf{x})$ are
$$
\begin{array}{rcl}
f'(\mathbf{x})&=&\left[\frac{\partial}{\partial x_1}f(\mathbf{x}),\frac{\partial}{\partial x_2}f(\mathbf{x})\right]
\\&=&
\left[4x_1-2+2.25\sin(9x_1+4x_2),\;\sin(9x_1+4x_2)+2x_2-1\right]\\
f''(\mathbf{x})&=&
\begin{bmatrix}
\frac{\partial^2}{\partial\partial x_1^2} f(x_1,x_2)& \frac{\partial^2}{\partial x_1 \partial x_2} f(x_1,x_2)
\frac{\partial^2}{\partial x_2\partial x_1} f(x_1,x_2)& \frac{\partial^2}{\partial x_2^2} f(x_1,x_2)
\end{bmatrix}
\\&=&\begin{bmatrix}
4+20.25\cos(9x_1+4x_2)&9\cos(9x_1+4x_2)\\
9\cos(9x_1+4x_2)&4\cos(9x_1+4x_2)+2
\end{bmatrix}
\end{array}
$$
Implement functions `f.gradient` and `f.hessian`.

(f) Finally, implement Newton's method. Does it always converge to a local minimum?

In ref://gammamle we estimated the shape parameter $\alpha$ of the Gamma distribution, assuming the rate parameter was known to be 1. This this example we will estimate both the shape parameter $\alpha$ and the rate parameter $\theta$.

$$
\begin{array}{rcl}
f_{\alpha,\theta}(x_i)&=&\frac{\theta^{\alpha}\exp(-\theta x_i)x^{\alpha-1}}{\Gamma(\alpha)}\\
\log f_{\alpha,\theta}(x_i)&=&\alpha\cdot\log(\theta)-\theta x_i+(\alpha-1)\cdot\log(x_i) - \log\Gamma(\alpha)
\end{array}
$$
This gives the following loglikelihood:
$$
\ell(\alpha,\theta)=\sum_{i=1}^n \log f_{\alpha,\theta}(x_i)=n\alpha\cdot\log(\theta)-\theta\sum_{i=1}^n x_i + (\alpha-1) \cdot \left(\sum_{i=1}^n \log x_i\right) - n\cdot \log \Gamma(\alpha)
$$
The first derivative (gradient) is now a vector with two entries, the derivative of the loglikelihood with respect to $\alpha$ and the derivative of the loglikelihood with respect to $\theta$.
$$
\begin{array}{rcl}
\ell'(\alpha,\theta)&=&\left[\frac{\partial}{\partial\alpha}\ell(\alpha,\theta), \frac{\partial}{\partial\theta}\ell(\alpha,\theta)\right]\\
&=&\left[
n\log(\theta)+(\sum_{i=1}^n\log x_i)-n\cdot(\log\Gamma(\alpha))',\;
\frac{n\alpha}{\theta}-\sum_{i=1}^nx_i
\right]
\end{array}
$$
The second derivative (Hessian) is now a $2\times 2$ matrix with two entries.
$$
\begin{array}{rcl}
\ell''(\alpha,\theta)&=&
\begin{bmatrix}
\frac{\partial^2}{\partial \alpha^2} \ell(\alpha,\theta)&
\frac{\partial^2}{\partial \alpha\partial \theta} \ell(\alpha,\theta)\\
\frac{\partial^2}{\partial \theta\partial \alpha} \ell(\alpha,\theta)&
\frac{\partial^2}{\partial \theta^2} \ell(\alpha,\theta)&
\end{bmatrix}
\\
&=&\begin{bmatrix}
-n\cdot(\log\Gamma(\alpha))''&\frac{n}{\theta}\\
\frac{n}{\theta} & -\frac{n\alpha}{\theta^2}
\end{bmatrix}
\end{array}
$$

We can now implement these as functions in R. In order to be able to use the function `newton` we have written in ref://rosenbrock and the function `optim` from R, we combine $\alpha$ and $\theta$ into a vector of length 2, which we will call `par.
