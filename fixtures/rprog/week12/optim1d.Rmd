## Optimisation in one dimension

<!-- ###[video, videoid="anSm8pfuZF4"]Optimisation in 1D -->

In Statistics and Data Science we often come across optimisation problems. This is typically when we want to estimate parameters. In *Learning from Data* you are currently looking at a technique called "maximum likelihood": its idea is that we can estimate parameters by maximising the (logarithm) of the joint probability density function or probability mass function.

In simple cases, we can solve optimisation problems in closed form. If the function is differentiable "all" we have to do is solve the equation that the derivative is zero (assuming the maximum does not occur on a boundary). However, often that equation cannot be easily solved. Thus, in many real life scenarios, we have to resort to numerical methods for maximising an objective function.


### Notation

In this section we will look at optimising functions over one parameter. Optimisation methods typically optimise a function over an argument called $x$. This how we will proceed in the section. 

Maximum likelihood is in that sense, an odd exception. When looking at likelihoods we use the letter $x$ for the data $x_1, \ldots, x_n$ and Greek letters like $\theta$ for the parameters, over which we want to maximise the log-likelihood. We will come back to likelihoods later on in this section.



### Local and global extrema

In optimisation we are looking to find maxima or minima of given objective functions. There are two types of maxima and minima.

- Local maxima (minima) are points at which the function takes larger (smaller) values than in its vicinity.
- Global maxima (minima) are points at which the function takes its largest (smallest) value.

Most techniques for optimisation can only find local maxima or minima. This section contains two examples (not yet using R to find the extrema) illustrating the difference between local extrema and global extrema.

####[example, label="quadopt"]
Consider the function $f(x)=-x^2+2x+2$. 

We can plot this function using 
```{r, out.width="50%"}
f <- function(x)
    -x^2+2*x+2

curve(f, from=-5, to=5)
```

We can see from the plot that is has a global maximum at $x=1$. Let's try to find this maximum without looking at a plot.

We can write the function as 
$$
f(x)=-x^2+2x+2=-\underbrace{x^2+2x-1}_{=(x-1)^2}+3=-(x-1)^2+3
$$
It is easy to see that 
$$
f(1)=3\geq-\underbrace{(x-1)^2}_{\geq 0}+3=f(x) 
$$
We can see that for all $x\neq 1$, $f(x)\leq f(1)$, hence the function has a global maximum at $x=1$.

We could have also set $f'(x)=0$ and solved this equation for $x$, i.e.
$$
f'(x)=  \frac{\partial}{\partial x} f(x) = \frac{\partial}{\partial x} -x^2+2x=-2x+2 = 0
$$
We can solve this equation in closed form to obtain $x=1$. Thus $f(\cdot)$ has an extremum at $x=1$. To determine whether it is a minimum or maximum we have to take the second derivative.
$$
f''(x)=  \frac{\partial^2}{\partial x^2} f(x) =  \frac{\partial}{\partial x} f'(x)=\frac{\partial}{\partial x}-2x+2=-2<0
$$
Because the second derivative is negative the function has a local maximum at $x=1$.

To establish that $x=1$ is indeed also a global maximum we can, for example argue that $x=1$ is the only local maximum with $f'(x)$ being positive to its left ($x<1$) and negative to its right ($x>1$). Hence it has to be a global maximum.
####[/example]

####[example, label="power4opt"]
In this example we will try to find the maximum of the quartic function 
$$
f(x)=-3x^4+28x^3-60x^2.
$$
We will first plot the function in R.
```{r, out.width="50%"}
f <- function(x)
    -3*x^4+28*x^3-60*x^2
curve(f, from=-1, to=6)
```

Its derivative is
$$
f'(x)=\frac{\partial}{\partial x} f(x)=-12x^3+84x^2-120x=
-12x(x^2-7x+10)=-12x(x-2)(x-5)
$$
It thus has three local extrema at $x=0$, $x=2$ and $x=5$. The second derivative of the function is
$$
f''(x)=\frac{\partial^2}{\partial x^2} f(x)=\frac{\partial}{\partial x} f'(x)=
-36x^2+148x-120
$$
As $f''(0)=-120$, $f''(2)=32$ and $f''(5)=-280$, the function has local maxima at $x=0$ and $x=5$ and a local minimum at $x=2$. However, only $x=5$ is a global maximum, i.e. f(5) is larger than the value of the function at any other $x$. $x=0$ is only a local maximum, i.e. f(0) is larger than the value of the function of the function for any $x$ *in a neighbourhood around* $x=0$.
####[/example]

We will see later on that finding local maxima or minima is a lot easier than finding global optima.

In both cases, we were able to resort to the derivatives to find (local) extrema. The two examples below however show that this is not always possible. Introducing constraints on the parameters can make optimisation problems more difficult, as the following example shows.

####[example, label="quadoptconstr"] Constrained optimisation
Consider again the function 
$$f(x)=-2x^2+2x+2$$
from ref://quadopt. This time however we would like to find the $x$ *which lies between -2 and 0* and which maximises $f(\cdot)$. Introducing this constraint makes the problem more difficult. There now can be two types of solutions.

- A maximum at a point in the inside of the interval $(-2, 0)$. Such a point would still have a derivative of zero and we can use the strategy we have used so far.
- The maximum can however also lie on the boundary of the interval, i.e. at $x=-2$ or $x=0$. We can observe a maximum at those two points without the derivative being necessarily zero. 

```{r, echo=FALSE, out.width="50%"}
f <- function(x)
    -x^2+2*x+2
curve(f, from=-5, to=5, lty=3)
curve(f, from=-2, to=0, lty=1, add=TRUE)
abline(v=c(-2,0), lty=2)
```

We already know from ref://quadopt that the only point with a derivative of zero lies outside the interval $(-2,0)$, so the maximum must lie on one of the boundary points. Given that the derivative f'(x) is negative on $(-2,0)$, the function is strictly increasing on this interval and thus the maximum is attained at the right-most point $x=0$.
####[/example]

So far, the functions we have looked at were continuously differentiable, i.e. their derivative was a continuous function. Such functions do not have "kinks". Functions which are not continuously differentiable are harder to optimise. 

####[example, label="quadoptnondiff"] Non-differentiable function
In this example we will consider the function
$$f(x)=-(|x-1|+1)^2+3=\begin{cases}
-(x-2)^2+3 &\mbox{for } x\leq1\\
-x^2+3 &\mbox{for }x\geq 1
\end{cases}
$$
which we would like to maximise over $x$. If we draw a sketch of the function,
```{r, echo=FALSE, out.width="50%"}
f <- function(x)
    -(abs(x-1)+1)^2+3
curve(f, from=-5, to=5)
abline(v=1, lty=2)
```

we can see that it has a maximum at $x=1$. 

However its derivative
$$
f'(x)=\frac{\partial}{\partial x}f(x)=\begin{cases}
-2x+4 &\mbox{for }x<1\\
-2x &\mbox{for }x>1
\end{cases}
$$
does not exist at the maximum $x=1$. However we can see that the derivative is positive for $x<1$ and negative for $x>1$, thus the function is increasing to the left of $x=1$ and decreasing to the right of $x=1$, thus $x=1$ is a local (and global) maximum.
####[/example]

### Closed-form solutions are not always available

In all the examples above the expression for the first derivative $f'(\cdot)$ was so simple that we could manage to find extrema in closed form. In many practical optimisation problems this is not the case. The example below illustrates this

####[example, label="quadsin"]
Suppose we want to find the maximum of the function
$$
f(x)=-\exp(x)-x^2
$$
Though the function looks simple and unimodal it is hard to pinpoint exactly where the maximum is.
```{r, echo=FALSE, out.width="50%"}
f <- function(x)
    -exp(x)-x^2
curve(f, from=-2, to=2)
```

The first derivative of the function is
$$
f'(x)=\frac{\partial}{\partial x}f(x)=-\exp(x)-2x
$$
Solving $f'(x)=0$ requires solving the equation $\exp(x)=-2x$, for which we have to resort to numerical methods. We will see that the maximum is at $x\approx -0.3517$.
####[/example]

We have seen that even for simple functions funding extrema in closed form can be a rather tricky task. It is often easier to perform optimisation numerically. But for most numerical methods for optimisation we can only hope to obtain a local maximum or minimum, unless we know that the objective function is unimodal (or even convex) in which case the local maximum (or minimum) would also be guaranteed to be a global maximum (or minimum). 

We will first introduce Newton's method, a very basic method for maximising / minimising a function. Note that there are two ways of viewing Newton's method. It can be seen as an algorithm for solving a nonlinear equation as well as a method of maximising / minimising a function (i.e. solving that the derivative is zero).

### Newton's method

####[background]The minimum / maximum of a quadratic function
Consider the quadratic function of a scalar $x$ 
$$q(x)=c+b\cdot(x-x_0)+\frac{1}{2}a\cdot(x-x_0)^2.$$ 
To find its only local extremum we need to set is derivative to zero, i.e. solve
$$\frac{\partial}{\partial x}q(x)=b+a\cdot(x-x_0)=0 \qquad \Longleftrightarrow \qquad x=x_0-\frac{b}{a}.$$
$x_0-\frac{b}{a}$ is the global maximum  of $q(x)$ if $a<0$. It is the global minimum of $q(x)$ if $a>0$.
####[/background]

The basic idea of Newton's method (which is sometimes also called the Newton-Raphson algorithm)  is to repeatedly approximate the objective function $f(x)$ by a quadratic function $q(x)$, or to be more precise, by its second order Taylor expansion.

In other words, we will use a quadratic approximation $q(x)$ so that around a given value of $x_0$,
$$f(x)\approx q(x) \qquad \mbox{for $x$ close to $x_0$}.$$

We will choose $q(x)$ such that in $x_0$ the functions $f(x)$ and its quadratic approximation $q(x)$ take the same value and have the same first derivative (tangent) as well as the same second derivative (curvature) in $x_0$, i.e. $f(x_0)=q(x_0)$, $f'(x_0)=q'(x_0)$ and $f''(x_0)$ and $q''(x_0)$.

This means we need to set
$$
q(x) = f(x_0) +  f'(x_0)\cdot  (x-x_0) + \frac{1}{2} \cdot f''(x_0)  \cdot  (x-x_0)^2,$$
where
$$
f'(x_0)= \left.\frac{\partial f}{\partial x}(x)\right|_{x=x_0},  \qquad f''(x_0)= \left.\frac{\partial^2 f}{\partial x^2}(x)\right|_{x=x_0}
$$

####[example, continued="quadsin"]
The figure below shows the function 
$f(x)=-\exp(x)-x^2$
together with its quadratic approximation $q(x)$ for $x_0=1$.

We can see that in $x_0=1$ both functions take the same value and have the same first derivative (tangent) and second derivative (curvature). The further $x$ is from $x_0$ the worse the approximation.

```{r, echo=FALSE, out.width="50%"}
f <- function(x)
    -exp(x)-x^2
curve(f, from=-1, to=2)
g <- function(x, x_0=1)
   -exp(x_0)-x_0^2 + (-exp(x_0)-2*x_0)*(x-x_0) + 0.5*(-exp(x_0)-2)*(x-x_0)^2
curve(g, add=TRUE, col="#951272")
legend("topright", lty=1, col=c("black", "#951272"), c("f(x)", "q(x)"))
abline(v=1, lty=3)
```
####[/example]
We can see in the example, that the maximum of $f(x)$ is somewhat close to the maximum of $q(x)$, but not exactly. However, in contrast to $f(x)$, we can find a closed-form expression for the location of the maximum (or minimum) of $q(x)$. It is attained at 
$$
x_0 -  \frac{f'(x_0)}{f''(x_0)}
$$

The idea behind Newton's method is that, starting with an initial guess of $x_0$, we repeatedly update the quadratic approximation using the location of the maximum (or minimum) of $q(x)$ as the new "anchor" $x_0$. 

Thus Newton's method consists of the following steps. 

1. Initialise $x^{(0)}$ to some value.
2. For $h=1,2,3,\ldots$ iterate until convergence ...
   $$
   x^{(h)}=
   x^{(h-1)} -    \frac{f'(x^{(h-1)})}{f''(x^{(h-1)})}
   $$

$x^{(h)}$ denotes our approximation of the location of the maximum (or minimum) of the function $f(x)$ at the end of the $h$-th iteration.

####[example, continued="quadsin"]
The figure below illustrates the first three steps of Newton's method applied to the function $f(x)=-\exp(x)-x^2$, starting with $x_0=1$.
 
```{r, echo=FALSE, out.width="100%", fig.width=9, fig.height=3}
x <- numeric(3)
x[1] <- 1
f.d <- function(x) 
  -exp(x)-2*x  
f.dd <- function(x)
  -exp(x)-2
cols <- c("#003865", "#00843D", "#00B5D1", "#FFB948", "#B30C00")
par(mfrow=c(1,3))
for (h in seq_along(x)) {
  curve(f, from=-1, to=2)
  x0 <- x[h]
  gg <- function(x) g(x, x0)
  curve(gg, add=TRUE, col="#951272")
  legend("topright", lty=1, col=c("black", "#951272"), c("f(x)", "q(x)"))
  x[h+1] <- x[h] - f.d(x[h])/f.dd(x[h])
  abline(v=x[h], lty=3, col=cols[h])
  abline(v=x[h+1], lty=3, col=cols[h+1])
  axis(3, at=x[h], labels=bquote(x^(.(h-1))), col=cols[h], col.ticks=NULL, col.axis=cols[h], hadj=0)
  axis(3, at=x[h+1], labels=bquote(x^(.(h))), col=cols[h+1],  col.ticks=NULL, col.axis=cols[h+1], hadj=1)
}
```
	
We can see that the quadratic approximations get better very quickly, and after after just 3 iterations we are very close to the maximum.

In Newton's method we approximate the function by a sequence of quadratic functions. This is the same as approximating the function's derivative by a sequence of linear functions. We can illustrate this by plotting the derivatives $f'(x)$ and $q'(x)$, instead of the functions themselves.

```{r, echo=FALSE, out.width="100%", fig.width=9, fig.height=3}
x <- numeric(3)
x[1] <- 1
f.d <- function(x) 
  -exp(x)-2*x
f.dd <- function(x)
  -exp(x)-2
cols <- c("#003865", "#00843D", "#00B5D1", "#FFB948", "#B30C00")
par(mfrow=c(1,3))
for (h in seq_along(x)) {
  curve(f.d, from=-1, to=2)
  x0 <- x[h]
  gg <- function(x) f.dd(x0) * (x-x0) + f.d(x0)
  curve(gg, add=TRUE, col="#951272", ylim=c(-6, 2))
  legend("topright", lty=1, col=c("black", "#951272"), c("f'(x)", "q'(x)"))
  x[h+1] <- x[h] - f.d(x[h])/f.dd(x[h])
  abline(v=x[h], lty=3, col=cols[h])
  abline(v=x[h+1], lty=3, col=cols[h+1])
  abline(h=0, lty=2)
  axis(3, at=x[h], labels=bquote(x^(.(h-1))), col=cols[h], col.ticks=NULL, col.axis=cols[h], hadj=0)
  axis(3, at=x[h+1], labels=bquote(x^(.(h))), col=cols[h+1],  col.ticks=NULL, col.axis=cols[h+1], hadj=1)
}
```
The new value $x^{(h)}$ is the location where the derivative of the current approximation is 0, i.e. $q'(x^{(h)})=0$.
####[/example]

We will now implement Newton's method in R.

####[example, continued="quadsin"]
We will start with defining the functions $f(x)$, $f'(x)$ and $f''(x)$ for our toy example $f(x)=-\exp(x)-x^2$.
```{r}
f <- function(x)
  -exp(x)-x^2
f.d <- function(x)
  -exp(x)-2*x
f.dd <- function(x)
  -exp(x)-2
```
We can now carry out 20 steps of Newton's method using
```{r}
n.iter <- 20                                 # Set number of iterations
x <- numeric(n.iter+1)                       # Create vector to hold results
x[1] <- 1
for (h in seq_along(x))
  x[h+1] <- x[h] - f.d(x[h])/f.dd(x[h])      # Perform Newton update
x                                            # Print history of updates
```
We can see that it wasn't really necessary to carry out 20 updates, so let's stop as soon as we have obtained a given tolerance (say $10^{-8}$).
```{r}
max.iter <- 20                               # Set maximal number of iterations
tol <- 1e-8                                  # Set tolerance
x <- numeric(max.iter+1)                     # Create vector for results
x[1] <- 1
for (h in seq_along(x)) {
  x[h+1] <- x[h] - f.d(x[h])/f.dd(x[h])      # Perform Newton update
  if (abs(x[h+1]-x[h])<tol)                  # Check for early convergence
    break
}
x <- x[1:(h+1)]                              # Get rid of unused slots
x                                            # Print history of updates
```
So, Newton's method has converged within six iterations.

We can now turn the code into a function implementing Newton's method.
```{r}
newton1d <- function(f.d, f.dd, initial, max.iter=100, tol=1e-8) {
  x <- numeric(max.iter+1)                   # Create vector to hold results
  x[1] <- initial
  for (h in seq_along(x)) {
    x[h+1] <- x[h] - f.d(x[h])/f.dd(x[h])    # Perform Newton update
    if (abs(x[h+1]-x[h])<tol)                # Check for early convergence
      return(x[h+1])
  } 
  warning("Newton's method might not have converged")
                                             # If we haven't returned already, 
                                             # it probably hasn't converged
  x[h+1]
}
```
We can now call the function using
```{r}
newton1d(f.d, f.dd, initial=1)
```
####[/example]

Once suitably close to a maximum / minimum, Newton's method will converge quadratically fast, i.e. the number of correct digits will double at each iteration. Newton's method however has some important drawbacks, which do not always make it the first choice for numerical optimisation. 


	 
- First of all, Newton's method requires calculating first and second derivatives (though these can be approximated numerically or "estimates" of them built-up over time as used in the so-called quasi-Newton methods). 
- Newton's method requires the function to be continuously differentiable (at the very least at the optimum) and cannot find optima obtained on boundaries, so it would not be of much use in ref://quadoptconstr  and ref://quadoptnondiff.
- Like all local optimisation methods, all we can hope for is finding a local optimum. Which one we will find will depend on the starting value of Newton's method. We will not necessarily find the local extremum closest to the initial value.
- It is important to note that there is no guarantee that Newton's method will converge to a maximum or minimum, or, actually, converge at all.  It might diverge (i.e. "wander off" to $+\infty$ or $-\infty$). Even if the objective function is concave, Newton's method is \emph{not} guaranteed to converge to a local maximum. There are additional conditions on the derivative of the objective function. Most simple statistical models however have these properties and thus Newton's method will converge when applied to the loglikelihood for these models. If the objective function is however multimodal, it would be altogether unrealistic to expect Newton's method to converge to a global extremum.

We will now look at examples illustrating the latter two issues. 

####[example, continued="power4opt"]
We will now use Newton's method to find the extrema of the function  
$f(x)=-3x^4+28x^3-60x^2$.

We have seen that the function has local maxima at $x=0$ and $x=5$ as well as a local minimum at $x=2$.

We start by defining functions for $f(x)$, $f'(x)$ and $f''(x)$ in R.
```{r, out.width="50%"}
f <- function(x)
    -3*x^4+28*x^3-60*x^2
f.d <- function(x)
    -12*x^3+84*x^2-120*x
f.dd <- function(x)
    -36*x^2+168*x-120
```

We will use the function `newton1d` from the previous example, using different starting values.
```{r}
newton1d(f.d, f.dd, -0.5)
newton1d(f.d, f.dd, 1.5)
newton1d(f.d, f.dd, 4.5)
```
So, if we start Newton's method close enough to a local extremum, it will find that local extremum. However, it is difficult to tell in practice what "close enough" means. For example, if we start Newton's method at $x_0=1$, it does not converge to either of the closer extrema at $x=0$ and $x=2$, but to the one at $x=5$. 
```{r}
newton1d(f.d, f.dd, 1)
```
####[/example]

We will now look an example for which Newton's method does not converge at all.
   
####[example, label="Gaussian density"]
Suppose we wanted to use Newton's method to find the maximum of the density of the standard normal distribution, i.e.
$$
f(x)=\phi(x)=\frac{1}{\sqrt{2\pi}}\exp(-x^2/2)
$$
We have that 
$$
\begin{array}{rcl}
f'(x)&=&\frac{1}{\sqrt{2\pi}}\exp(-x^2/2)\cdot (-x) = -x f(x)\\
f''(x)&=&-f(x)+x^2f(x)
\end{array}
$$
```{r}
f <- function(x)
  dnorm(x)
f.d <- function(x)
  -x*dnorm(x)
f.dd <- function(x)
  -dnorm(x)+x^2*dnorm(x)
```
If we start Newton's method close enough to the maximum of the density at $x=0$, all is well
```{r}
newton1d(f.d, f.dd, 0.5)
```
However, if we start Newton's method a little bit further away, then it diverges
```{r}
newton1d(f.d, f.dd, 1.5)
```
The figure below illustrates the first three iterations of Newtons method when srarted at $x^{(0)}=1.5$. It shows how the approximation moved increasingly further away from the objective function. 	
```{r, echo=FALSE, out.width="100%", fig.width=9, fig.height=3}
x <- numeric(3)
x[1] <- 1.5
cols <- c("#003865", "#00843D", "#00B5D1", "#FFB948", "#B30C00")
par(mfrow=c(1,3))
for (h in seq_along(x)) {
  curve(f, from=-5, to=5)
  x0 <- x[h]
  gg <- function(x) 0.5 * f.dd(x0) * (x-x0)^2 + f.d(x0)*(x-x0)+f(x0)
  curve(gg, add=TRUE, col="#951272", ylim=c(-6, 2))
  legend("topright", lty=1, col=c("black", "#951272"), c("f'(x)", "q'(x)"))
  x[h+1] <- x[h] - f.d(x[h])/f.dd(x[h])
  abline(v=x[h], lty=3, col=cols[h])
  abline(v=x[h+1], lty=3, col=cols[h+1])
  axis(3, at=x[h], labels=bquote(x^(.(h-1))), col=cols[h], col.ticks=NULL, col.axis=cols[h], hadj=1)
  axis(3, at=x[h+1], labels=bquote(x^(.(h))), col=cols[h+1],  col.ticks=NULL, col.axis=cols[h+1], hadj=0)
}
```
####[/example]  

####[task, label="taskoptimcos"]
Consider the following function $f:\;\mathbb{R} \rightarrow \mathbb{R}$ and its derivatives
$$
\begin{array}{rcl}
f(x)&=&\sin(3x)-x^2\\
f'(x)&=&\frac{\partial}{\partial x}f(x)=3\cos(3x)-2x\\
f''(x)&=&\frac{\partial^2}{\partial x^2} f(x)=-9\sin(3x)-2
\end{array}
$$

(a) Write three functions `f`, `f.d`, and `f.dd`, which take `x` as argument and which return $f(x)$, $f'(x)$, and $f''(x)$, respectively.
(b) Use R to create a sketch of the function $f(x)$ for $x\in[-3,3]$.
(c) Use Newton's method to find the global maximum of $f(x)$. You might need to experiment with different starting values.

#####[answer]
(a) We start with defining the functions.
```{r}
f <- function(x) sin(3*x) - x^2
f.d <- function(x) 3*cos(3*x) - 2*x
f.dd <- function(x) -9*sin(3*x) - 2
```

(b) We can either use 
```{r, eval=FALSE}
x <- seq(-3, 3, length.out=100)
plot(x, f(x), type="l")
```
or make use of the function `curve`
```{r, out.width="50%"}
curve(f, from=-3, to=3)
```

(c) We can either implement Newton's method ourselves (this time using `x` and `xold` instead of using a vector to store all the full history of values).
```{r}
x <- 2
for (h in 1:50) {
  x.old <- x
  x <- x - f.d(x) / f.dd(x)
  if (abs(x-x.old)<1e-10)
    break
}
x
```
Alternatively we can use the function `newton1d` from the examples.
```{r}
newton1d(f.d, f.dd, initial=2)
```
We were lucky in terms of our choice of the initial value. Newton's method has converged to the global maximum at $x=0.4273$. However if we start
at $x=-4$ Newton's method converges to the local maximum at $x=-1.2446$.
```{r}
newton1d(f.d, f.dd, initial=-4)
```
If we start at $x=1.5$ Newton's method converges to the local minimum at $x=-0.6806$.
```{r}
newton1d(f.d, f.dd, initial=-1.5)
```
#####[/answer]
####[/task]


### Alternatives to Newton's method
	
Compared to Newton's method, there are many safer (but slower) alternatives for functions of one parameter like [successive parabolic interpolation](https://en.wikipedia.org/wiki/Successive_parabolic_interpolation) or [golden section search](https://en.wikipedia.org/wiki/Golden_section_search).

The basic idea of successive parabolic interpolation is to update triplets of points, so that the objective function is (eventually) largest at the point in the middle. This (eventually) ensures that there is a local maximum somewhere between the point to the left-most and the right-most of the three active points. A new point is added by interpolating the three points with a parabola and computing its maximum.

####[example, continued="quadsin"]
The figure below illustrates the first three steps oft the successive parabolic interpolation method applied to the function $f(x)=-\exp(x)-x^2$, starting with $x_1=-1.75$, $x_2=0$ and $x_3=1.75$.
```{r, echo=FALSE, out.width="100%", fig.width=9, fig.height=3}
cols <- c("#003865", "#00843D", "#00B5D1", "#FFB948", "#B30C00")
par(mfrow=c(1,3))

get_quadratic <- function(x, y) {
     A <- cbind(x^2, x, 1)
     solve(A, y)
}

get_mode <- function(par) {
     -par[2]/(2*par[1])
}
f <- function(x)
     -exp(x)-x^2

numbers <- c(1, 2, 3)
x <- c(-1.75, 0, 1.75)
y <- f(x)

par(mfrow=c(1,3))

for (h in 1:3) {
    curve(f, from=-2, to=2)
    q <- get_quadratic(x, y)
    g <- function(x)
        q[1]*x^2+q[2]*x+q[3]
    curve(g, add=TRUE, col="#951272")
    legend("topright", lty=1, col=c("black", "#951272"), c("f(x)", "q(x)"))
    for (i in 1:3) {
        abline(v=x[i], lty=3, col=cols[numbers[i]])
        axis(3, at=x[i], labels=bquote(x^(.(numbers[i]))), col=cols[numbers[i]], col.ticks=NULL, col.axis=cols[numbers[i]], hadj=0)
    }
    new <- get_mode(q)
    newx <- c(x, new)
    newy <- c(y, f(new))
    newnumbers <- c(numbers, max(numbers)+1)
    newxo <- order(newx)
    newx <- newx[newxo]
    newnumbers <- newnumbers[newxo]
    newy <- newy[newxo]
    keep <- rep(FALSE, 4)
    keep[order(newy)[2:4]] <- TRUE
    x <- newx[keep]
    numbers <- newnumbers[keep]
    y <- newy[keep]
}
```
####[/example]

A combination of these two methods is implemented in R's \texttt{optimize} function. 

The important parts of the syntax of `optimize` are 

```{r eval=FALSE}
optimize(f, interval=c(from, to), ..., maximum=FALSE)
```

- `f ` is the function you want to optimize (over its first argument).
- `from` and `to` are the lower and upper bound of the interval over which `f` is to be optimised.
- `...` allows for passing on further arguments to `f`.
- `maximum` indicates whether R should look for a (local) minimum or a (local) maximum.

####[example, continued="quadsin"]
We can use `optimize` to find the maximum of the function $f(x)=-\exp(x)-x^2$. We know the maximum lies between -2 and 2.
```{r}
f <- function(x)
  -exp(x)-x^2
optimize(f, c(-2, 2), maximum=TRUE)
```
####[/example]


####[example, continued="quadoptnondiff"] 
`optimize` also works if the objective function is not differentiable as long as it is continuous.

We will again look at the function
$$f(x)=-(|x-1|+1)^2+3=\begin{cases}
-(x-2)^2+3 &\mbox{for } x\leq1\\
-x^2+3 &\mbox{for }x\geq 1
\end{cases}
$$
which is not differentiable at its maximum at $x=1$. Newton's method would fail to find this maximum, but `optimize` succeeds.
```{r}
f <- function(x)
    -(abs(x-1)+1)^2+3
optimize(f, c(-2, 1), maximum=TRUE)
```
To obtain a more precise answer, we need to decrease the tolerance.
```{r}
optimize(f, c(-2, 1), maximum=TRUE, tol=1e-10)
```
####[/example]

<!--[if PDF]>
\newpage
<![endif]-->


####[task]
Consider again the function $f:\;\mathbb{R} \rightarrow \mathbb{R}$ from ref://taskoptimcos,
$$
f(x)=\sin(3x)-x^2.
$$
Use `optimize` to find the global maximum of the function.
#####[answer]
Though not guaranteed, compared to Newton's method, it is often more likely that `optimize` will find the global optimum, as is the case in this example.
```{r}
optimize(f, lower=-3, upper=3, maximum=TRUE)
```
#####[/answer]
####[/task]

####[task]
Use the function `rexp` to create a random vector `x` of size 10,000 from the $\textsf{Expo}(1)$ distribution.

(a) Compute the mean of `x`.
(b) Use the function `optimize` to find the value $m$ minimising the objective function
$$
\sum_{i=1}^n (x_i-m)^2.
$$
Compare your result to the result from part (a).
(c) Compute the median of `x`.
(d) Use the function `optimize` to find the value $m$ minimising the objective function
$$
\sum_{i=1}^n |x_i-m|.
$$
Compare your result to the result from part (c).

#####[answer]

We start with simulating the data.
```{r}
x <- rexp(1e4)
```

(a) The mean of `x` is
```{r}
mean(x)
```
(b) We also obtain the mean when minimising
```{r}
f <- function(m, x) {
  sum((m-x)^2)
}
```
yielding
```{r}
optimize(f, range(x), x=x)   # gives same result as mean(x)
````
Note that `f` is a function of both `m` and the data `x`, which we want to optimise over `m` (rather than `x`). 

(c) The median of `x` is
```{r}
median(x)
```
(b) We also obtain the median when minimising
```{r}
f <- function(m, x) {
  sum(abs(m-x))
}
```
yielding
```{r}
optimize(f, range(x), x=x)   # gives same result as median(x)
````
We could have not used Newton's method in this case as the function is not continuously differentiable.

**Background: Why the mean minimises $\sum_{i=1}^n (x_i-m)^2$**

$$
\begin{array}{rcl}
\frac{\partial}{\partial m}\sum_{i=1}^n (x_i-m)^2&=&\sum_{i=1}^n 2(x_i-m)\\
&=&2\left(\left(\sum_{i=1}^n x_i\right)-n\cdot m\right)\\
&=&2n\left(\left(\frac1n\sum_{i=1}^n x_i\right)-m\right)\\
&=&2n(\bar x-m)
\end{array}
$$
Setting this derivative to zero yields $m=\bar x=\frac1n\sum_{i=1}^n x_i.$

**Background: Why the median minimises $\sum_{i=1}^n |x_i-m|^2$**
Assume $m\neq x_i$ for all $i\in\{1,\ldots, n\}$. Then $\frac{\partial}{\partial m}|x_i-m|$ is $+1$ if $x_i<m$ and $-1$ if $x_i>m$. Hence
$$
\begin{array}{rcl}
\frac{\partial}{\partial m}\sum_{i=1}^n |x_i-m|&=&
\textrm{(\#obs with $x_i<m$)} - \textrm{(\#obs with $x_i>m$)}
\end{array}
$$

If $m$ is less than the median of the $x_i$, then more $x_i$ are greater than $m$ compared to how many $x_i$ are less than $m$, i.e. $\frac{\partial}{\partial m}\sum_{i=1}^n |x_i-m|$ is positive, i.e the objective function in increasing.

If $m$ is greater than the median of the $x_i$, then more $x_i$ are less than $m$ compared to how many $x_i$ are greater than $m$, i.e. $\frac{\partial}{\partial m}\sum_{i=1}^n |x_i-m|$ is negative, i.e. the objective function is decreasing.

Hence the objective function is largest if $m$ is the median of the $x_i$.
#####[/answer]

####[/task]

### Likelihood estimation

In this section we will use numerical optimisation to find maximum-likelihood estimates in one-parameter models. In terms of the notation, what might be slightly confusing is that the (log)likelihood makes use of both parameters denoted by $\theta$ (or another Greek letter) and data denoted by $x$, but we want to perform the optimisation over $\theta$, rather than over $x$.

####[example, label="gammamle"] Gamma distribution
Consider a sample $x_1$,\ldots, $x_n$ from the gamma distribution with known rate parameter (set to 1) and unknown shape parameter $\alpha$. We will use the Greek letter $\alpha$ in this example, as the Greek letter $\theta$ is typically used for the rate parameter of the Gamma distribution, which we assume is known to be one. The probability density function is thus for $x_i>0$ and $\alpha>0$
$$
\begin{array}{rcl}
f_{\alpha}(x_i)&=&\frac{\exp(-x_i)x^{\alpha-1}}{\Gamma(\alpha)}\\
\log f_{\alpha}(x_i)&=&-x_i+(\alpha-1)\cdot \log(x_i) - \log\Gamma(\alpha)
\end{array}
$$
This gives the following loglikelihood:
$$
\begin{array}{rcl}
\ell(\alpha)&=&\sum_{i=1}^n \log f_{\alpha}(x_i)=-\sum_{i=1}^n x_i + (\alpha-1) \cdot \left(\sum_{i=1}^n \log x_i\right) - n\cdot \log \Gamma(\alpha)\\
\ell'(\alpha)&=&\left(\sum_{i=1}^n \log x_i\right) - n\cdot (\log \Gamma(\alpha))'\\
\ell''(\alpha)&=& - n\cdot (\log \Gamma(\alpha))''
\end{array}
$$
R has built-in functions to calculate $\log \Gamma(\alpha)$ (`lgamma`), $(\log \Gamma(\alpha))'$ (`digamma`) and $(\log \Gamma(\alpha))''$ (`trigamma`).

We can write R functions to compute the objective function and its derivatives:
```{r}
loglik <- function(alpha, x) {
  -sum(x) + (alpha-1)*sum(log(x)) - length(x) * lgamma(alpha)
}

loglik.d <- function(alpha, x) {
  sum(log(x)) - length(x) * digamma(alpha)
}

loglik.dd <- function(alpha, x) {
  -length(x) * trigamma(alpha)
}
```
We also need some data `x`, which we will draw from an exponential distribution, which is a special case of the gamma distribution with shape parameter equal to one.
```{r, echo=FALSE}
set.seed(123)
``` 
```{r}
x <- rexp(1e3) 
```

To get an idea of how the objective function looks like, we can plot it.
```{r, out.width="50%"}
alpha.range <- seq(0.1, 5, length.out=1000)
plot(alpha.range, loglik(alpha.range, x), type="l", 
     xlab=expression(alpha), ylab=expression(loglik(alpha)))
```

We can implement Newton's method, keeping in mind that we want to maximise the loglikelihood over $\alpha$ (and not $x$).
```{r}
max.iter <- 100  
alphas <- numeric(max.iter+1)
alphas[1] <- 1.5
for (h in seq_along(alphas)) {
    alphas[h+1] <- alphas[h] - loglik.d(alphas[h], x)/loglik.dd(alphas[h], x)
                                             # Perform Newton update
    if (abs(alphas[h+1]-alphas[h])<1e-8)     # Check for early convergence
      break
}
alphas[h]
```
	
In order to be able to use the function `newton1d` in this context, we need to redefine it, so that we can pass on additional arguments (the data) to the the functions calculating the derivatives. We will also rename the parameter over which we optimise from `x` to `par` to avoid creating additional confusion as to the role of `x`.
```{r}
newton1d <- function(f.d, f.dd, initial, ..., max.iter=100, tol=1e-8) {
  par <- numeric(max.iter+1)                 # Create vector to hold results
  par[1] <- initial
  for (h in seq_along(par)) {
    par[h+1] <- par[h] - f.d(par[h],...)/f.dd(par[h],...)
                                             # Perform Newton update
    if (abs(par[h+1]-par[h])<tol)            # Check for early convergence
      return(par[h+1])
  } 
  warning("Newton's method might not have converged")
                                             # If we haven't returned already, 
                                             # it probably hasn't converged
  par[h+1]
}
```
We can now find the maximum-likelihood estimate using the function.
```{r}
newton1d(loglik.d, loglik.dd, initial=1.5, x=x)
```
Alternatively, we can use `optimize`. In this case we do not need any derivatives. The plot suggests that the maximum-likelihood estimate lies between 0.5 and 2.
```{r}
optimize(loglik, c(0.5, 2), x=x, maximum=TRUE)
```
Because of `optimize`'s large default tolerance, the answers are not exactly the same. However the answers will become the same  after decreasing the tolerance.
```{r}
optimize(loglik, c(0.5, 2), x=x, maximum=TRUE, tol=1e-10)
```
####[/example]

####[task]
Consider a sample $x_1,\dots,x_n$ from the Pareto distribution, which you can generate in R using
```r
n <- 50
theta <- 3
x <- 1/runif(n)^(1/theta)
```	 
The probability density function of the Pareto distribution is for $\theta>0$
$$
f_{\theta}(x) = \begin{cases} \frac{\theta}{x^{\theta+1}}& \mbox{for }x>1\\
	      0&\mbox{otherwise.}
	      \end{cases}
$$

(a) Compute the loglikelihood and implement it as a function taking arguments `theta` and `x`.
(b) Create a plot of the loglikelihood function of the sample `x`.
(c) Use `optimize` to find the maximum-likelihood estimate
(d) Compute the first two derivatives of the loglikelihood and use Newton's method to find the maximum-likelihood estimate.
(e) Can you find a closed-form expression for the maximum-likelihood estimate?
    
#####[answer]
We start with simulating the data using the code provided.
```{r}
n <- 50
theta <- 3
x <- 1/runif(n)^(1/theta)
```	 

(a) The logarithm of the probability density function is
    $$
      \log f_{\theta}(x_i)=\log(\theta) - (\theta+1)\log(x_i)
    $$
    The loglikelihood is thus
    $$
    \ell(\theta) = n\log(\theta) - (\theta+1)\sum_{i=1}^n \log(x_i)	
    $$
    (We could have omitted the $+1$ in $(\theta+1)$ as this is just a constant not depending on $\theta$).

```{r}
loglik <- function(theta, x) {
    n <- length(x)
    n*log(theta) - (theta+1) * sum(log(x))
}
```

(b) We will plot the loglikelihood for the interval $\theta\in(0.1, 5)$
```{r, out.width="50%"}
theta <- seq(0.1, 5, length.out=1000)
plot(theta, loglik(theta, x), type="l",
     xlab=expression(theta), ylab=expression(loglik(theta)))
```

(c) Using `optimize` we obtain
```{r}
optimize(loglik, c(0.1, 5), x=x, maximum=TRUE)
```

(d) The derivatives are
    $$
    \begin{array}{rcl}
    \ell'(\theta)&=& \frac{\partial}{\partial \theta}  n\log(\theta) - (\theta+1)\sum_{i=1}^n \log(x_i)\\&=&\frac{n}{\theta}-\sum_{i=1}^n\log(x_i)\\
    \ell''(\theta)&=& \frac{\partial^2}{\partial \theta^2}  n\log(\theta) - (\theta+1)\sum_{i=1}^n \log(x_i)\\
    &=&\frac{\partial}{\partial \theta}\frac{n}{\theta}-\sum_{i=1}^n\log(x_i)\\
    &=&-\frac{n}{\theta^2}
    \end{array}{rcl}
    $$
```{r}
loglik.d <- function(theta, x) {
    n <- length(x)
    n / theta - sum(log(x))
}
loglik.dd <- function(theta, x) {
    n <- length(x)
    -n / theta^2
}
```
We can use the `newton1d` function from the example.
```{r}
newton1d(loglik.d, loglik.dd, initial=1, x=x)  
```
     
(e) We can actually solve the equation $\ell'(\theta)=0$ in closed form.
    $$
    \ell'(\theta) = \frac{n}{\theta}-\sum_{i=1}^n\log(x_i) =0, 
    $$
    which is the same as
    $$
    \frac{n}{\theta}=\sum_{i=1}^n\log(x_i),
    $$
    which we can re-arrange to become
    $$
    \theta=\frac{n}{\sum_{i=1}^n\log(x_i)}.
    $$
```{r}
mle <- length(x) / sum(log(x))
mle
```
#####[/answer]
	
####[/task]




\newpage




