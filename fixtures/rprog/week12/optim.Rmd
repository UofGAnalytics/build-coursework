# Optimisation in more than one dimension  


In this section we look at maximising (or minimising) a function of more than one input. We will group these inputs into a vector $\boldsymbol x$.


###[background] The minimum / maximum of a quadratic function
The extremum of a quadratic function of a vector ${\mathbf{x}}$ 
$$Q({\mathbf{x}})=\frac{1}{2}({\mathbf{x}}-{\mathbf{x}}_0)'\mathbf{A}({\mathbf{x}}-{\mathbf{x}}_0) + \mathbf{b}'({\mathbf{x}}-{\mathbf{x}}_0)+c$$ 
($\mathbf{A}=\mathbf{A}'$ symmetric) can be found using the same approach as for a function of a scalar:
$$\frac{\partial}{\partial {\mathbf{x}}}Q({\mathbf{x}})=\mathbf{A}({\mathbf{x}}-{\mathbf{x}}_0)+\mathbf{b}={\boldsymbol 0} \qquad \Longleftrightarrow \qquad {\mathbf{x}}={\mathbf{x}}_0-\mathbf{A}^{-1}\mathbf{b}.$$
If $\mathbf{A}$ is negative definite ${\mathbf{x}}_0-\mathbf{A}^{-1}\mathbf{b}$ is the global maximum of $Q(x)$, if $\mathbf{A}$ positive definite it is the global minimum of $Q(x)$. The case of $\mathbf{A}$ being indefinite corresponds to a saddle point.
###[/background]

### Newton's method in more than one dimension
We have already looked at Newton's method for functions of one variable. Newton's method can however be used as well to maximise a function of than one variable.

Again we start by looking at a quadratic approximation of $f(\mathbf{x})$ around ${\mathbf{x}}$.

$$f({\mathbf{x}})\approx f({\mathbf{x}}_0) +  f'({\mathbf{x}}_0)'({\mathbf{x}}-{\mathbf{x}}_0) + \frac{1}{2} ({\mathbf{x}}-{\mathbf{x}}_0)' f''({\mathbf{x}}_0) ({\mathbf{x}}-{\mathbf{x}}_0),$$
where
$$
f'({\mathbf{x}}_0)= \left.\frac{\partial f}{\partial {\mathbf{x}}}({\mathbf{x}})\right|_{{\mathbf{x}}={\mathbf{x}}_0},  \qquad f''({\mathbf{x}}_0)= \left.\frac{\partial^2 f}{\partial {\mathbf{x}}\partial{\mathbf{x}}'}({\mathbf{x}})\right|_{{\mathbf{x}}={\mathbf{x}}_0}
$$
Note that the first derivative $f'({\mathbf{x}}_0)$ is a vector and that the second derivative $f''({\mathbf{x}}_0)$ is a matrix.

The local extremum of the quadratic approximation is, using the formula derived above, at
$$
{\mathbf{x}}_0 - \left(f''({\mathbf{x}}_0)\right)^{-1}f'({\mathbf{x}}_0)
$$

![Objective function and quadratic approximation (with maximum)](newtonidea.pdf)

Repeatedly using this approximation again yields Newton's algorithm.

1. Initialise ${\mathbf{x}}^{(0)}$ to some value.
2. For $h=1,2,3,\ldots$ iterate until convergence ...
   $$
   {\mathbf{x}}^{(h)}=
   {\mathbf{x}}^{(h-1)} - \left(f''({\mathbf{x}}^{(h-1)})\right)^{-1}  f'({\mathbf{x}}^{(h-1)}).
   $$

Note that rather than computing the inverse $\left(f''({\mathbf{x}}^{(h-1)})\right)^{-1}$, it is more efficient to solve the system of equations $f''({\mathbf{x}}^{(h-1)})\mathbf{z}=f'({\mathbf{x}}^{(h-1)})$ for $\mathbf{z}$ and set ${\mathbf{x}}^{(h)}={\mathbf{x}}^{(h-1)}-\mathbf{z}$.

Remember that there is in general no guarantee that Newton's method will converge. Only if the objective function is concave (convex in the case of minimisation) and if additional properties relating to its derivative hold, Newton's method is guaranteed to converge.

####[example, label="rosenbrock"]Rosenbrock function
The Rosenbrock function is a quartic function in two dimensions given by
$$f(\mathbf{x})=f(x_1,x_2)= 100(x_2 - x_1^2)^2 + (1-x_1)^2 $$

 The Rosenbrock function is a test function frequently used in Numerical Optimisation although one can determine its minimum analytically.

It is easy to see that $f$ is the sum of two quadratic functions, so $f(\mathbf{x})\geq 0$. The second quadratic is $0$ if and only if $x_1=1$. If $x_1=1$ the first term is only $0$ if $x_2=1$ as well, thus $f$ has a global minimum at $(1, 1)$. 

The first derivative ("gradient") of the Rosenbrock function is
 $$
 \mathbf{f}'(\mathbf{x})=\left[\frac{\partial f}{\partial x_1},\frac{\partial f}{\partial x_2}\right](\mathbf{x})=\left[-400x_1(x_2-x_1^2) - 2(1-x_1), 200(x_2-x_1^2)\right]
$$
(Remember the derivative of a scalar-valued function that takes a vector is input is a vector of the same length.)

The second derivative ("Hessian") of the Rosenbrock function is
 $$
 \begin{array}{rcl}
 \mathbf{f}''(\mathbf{x})&=&\frac{\partial^2}{\partial\mathbf{x}\partial\mathbf{x}'}f(\mathbf(x))\\
 &=&\begin{bmatrix}
\frac{\partial^2}{\partial x_1^2} f(x_1,x_2) & \frac{\partial^2}{\partial x_1\partial x_2} f(x_1,x_2)\\
\frac{\partial^2}{\partial x_2\partial x_1} f(x_1,x_2)& \frac{\partial^2}{\partial x_2^2} f(x_1,x_2)
\end{bmatrix}
=\begin{bmatrix}
1200x_1^2-400x_2+2 & -400x_1\\
-400x_1 & 200
\end{bmatrix}
\end{array}
$$

We start with defining the Rosenbrock function and its derivatives in R. In order to be able to use \texttt{optim} later on, we need to combine $x_1$ and $x_2$ into a single parameter
```{r}
rosenbrock <- function(x) {
   100 * (x[2] - x[1]^2)^2 + (1-x[1])^2
}

rosenbrock.gradient <- function(x) {
    c(-400*x[1]*(x[2]-x[1]^2) - 2*(1-x[1]),
      200*(x[2]-x[1]^2))
}

rosenbrock.hessian <- function(x) {
    rbind(c(1200*x[1]^2-400*x[2]+2, -400*x[1]),
    	  c(-400*x[1],                    200))
}

```
 
We can now plot the objective function and implement Newton's method.
```{r, out.width="50%"}
x1 <- seq(-1.5, 1.5, length.out=100)              
x2 <- seq(-1.5, 1.5, length.out=100)

val <- matrix(nrow=length(x1), ncol=length(x2))
for (i in seq_along(x1))                    # Evaluate function on grid
    for (j in seq_along(x2))
        val[i,j] <- rosenbrock(c(x1[i],x2[j]))

image(x1, x2, val, col=topo.colors(128))     # Draw image

max.iter <- 20                               # Set maximal number of iterations
tol <- 1e-8                                  # Set tolerance
x <- c(-0.5, 0.5)                         # Initial value
for (h in 1:max.iter) {
  xold <- x                                  # Store old x
  points(x[1], x[2])                         # Add point to plot
  print(c(x1=x[1], x2=x[2],
          f=rosenbrock(x)))                  # Print current x to the screen
  x <- x - solve(rosenbrock.hessian(x), rosenbrock.gradient(x))
                                             # Perform Newton update
  if (all(abs(x-xold)<tol))                  # Check for early convergence
    break
}
```
Newton's method has again converged within a very small number of iterations. However, we can see that in some steps the objective function increased, rather than decreased. This is because Newton's method does not contain any "safety checks". But in this example we were lucky, as Newton's method very quickly converged to the correct minimum.

We can now turn the code into a function implementing Newton's method.
```{r}
newton <- function(gradient, hessian, initial, ..., max.iter=100, tol=1e-8) {
  x <- initial                               # Initial value
  for (h in 1:max.iter) {
    xold <- x                                # Store old x
    x <- x - solve(hessian(x, ...), gradient(x, ...))
                                             # Perform Newton update
    if (all(abs(x-xold)<tol))                # Check for early convergence
      return(x)
  }
  warning("Newton's method might not have converged")
                                             # If we haven't returned already, 
                                             # it probably hasn't converged
  x
}
```
We can now employ the function to maximise the Rosenbrock function.
```{r}
newton(rosenbrock.gradient, rosenbrock.hessian, c(-0.5, 0.5))
```
####[/example]

### Quasi-Newton methods

Newton's method makes use of the second derivative $f''({\mathbf{x}})$ (``Hessian''). Taking the second derivative of the objective function is often cumbersome and time consuming (for the programmer) and expensive to evaluate (for the computer). This suggests approximating the Hessian $f''({\mathbf{x}})$. A popular approximation is the Broyden-Fletcher-Goldfarb-Shanno formula (BFGS), which approximates $f''({\mathbf{x}})$ in the $(h+1)$-st step by
$$
\mathbf{H}^{(h+1)} = \mathbf{H}^{(h)} + \frac{\mathbf{y}^{(h)} {\mathbf{y}^{(h)}}'}{{\mathbf{y}^{(h)}}'\mathbf{s}^{(h)}} - \frac{\mathbf{H}^{(h)}\mathbf{s}^{(h)}\left(\mathbf{H}^{(h)}\mathbf{s}^{(h)}\right)'}{{\mathbf{s}^{(h)}}'\mathbf{H}^{(h)}\mathbf{s}^{(h)}},
$$
where $\mathbf{y}^{(h)}=f'({\mathbf{x}}^{(h+1)}) - f'({\mathbf{x}}^{(h)})$, and $\mathbf{s}^{(h)}=-\left(H^{(h)}\right)^{-1}f'({\mathbf{x}}^{(h)})$.


### Safeguarding Newton's method

We have seen that Newton's method in 1D is not guaranteed to converge to a local maximum (or minimum). The same is true in higher dimension (even more so):  there is the possibility that it might diverge or converge to a minimum (or maximum) or a saddle point.

One of the benefits of Newton's method is its fast convergence (of course only provided it converges). This suggests trying to safeguard Newton's method to make sure it converges to a local maximum (or minimum). There are essentially two schools of thought of how this can be achieved.

#### Trust-region methods
As the name implies trust-region methods only "trust" the quadratic approximation within a (small) neighbourhood around the current value ${\mathbf{x}}^{(h)}$. At every step a constrained maximisation problem is solved:


Maximise $\displaystyle
\frac{1}{2}({\mathbf{x}}-{\mathbf{x}}^{(h)})'f''({\mathbf{x}}^{(h)})({\mathbf{x}}-{\mathbf{x}}^{(h)}) + ({\mathbf{x}}-{\mathbf{x}}^{(h)})'f'({\mathbf{x}}^{(h)})$ subject to $\|{\mathbf{x}}-{\mathbf{x}}^{(h)}\|^2\leq r^2$

Fast algorithms for solving this type of problem exist and it is easy to deal with the problem of the quadratic approximation being convex instead of concave: in this case the solution lies on the boundary.

If the value proposed by each trust-region step yields an increase in the objective function, the new value is accepted and the radius $r$ of the trust region is possibly increased. If the value proposed does not yield an increase in the objective function, it is discarded and the radius of the trust region decreased.

![Illustration of the idea behind trust-region methods](trustregion_white.pdf)

#### Line-search methods
These methods are based on the idea of keeping the direction of the Newton updates, but altering the step size if the Newton update does not yield an increase in the objective function. In this case the one-dimensional optimisation problem of maximising
$$
f\left({\mathbf{x}}^{(h)} + \xi \cdot({\mathbf{x}}^{(h+1)} - {\mathbf{x}}^{(h)})\right)
$$
over $\xi$ is solved. This corresponds to (partially) maximising $f(x)$ along the red line in the figure below. This one-dimensional optimisation problem can be solved using successive parabolic interpolation or golden section search. Line search methods are implemented in the R function `optim`.

![Illustration of the idea behind line-search methods](linesearch_white.pdf)

### Using optim in R

#### Quasi-Newton methods 
A line-search based Quasi-Newton method is implemented in R's function `optim` (whereas `optimize` performs optimisation in 1D). Its syntax is 
```{r eval=FALSE}
optim(par, fn, gr, method="BFGS", ...)
```

- `par` are the initial values for the parameters to be optimised over.
- `fn` is the function to be optimised over its first argument.
- `gr` is the  gradient (first derivative) of the function (optional - if omitted `optim` will approximate the gradient).
- Additional optional arguments (`...`) are passed on to `fn` and `gr`.

Note that `optim` performs minimisation by default. Use the additional argument `control=list(fnscale=-1)` to perform maximisation.

If `method="BFGS"` is used, `optim` can also be used for one-dimensional optimisation problems. However it is usually a better idea to `optimize` for problems in which only one parameter needs to be optimised.

####[example, continued="rosenbrock"]
We will now use the BFGS method and `optim` to find the minimum of the Rosenbrock function.
We start with redefining the Rosenbrock function in R. 
```{r}
rosenbrock <- function(x, trace=FALSE) {
   if (trace)
     points(t(x))
   100 * (x[2] - x[1]^2)^2 + (1-x[1])^2
}
```
The function has an additional argument `trace`, which, if set to `TRUE`  adds a point to a plot, so that we can trace how the algorithm evolves.

We again first create an image plot of the objective function.
```{r, out.width="50%"}
image(x1, x2, val, col=topo.colors(128))       # Draw image

optim(c(-0.5, 0.5), fn=rosenbrock,
      method="BFGS", trace=TRUE)
```
Compared to Newton's method, we needed many more iterations to find the solution (contained in `$par`), but we did not have to calculate any derivatives and `optim` made sure that we did not pursue points for which the objective function would have increased.. 
####[/example]
    
#### Nelder-Mead algorithm
`optim` also implements the Nelder-Mead algorithm. This algorithm does not make use of any derivatives (provided or calculated numerically), and it is suited to objective functions which are not differentiable or not particularly smooth. It is based on the idea of "flipping over triangles". The following section describes the basic idea of the method for two-dimensional parameters.

At every step we consider a triangle ${\mathbf{x}}^{(h)}_1$, ${\mathbf{x}}^{(h)}_2$, ${\mathbf{x}}^{(h)}_3$. We relabel the three points such that $f({\mathbf{x}}^{(h)}_1) < f({\mathbf{x}}^{(h)}_2) < f({\mathbf{x}}^{(h)}_3)$. At each step we attempt to replace ${\mathbf{x}}^{(h)}_1$ by a "better" point ${\mathbf{x}}$, i.e. a point ${\mathbf{x}}$ with $f({\mathbf{x}})>f({\mathbf{x}}^{(h)}_1)$. 

We obtain the new point ${\mathbf{x}}$ by reflecting ("flipping") the triangle, i.e. the newly proposed point is 
$${\mathbf{x}}= 2 \cdot ({\mathbf{x}}^{(h)}_2+{\mathbf{x}}^{(h)}_3) - {\mathbf{x}}^{(h)}_1.$$

If $f({\mathbf{x}})>f({\mathbf{x}}^{(h)})$ (improvement) we keep the new point and consider the triangle ${\mathbf{x}}, {\mathbf{x}}^{(h)}_2, {\mathbf{x}}^{(h)}_3$. To accelerate the algorithm we might want to expand the triangle.

If $f({\mathbf{x}})<f({\mathbf{x}}^{(h)})$ (no improvement) we discard the new triangle and shrink the current triangle.

Nelder \& Mead's method is implemented in \texttt{optim} and will be used when setting `method="Nelder-Mead"`.

```{r eval=FALSE}
optim(par, fn, method="Nelder-Mead", ...)
```

####[example, label=rosenbrock2]Rosenbrock function (continued)
In this example we apply the Nelder-Mead method to the Rosenbrock function from ref://rosenbrock.

```{r, out.width="50%"}
image(x1, x2, val, col=topo.colors(128))       # Draw image

optim(c(0,0), fn=rosenbrock,
      method="Nelder-Mead", trace=TRUE)
```
Compared to the BFGS method, the Nelder-Mead algorithm proceeds in much smaller steps.
####[/example]

If the objective function is reasonably smooth and differentiable, it is more efficient and more reliable to use the BFGS method, even if the derivative `gr` is not specified and the gradient needs to be approximated.


### A few general remarks on the practical side of local optimisation

- It is usually a good idea to plot the objective function first.
- If there are multiple extrema the solution often depends on the initialisation. Run `optim` with several different starting values (especially if you are not sure whether there are local extrema).
- Be very critical of the results obtained. Optimisation algorithms are largely black boxes which can fail in many ways!
- Rescaling (or using the control option `parscale`) is often helpful. Most optimisation methods work best if each parameter has an equally strong effect.

####[task]
Consider the bivariate function $f: [-1, 2] \times [-1, 2]\rightarrow\mathbb{R}$
$$
f(\mathbf{x})=f(x_1,x_2)=2 x_1(x_1-1) - 0.25 \cos(9x_1 + 4x_2) + x_2 (x_2-1)
$$

(a) Write a function `f` that takes a vector `x` of length 2 as argument and that returns $f(\mathbf{x})$.
(b) Create a plot of $f(\mathbf{x})$.
(c) Use the BFGS method or Nelder \& Mead's method in `optim` to find a local minimum of $f$.
(d) Using the plot from part (b) try to find the global minimum of $f(\mathbf{x})$.
(e) The derivatives of $f(\mathbf{x})$ are
$$
\begin{array}{rcl}
f'(\mathbf{x})&=&\left[\frac{\partial}{\partial x_1}f(\mathbf{x}),\frac{\partial}{\partial x_2}f(\mathbf{x})\right]
\\&=&
\left[4x_1-2+2.25\sin(9x_1+4x_2),\;\sin(9x_1+4x_2)+2x_2-1\right]\\
f''(\mathbf{x})&=&
\begin{bmatrix}
\frac{\partial^2}{\partial\partial x_1^2} f(x_1,x_2)& \frac{\partial^2}{\partial x_1 \partial x_2} f(x_1,x_2)
\frac{\partial^2}{\partial x_2\partial x_1} f(x_1,x_2)& \frac{\partial^2}{\partial x_2^2} f(x_1,x_2)
\end{bmatrix}
\\&=&\begin{bmatrix}
4+20.25\cos(9x_1+4x_2)&9\cos(9x_1+4x_2)\\
9\cos(9x_1+4x_2)&4\cos(9x_1+4x_2)+2
\end{bmatrix}
\end{array}
$$
Implement functions `f.gradient` and `f.hessian`.

(f) Finally, implement Newton's method. Does it always converge to a local minimum?
#####[answer]
(a) We first define the function `f`. We need to take in $x_1$ and $x_2$ as a single vector `x`, so that we can use the function with `optim`.
```{r}
f <- function(x) 
   2 * x[1] * (x[1]-1) - 0.25 * cos (9*x[1] + 4*x[2]) + 1.0 * x[2] * (x[2]-1)
```

(b) We will plot the function for $x_1,x_2\in [-1, 2]$.
```{r, out.width="50%"}
x1 <- seq(-1, 2, length.out=100)              
x2 <- seq(-1, 2, length.out=100)

val <- matrix(nrow=length(x1), ncol=length(x2))
for (i in seq_along(x1))                    # Evaluate function on grid
    for (j in seq_along(x2))
        val[i,j] <- f(c(x1[i],x2[j]))

image(x1, x2, val, col=topo.colors(128))     # Draw image
```
It seems that there are three local minimum, with the one in the middle being the global minimum.

(c) and (d)

```{r}
optim(c(0.5,0.5), f, method="BFGS")        # Global minimum
optim(c(0,0), f, method="BFGS")            # A local minimum
optim(c(0.9,0.9), f, method="BFGS")        # Another local minimum
```

(e) The derivatives are
```{r}
gradient <- function(x) {
  c(4*x[1]-2+2.25*sin(9*x[1]+4*x[2]),sin(9*x[1]+4*x[2])+2*x[2]-1)
}

hessian <- function(x) {
  rbind(c(4+20.25*cos(9*x[1]+4*x[2]), 9*cos(9*x[1]+4*x[2])),
        c(9*cos(9*x[1]+4*x[2]), 4*cos(9*x[1]+4*x[2])+2))
}
```

(f)
We can either code up Newton's method from first principles (for simplicity without check for early convergence) ...
```{r}
x <- c(0.5, 0.5)
for (h in 1:100)
  x <- x - solve(hessian(x), gradient(x))
x
```
... or use the function newton from ref://rosenbrock
```{r}
newton(gradient, hessian, c(0.5, 0.5))
```
However, we have t be very careful when using Newton's method in this example. It might also converge to a local maximum, which for examples happens when we start the algorithm at $x_1=0.1$ and $x_2=0.5$
```{r}
newton(gradient, hessian, c(0.1, 0.5))
```
Newton's method might also diverge.
```{r}
newton(gradient, hessian, c(0.1, 0.9))
```
This illustrates that it is a lot simpler and safer to use `optim` rather than implementing Newton's method if the objective function is multi-modal.
#####[/answer]
####[/task]


### Statistical applications

#### Maximum likelihood estimation of more than parameter

So far, we have only looked at maximum-likelihood estimation in models which only have one parameter. In this section, we will extend this to multi-parameter models.

#####[example, label="gammamle2"] Estimation of both parameters of the gamma distribution

In ref://gammamle we estimated the shape parameter $\alpha$ of the Gamma distribution, assuming the rate parameter was known to be 1. This this example we will estimate both the shape parameter $\alpha$ and the rate parameter $\theta$. 

$$
\begin{array}{rcl}
f_{\alpha,\theta}(x_i)&=&\frac{\theta^{\alpha}\exp(-\theta x_i)x^{\alpha-1}}{\Gamma(\alpha)}\\
\log f_{\alpha,\theta}(x_i)&=&\alpha\cdot\log(\theta)-\theta x_i+(\alpha-1)\cdot\log(x_i) - \log\Gamma(\alpha)
\end{array}
$$
This gives the following loglikelihood:
$$
\ell(\alpha,\theta)=\sum_{i=1}^n \log f_{\alpha,\theta}(x_i)=n\alpha\cdot\log(\theta)-\theta\sum_{i=1}^n x_i + (\alpha-1) \cdot \left(\sum_{i=1}^n \log x_i\right) - n\cdot \log \Gamma(\alpha)
$$
The first derivative (gradient) is now a vector with two entries, the derivative of the loglikelihood with respect to $\alpha$ and the derivative of the loglikelihood with respect to $\theta$.
$$
\begin{array}{rcl}
\ell'(\alpha,\theta)&=&\left[\frac{\partial}{\partial\alpha}\ell(\alpha,\theta), \frac{\partial}{\partial\theta}\ell(\alpha,\theta)\right]\\
&=&\left[
n\log(\theta)+(\sum_{i=1}^n\log x_i)-n\cdot(\log\Gamma(\alpha))',\;
\frac{n\alpha}{\theta}-\sum_{i=1}^nx_i
\right]
\end{array}
$$
The second derivative (Hessian) is now a $2\times 2$ matrix with two entries.
$$
\begin{array}{rcl}
\ell''(\alpha,\theta)&=&
\begin{bmatrix}
\frac{\partial^2}{\partial \alpha^2} \ell(\alpha,\theta)&
\frac{\partial^2}{\partial \alpha\partial \theta} \ell(\alpha,\theta)\\
\frac{\partial^2}{\partial \theta\partial \alpha} \ell(\alpha,\theta)&
\frac{\partial^2}{\partial \theta^2} \ell(\alpha,\theta)&
\end{bmatrix}
\\
&=&\begin{bmatrix}
-n\cdot(\log\Gamma(\alpha))''&\frac{n}{\theta}\\
\frac{n}{\theta} & -\frac{n\alpha}{\theta^2}
\end{bmatrix}
\end{array}
$$

We can now implement these as functions in R. In order to be able to use the function `newton` we have written in ref://rosenbrock and the function `optim` from R, we combine $\alpha$ and $\theta$ into a vector of length 2, which we will call `par.

```{r}
loglik <- function(par, x) {
  n <- length(x)
  alpha <- par[1]
  theta <- par[2]
  n*alpha*log(theta)-theta*sum(x)+(alpha-1)*sum(log(x))-n*lgamma(alpha)
}

loglik.gradient <- function(par, x) {
  n <- length(x)
  alpha <- par[1]
  theta <- par[2]
  c(n * log(theta)+sum(log(x)) - n*digamma(alpha),
    n*alpha/theta-sum(x))
}	 

loglik.hessian <- function(par, x) {
  n <- length(x)
  alpha <- par[1]
  theta <- par[2]
  rbind(c(- n*trigamma(alpha), n/theta),
        c(n/theta, -n*alpha/theta^2)) 
}	 
```

To test our functions, we will again use a sample from the $\textsf{Expo}(1)$ distribution, which corresponds to $\alpha=1$ and $\theta=1$.

```{r, out.width="50%"}
x <- rexp(1e3)
```
We first plot the loglikelihood for $\alpha,\theta \in (0.5, 1.5)$.
```{r, out.width="50%"}
alpha <- seq(0.5, 1.5, length.out=100)
theta <- seq(0.5, 1.5, length.out=100)
val <- matrix(nrow=length(alpha), ncol=length(theta))
for (i in seq_along(alpha))                   
    for (j in seq_along(theta))
    	val[i,j] <- loglik(c(alpha[i],theta[j]), x=x)
image(alpha, theta, val, col=topo.colors(128))
```
The plot suggests there is only one local (and thus also global) minimum.

We can now employ Newton's method ...
```{r}
newton(loglik.gradient, loglik.hessian, c(0.5, 0.5), x=x)
```
... or the BFGS Quasi-Newton method from `optim` ...
```{r}
optim(c(0.5, 0.5), loglik, loglik.gradient,  method="BFGS", x=x, 
      control=list(fnscale=-1))
```
Note that we had to use `control=list(fnscale=-1)` to get `optim` to perform maximisation.

Both methods correctly identify the maximum likelihood estimate. 
#####[/example]


#### Robust regression

#####[example] 
The Predictive Modelling course focuses on least squares regression. For a data set of $n$ observations $(x_i,y_i)$, least squares regression tries to find the regression coefficients $\hat{\boldsymbol\beta}=(\hat \beta_0, \hat\beta_1)$, which minimise
$$
\sum_{i=1}^n (y_i-\beta_0-\beta_1x_i)^2
$$
You have learned that the least squares estimate can be found in closed form
$$\hat{\boldsymbol\beta}=(\mathbf{X}^{\top}\mathbf{X})^{-1}\mathbf{X}^{\top}\mathbf{y}$$, where
$$
\mathbf{X}=\begin{bmatrix}1&x_1\\\vdots&\vdots\\1&x_n\end{bmatrix}\qquad\mbox{and}\qquad \mathbf{y}=\begin{bmatrix}y_1\\\vdots\\y_n\end{bmatrix}
$$
However, we could ignore our linear algebra knowledge and find the regression estimates by "brute force" using `optim`. We use a subset of the mammals data from `MASS` an an example. In this data set we try to explain the brain weight of mammals as a function of the body weight. 
```{r}
library(MASS)
mammals <- subset(mammals, body<=80)
X <- cbind(1, mammals$body)
y <- mammals$brain
objective.function <- function(beta, X, y) {
  sum((X%*%beta-y)^2)
}		   
least.squares.estimate <- optim(c(0,0), objective.function, method="BFGS", X=X, y=y)
least.squares.estimate
```
We obtain the same coefficients if we use `lm`.
```{r}
lm(brain~body, data=mammals)
```
So far we have not gained anything by using `optim`. However, we can make our function more generic by replacing the square by a more generic loss function.
```{r}
objective.function <- function(beta, X, y, loss=function(eps) eps^2, ...) 
  sum(loss(X%*%beta-y, ...))
```
So, by default, this function computes the least squares estimate, but it can also use other loss functions. We have add a `...` argument, passed on to `loss`, in case we need to pass on arguments to the loss function.

This is advantageous in this example, because the regression line is influenced by one outlier (humans).
```{r, out.width="50%"}
plot(brain~body, data=mammals)
abline(least.squares.estimate$par)
```

The reason why least-squares regression is so sensitive to outliers is that the quadratic loss function gives large residuals an very large weight.

You might want to argue that the plot suggests that we should work with the data on the log-scale, but in this example we would like to use a robust regression model instead.

We could for example consider the sum of the moduli of the residuals instead ("$L_1$ loss). Large residuals do not get such large weight then, so this is one way of making our regression algorithm more robust against outliers.
```{r}
l1.loss <- function(eps)
  abs(eps)
l1.estimate <- optim(c(0,0), objective.function, method="Nelder-Mead", X=X, y=y, 
                     loss=l1.loss)
l1.estimate
```
We can see that the slope is now estimated to be a lot lower. However the objective function is not continuously differentiable any more, which is why we have used the Nelder-Mead algorithm.

We can now compare the robust regression line to the least-squares regression line computed using the three different loss functions.
```{r, out.width="50%"}
plot(brain~body, data=mammals)
abline(least.squares.estimate$par, lty=2)
abline(l1.estimate$par, lty=1)
legend("topleft", lty=c(2, 1), c("Least squares", "L1 loss"))
```


If we want to maintain robustness but make the loss function continuously differentiable, we can use the Huber loss function. 
$$
H(\epsilon) = \begin{cases}
\epsilon^2/2 & \mbox{for} |\epsilon|\leq \epsilon_0\\
\epsilon_0(|\epsilon|-\epsilon_0/2) & \mbox{for } |\epsilon|\geq \epsilon_0
\end{cases}
$$
for some $\epsilon_0>0$.

Huber's loss is a quadratic function (parabola) for $\epsilon\in(-\epsilon_0,\epsilon_0)$, but a linear function outside this interval, which means that it still is robust as long as we do not choose $\epsilon_0$ too large.

The figures below compare the three loss functions (using $\epsilon_0=2$).

```{r, out.width="100%", fig.width=9, fig.height=3, echo=FALSE}
huber.loss <- function(eps, eps0=1) 
  ifelse(abs(eps)<eps0, eps^2/2, eps0*(abs(eps)-eps0/2)) 
epsilon <- seq(-3, 3, len=1000)
par(mfrow=c(1,3))
plot(epsilon, epsilon^2, type="l", ylim=c(0, 9),
     xlab=expression(epsilon), ylab=expression(epsilon^2),
     main="Least-squares loss")
plot(epsilon, abs(epsilon), type="l", ylim=c(0, 9),
     xlab=expression(epsilon), ylab=expression(epsilon^2),
     main="L1 loss")
plot(epsilon, huber.loss(epsilon, eps0=2), type="l", ylim=c(0, 9),
     xlab=expression(epsilon), ylab=expression(H(epsilon)),
     main="Huber loss")
```

We can now implement the Huber loss function in R and then use `optim` again to find the regression coefficients
```{r}
huber.loss <- function(eps, eps0=1) 
  ifelse(abs(eps)<eps0, eps^2/2, eps0*(abs(eps)-eps0/2)) 
huber.estimate <- optim(c(0,0), objective.function, method="BFGS", X=X, y=y, 
                     loss=huber.loss, eps0=1)
huber.estimate
```
The Huber estimate is almost identical to the one we obtained using the $L_1$ loss.
#####[/example]


#### Censored regression
#####[example] 
In the Predictive Modelling course you have also learned that the least squares regression estimate of $\hat{\boldsymbol \beta}$ is the maximum-likelihood estimate in the model
$$
y_i\sim\textsf{N}(\beta_0+\beta_1x_i, \sigma^2), \qquad i=1,\ldots,n.
$$
(The estimates of $\sigma^2$ are different by a factor of $n/(n-2))$.

As an example we will consider an example dataset in which the concentration of a contaminant (measured in mg/l) was measured over time (measured in days). 
```{r}
days <- 1:10
concentrations <- c(2.6, 2.3, 1.5, 1.4, 1.4, 1.1, 0.8, 0.8, 0.6, 0.7)
```
We will use a linear model relating the logarithm of the concentration to the number of days.
```{r}
lm(log(concentrations) ~ days) 
```

We will now try to find these coefficients using `optim`. We use a small trick here: we use $\log(\sigma^2)$ instead of $\sigma^2$ as a parameter. The range of $\log(\sigma^2)$ is the entire real line, so it doesn't matter if `optim` sets it to a negative value, which would be an issue for $\sigma^2$. So our parameter vector is $(\beta_0, \beta_1, \log\sigma^2)$.

```{r}
y <- log(concentrations)
X <- cbind(1, days)

regression.loglikelihood <- function(par, X, y) {
  beta <- par[1:(length(par)-1)]             # Parameters except for last are beta
  sigma2 <- exp(par[length(par)])            # Last parameter is log(variance)
  sum(dnorm(y, X%*%beta, sqrt(sigma2), log=TRUE))
}

optim(c(0, 0, 0), regression.loglikelihood, method="BFGS",
      control=list(fnscale=-1), X=X, y=y)
```
The estimated regression coefficients (first two entries of `$par`) are (almost) identical to the ones found using `lm`.

There is, however, no point in using `optim` in this context, given that we can use `lm`, which exploits that we can find the parameter estimates in closed form.

However, `lm` cannot cope with an issue arising quite often in practice when working with such data: limits of detection. Imagine the concentrations were as above but that the device we use for measuring the concentrations cannot register any concentrations below 1 mg/l. In this case it would report the last four observations as below the detection limit. How could we deal with this?

- We could simply omit the last four values, but this would not be ideal, because we would be ignoring information that we possess. In our example, this strategy would leave us with just six observations.
- We could replace the "non-detects" by the detection limit, or, which is also common in practice, half the detection limit. However this completely arbitrary choice will affect the result of our regression model, which is not ideal.
- A third approach to dealing with this issue is to turn to a likelihood-based approach. In our case, we managed to make a "normal" observation for the first six observations, hence we can use the logarithm of the probability density function of the $\textsf{N}(\beta_0+\beta_1x_i, \sigma*2)$ distribution as the corresponding contributions to the loglikelihood. For the "censored" observations we only know that the true value was less than the detection limit. Thus we can use the logarithm of the cumulative distribution function of the $\textsf{N}(\beta_0+\beta_1x_i, \sigma*2)$ distribution as the corresponding contributions to the loglikelihood. If we denote the probability density function of the $\textsf{N}(\mu,\sigma^2)$ distribution by $f_{\mu\,sigma^2}(x)$ and its cumulative distribution function by $F_{\mu,\sigma^2}(x)$, then the loglikelihood becomes (using $t$ as the detection limit)
$$
\sum_{i=1}^6 \log f_{\beta_0+\beta_1 x_i,\sigma^2}(y_i) + \sum_{i=7}^{10} \log F_{\beta_0+\beta_1 x_i,\sigma^2}(t).
$$

We can now implement this in R. We start by creating separate vectors and matrices for the observed data and for the censored data.
```{r}
y.observed <- y[1:6]
X.observed <- X[1:6,]
t <- 1
X.censored <- X[7:10,]
```
We can now define the loglikelihood as described above.
```{r}
censored.regression.loglikelihood <- function(par, X.observed, y.observed, 
                                              X.censored, t) {
  beta <- par[1:(length(par)-1)]
  sigma2 <- exp(par[length(par)])
  sum(dnorm(y.observed, X.observed%*%beta, sqrt(sigma2), log=TRUE)) +
    sum(pnorm(t, X.censored%*%beta, sqrt(sigma2), log=TRUE))
}

optim(c(0, 0, 0), censored.regression.loglikelihood, method="BFGS",
      control=list(fnscale=-1), X.observed=X.observed, y.observed=y.observed,
      X.censored=X.censored, t=1)
```
This approach gives a much better estimate than say substituting the observations below the limit of detection by the limit. The censored regression gives a regression line that is very close to the line we would obtain if the device we used had no limit of detection and allowed us to observe all concentrations.

```{r, out.width="50%",echo=FALSE}
plot(days, log(concentrations), col=rep(c("black", "darkgrey"), c(6,4)))
points(7:10, rep(0,4), pch=6)
abline(lm(log(concentrations) ~ days), lty=2)
abline(lm(log(ifelse(concentrations<1, 1, concentrations)) ~ days), lty=3)
abline(optim(c(0, 0, 0), censored.regression.loglikelihood, method="BFGS",
      control=list(fnscale=-1), X.observed=X.observed, y.observed=y.observed,
      X.censored=X.censored, t=1)$par[1:2])
legend("topright", lty=c(1,2,3), c("Censored regression", "Using all data (no limit of detection)", "Using detection limit as data"))
```
#####[/example]


### Global optimisation
All methods considered so far (Newton's method, BFGS, Nelder&Mead, ...) perform (at most) *local* optimisation. However, local extrema are not necessary global extrema. Local extrema can be very far from the true global extremum. In the case of maximum likelihood estimation, getting stuck in a local extremum can yield invalid conclusions.

There are several strategies for global optimisation:

- One simple approach would  be to evaluate function on a regular grid. Additionally we could start a local optimiser at each grid point. Unless we know some properties of the objective function, choosing an appropriate grid is difficult (How fine should the grid be?). However, most importantly, this method is computationally prohibitive, especially for high-dimensional parameter spaces.
- A usually more promising approach is to use stochastic optimisation methods. One such method is simulated annealing.

#### Simulated annealing
The basic idea of simulated annealing (and pretty much every other stochastic optimisation algorithm) is to propose new values at random. In order to achieve global optimisation it is not enough to only accept values that yield an increase in the objective function. We need to find a suitable trade-off between maximising the objective function and exploring the parameter space:

- Maximisation: We should accept every value that yields an increase in the objective function.
- Exploration: "Sometimes" we should accept values that yield a decrease in the objective function. This allows us to escape from local extrema.

The most basic version of the simulated annealing algorithm is as follows:

For $h=1,2,3,\ldots$:

i. Draw $\boldsymbol \epsilon$ from a symmetric distribution (Cauchy, t, normal)
ii. New candidate: ${\mathbf{x}}={\mathbf{x}}^{(h)}+ \boldsymbol \epsilon$.
    - If $f({\mathbf{x}})\geq f({\mathbf{x}}^{(h-1)})$ accept ${\mathbf{x}}$, i.e. set ${\mathbf{x}}^{(h+1)}={\mathbf{x}}$.
    - If $f({\mathbf{x}})<f({\mathbf{x}}^{(h-1)})$ accept ${\mathbf{x}}$ with probability
      $$
      \exp\left(\frac{f({\mathbf{x}})-f({\mathbf{x}}^{(h-1)})}{T}\right)
      $$
iii. Possibly adapt ``temperature'' $T$.

The parameter $T$ ("temperature") governs trade-off between exploration (large $T$) and maximisation (small $T$).

The simulating annealing algorithm has a stochastic interpretation (for fixed $T$). It eventually samples from a distribution with p.d.f. proportional to
$$\left(\exp(f({\mathbf{x}}))\right)^{1/T}$$
This distribution is for small $T$ increasingly concentrated around the global maximum. You will learn more about such methods in *Uncertainty Assessment and Bayesian Computation*.

An important question is how to choose $T$. Initially exploration is more important than maximisation. This suggests starting off with a large $T$. Towards the end however maximisation becomes more important than exploration (we want to eventually find the maximum!). This suggests choosing a small $T$ towards the end. Thus we should start with a large temperature $T$ and slowly decrease it. There are many ways of decreasing the temperature ("tempering"):

- Logarithmic tempering: $T^{(h)}=\frac{T_0}{\log(1+h)}$.Theoretical convergence results are available for logarithmic tempering (and  discrete parameter space). It is however very slow.
- Geometric tempering: $T^{(h)}=\alpha^h\cdot T_0$ for some $\alpha<1$. Geometric tempering is a popular choice in practice and is usually faster.

![](sandemo2.pdf)

Simulated annealing can be performed using R's \texttt{optim} function:
```{r eval=FALSE}
optim(par, fn, method="SANN", ...,
      control=list(parscale=parscale))
```
You can (and often have to) use the control parameter `parscale` to ensure that proposed updates are spread out "enough". Otherwise `optim` is unlikely to find the global maximum.

Note that there is no guarantee that simulated annealing converges to a global maximum of the objective function in finite time.  In practice it would be unrealistic  to expect simulated annealing to reliably converge to a global maximum, however in most cases it will find a "good enough" solution.

####[example, continued=rosenbrock]
In this example we apply simulated annealing to find the minimum of the Rosenbrock function from ref://rosenbrock.

```{r, out.width="50%"}
image(x1, x2, val, col=topo.colors(128))       # Draw image

sol <- optim(c(0,0), fn=rosenbrock,
             method="SANN", trace=TRUE)

sol

points(sol$par[1], sol$par[2], cex=2, pch=3, col="white")
                                        # Mark minimum found (should be (1,1))
```
Simulated annealing is overkill in this example, as there is only one minimum. 
####[/example]



\newpage
