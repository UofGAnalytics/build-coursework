## Review exercises
```{r, echo=FALSE, results="hide"}
library(ggplot2)
theme_update(plot.background = element_rect(fill = "transparent",colour = NA),
                                 legend.background = element_rect(fill = "transparent",colour = NA))
```

For the tasks in this section you need to load a data file `t4.RData`, which you can download from

- http://www.stats.gla.ac.uk/~levers/rp/t4.RData

If you run R on a computer connected to the internet, you can simply run
```{r}
load(url("http://www.stats.gla.ac.uk/~levers/rp/t4.RData"))
```

###[task]
The binomial coefficient is defined as
$$
{n \choose k}=\frac{n!}{k!(n-k)!}
$$

(a) Write a function `binomial.coefficient` that takes `n` and `k` as arguments and that returns $n \choose k$, calculated as set out above. Use it to compute $6 \choose 3$.

*Hint:* $x!$ can be computed using the function `factorial`.

(b) Next write a function `binary.entropy`, which takes a number `p` ($0<p<1$) as argument and returns the binary entropy
$$
H(p)=-p\log(p)-(1-p)\log(1-p)
$$

(c) Write a function `approx.lbincoef`, which takes integers `n` and `k` as arguments and returns the approximation 
$$
n\cdot H\left(\frac{k}{n}\right)
$$
to $\log{n\choose k}$, the logarithm of binomial coefficient. $H(\cdot)$ is the function defined in part (b) of this question. 

(d) Use your functions from part (a) and (c) to compute (an approximation to)
$\log{9000\choose 4000}$.

*Hint:* The exact answer is $\log{9000\choose 4000}=6177.88\ldots$.

####[answer]

\mbox{ }

#####[video, videoid="ep4PVEB5WG4", duration="4m49s"] Video model answers

We can use the following R code.

```{r}
#' Calculate the binomial coefficient 
#' @param n number of items to choose from (urn model)
#' @param k number of items to choose (urn model)
#' @return the binomial coefficient  (calculated in a numerically unstable way) 
binomial.coefficient <- function(n,k) {
  factorial(n) / (factorial(n-k)*factorial(k))
}

binomial.coefficient(6, 3)

#' Calculate the entropy of a binary random variable (Bernoulli distribution)
#' @param p probability of success
#' @return the binary entropy \eqn{-p\log(p)-(1-p)\log(1-p)}
binary.entropy <-  function(p) {
  -p*log(p)-(1-p)*log(1-p)
}

#' Calculate approximation to the binomial coefficient based on the entropy
#' @param n number of items to choose from (urn model)
#' @param k number of items to choose (urn model)
#' @return approximation to the logarithm of the binomial coefficient
approx.lbincoef <- function(n, k) {
  n*binary.entropy(k/n)
}

log(binomial.coefficient(9000, 4000))
                                 # fails (numerically unstable)
approx.lbincoef(9000,4000)       # gives an approximation         
```




####[/answer]
###[/task]

###[task]
In this task you implement two methods for drawing (pseudo-)random realisations from the normal distribution. It is fairly easy to draw (pseudo-)random numbers from a uniform distribution on a computer. This task will show two related methods for converting these uniform random numbers into draws from a normal distribution.
  

(a) Write a function `box.muller`, which takes no arguments, and which returns a pair of numbers $(z_1,z_2)$ calculated as follows:

	1. Draw $u_1$ and $u_2$ from a uniform distribution on the interval $(0,1)$. (You can use the function call `runif(2)` for this.)
	2. Compute
	   $$
	   \begin{array}{rcl}\theta&=&2\pi u_1\\r&=&\sqrt{-2\log(u_2)}\\z_1&=&r\sin(\theta)\\z_2&=&r\cos(\theta)\end{array}
       $$
       $z_1$ and $z_2$ is then an independent pair of random realisations from the $\textsf{N}(0,1)$ distribution.

(b) Write a function `polar.marsaglia`, which takes no arguments and which returns a pair of numbers $(z_1,z_2)$ calculated as follows:  
	1. Carry out steps i. and ii. until $s<1$ ...
	      i. Use the R function `runif` to draw a pair of independent random numbers, $u_1$ and $u_2$, from the uniform distribution on the interval $[-1,1]$. (You can use the function call `runif(2, -1, 1)` for this.)
	      ii. Compute $s=u_1^2+u_2^2$.
    2. Compute $\rho=\sqrt{\frac{-2\log(s)}{s}}$.
    3. Compute $z_1=\rho \cdot u_1$ and $z_2=\rho \cdot u_2$

 $z_1$ and $z_2$ is then again an independent pair of random realisations from the $\textsf{N}(0,1)$ distribution.

(c) Write a function `simulate.normal`, which takes the arguments `n`, `mu`, `sigma`, and `method`. The default value of `mean` should be 0, the default value of `sigma` should be 1 and the default value of `method` should be `box.muller`. 
  
	The function should use the requested method to generate a sample $(z_1, \ldots, z_n)$ from the standard normal distribution (using the functions you have defined in parts (a) and (b)) and then return $(x_1,\ldots, x_n)$ calculated as $x_i=\mu+\sigma\cdot z_i$.

(d) Use your function to generate a sample of size 1,000 from the $\textsf{N}(2, 3^2)$ distribution and calculate the empirical mean and empirical standard deviation of the sample.

####[answer]

\mbox{ }

#####[video, videoid="aslFqiANyZg", duration="9m31s"] Video model answers

We can use the following R code.

```{r}
#' Draw two realisations from the standard normal distribution using the
#' Box-Muller method
#' @return a vector of length two containing the random numbers
box.muller <- function() {
  u <- runif(2)
  theta <- 2*pi*u[1]
  r <- sqrt(-2*log(u[2]))
  r*c(sin(theta), cos(theta))
}

#' Draw two realisations from the standard normal distribution using the
#' polar Marsaglia method
#' @return a vector of length two containing the random numbers
marsaglia.polar <- function() {
  while (TRUE) {
    u <- runif(2, -1, 1)
    s <- sum(u^2)
    if (s<1) break
  }
  rho <- sqrt(-2*log(s)/s)
  rho*u
}

#' Draw n realisations from the normal distribution
#' @param n desired sample size
#' @param mu mean (default 0)
#' @param sigma standard deviation (default 1)
#' @method method to be used, either \code{box.muller} or \code{polar.marsaglia}
#' @return a vector of length n containing the random numbers
simulate.normal <- function(n, mu=0, sigma=1, method=box.muller) {
    result <- matrix(nrow=ceiling(n/2), ncol=2)  
    for (i in 1:nrow(result))
        result[i,] <- mu+sigma*method()
    result[1:n]     
}

test <- simulate.normal(1000, mu=2, sigma=3)
mean(test)
sd(test)   
```
####[/answer]
###[/task]
  

###[supplement] The Probability Theory behind the Box-Muller method and the polar Marsaglia method (not an easy read)
For those of you who want to know why these methods work, here are the proofs, which use theory covered in the *Probability and Stochastic Models* course (but which are admittedly rather contrived).

#### Box-Muller method
$U_1$ and $U_2$ are independent realisations from the $\textsf{U}(0,1)$ distribution, thus their marginal probability density functions are 
$$
f_{U_i}(u_i)=\begin{cases}
  1 &\mbox{if } 0<u_i< 1\\
  0 & \mbox{otherwise,}
\end{cases}
$$
and their joint probability density function is
$$
f_{U_1U_2}(u_1,u_2)=f_{U_1}(u_1)\cdot f_{U_2}(u_2)=\begin{cases}
  1 &\mbox{if } 0< u_1< 1, \; 0<u_2< 1\\
  0 & \mbox{otherwise }
\end{cases}
$$
We first work out the probability density functions of $\theta=2\pi U_1$ and $R=\sqrt{-2\log(U_2)}$. The corresponding inverse transformations are $U_1=\frac{1}{2\pi}\theta$ and $U_2=\exp(-R^2/2)$.
$\theta$ and $R$ are defined independently of each other and thus we can work out their probability density functions one-by-one.
For $0<\theta<2\pi$
$$
f_{\theta}(\theta)=f_{U_1}\left(\frac{1}{2\pi}\theta\right)\cdot\left|\frac{\partial u_1}{\partial \theta}\right| = 1\cdot \left|\frac{1}{2\pi}\right|=\frac{1}{2\pi}
$$
and for $r> 0$
$$
f_R(r)=f_{U_2}(\exp(-r^2/2))\cdot\left|
\frac{\partial u_2}{\partial r}\right| = 1\cdot  \left| r\exp(-r^2/2)\right|=r\exp(-r^2/2).
$$

As $\theta$ and $R$ are independent their joint probability density function is
$$
f_{\theta R}(\theta, r)=f_{\theta}(\theta)\cdot f_R(r)=\begin{cases}
  \frac{r}{2\pi}\exp(-r^2/2) &\mbox{for } 0<\theta<2\pi,\; r>0\\
  0 &\mbox{otherwise.}
\end{cases}
$$

We now have to consider the transformation $Z_1=R\cdot\sin(\theta)$ and $Z_2=R\cdot\cos(\theta)$

The inverse transformation (which you can find by adding up $Z_1^2+Z_2^2$ and exploiting that $\cos(\theta)^2+\sin(\theta^2)=1$ and using that $\tan(\theta)=\frac{\sin(\theta)}{\cos(\theta)}$) is
$\theta=\arctan\left(\frac{z_1}{z_2}\right)$ and $R=\sqrt{Z_1^2+Z_2^2}$.

Essentially, $R$ and $\theta$ are nothing other than $Z_1$ and $Z_2$ expressed in polar coordinates.

The Jacobian is
$$
\begin{vmatrix}
\frac{\partial\theta}{\partial z_1} & \frac{\partial\theta}{\partial z_2}\\
\frac{\partial r}{\partial z_1} & \frac{\partial r}{\partial z_2}
\end{vmatrix}
=\begin{vmatrix}
\frac{z_2}{z_1^2+z_2^2} & -\frac{z_1}{z_1^2+z_2^2}\\
\frac{z_1}{\sqrt{z_1^2+z_2^2}} & \frac{z_2}{\sqrt{z_1^2+z_2^2}}
\end{vmatrix}
=\frac{1}{\sqrt{z_1^2+z_2^2}}
$$
Thus the joint probability density function of $Z_1$ and $Z_2$ is
$$
\begin{array}{rcl}
f_{Z_1Z_2}(z_1,z_2)&=&
f_{\theta R}\left(\arctan(z_1/z_2), \sqrt{z_1^2+z_2^2}\right)\cdot \begin{vmatrix}
\frac{\partial\theta}{\partial z_1} & \frac{\partial\theta}{\partial z_2}\\
\frac{\partial r}{\partial z_1} & \frac{\partial r}{\partial z_2}
\end{vmatrix}\\
  &=&\frac{\sqrt{z_1^2+z_2^2}}{2\pi}\exp\left(-(z_1^2+z_2^2)/2\right)\cdot \frac{1}{\sqrt{z_1^2+z_2^2}}\\
  &=&\frac{1}{2\pi}\exp\left(-(z_1^2+z_2^2)/2\right)\\=
  &=&\underbrace{\frac{1}{\sqrt{2\pi}}\exp(-z_1^2/2)}_{=f_{Z_1}(z_1)}\cdot \underbrace{\frac{1}{\sqrt{2\pi}}\exp(-z_2^2/2)}_{=f_{Z_2}(z_2)},
  \end{array}
$$  
thus $Z_1$ and $Z_2$ are independent realisations from the standard normal distribution. 


#### Polar Marsaglia method
$s=\sqrt{u_1^2+u_2^2}$ in step 1 is the squared distance of the point $(u_1, u_2)$ from the origin. We keep drawing until $s<1$, i.e. until $(u_1,u_2)$ lies inside the unit disc. 

```{r, echo=FALSE, dev.args=list(pointsize=10), fig.width=10, fig.height=6, out.width="\\textwidth"}
n <- 1000
data <- data.frame(u1=runif(n, -1, 1), u2=runif(n, -1, 1))
data <- transform(data, accepted=ifelse(u1^2+u2^2<1, "accepted", "rejected"))
library(ggplot2)
qplot(u1, u2, colour=accepted, data=data, xlab=quote(u[1]), ylab=quote(u[2]))+coord_equal(ratio=1)
```

After the rejection step, $(U_1,U_2)$ are drawn from a uniform distribution on the unit disc (rather than the unit interval). You will learn more about rejection sampling in the course \textit{Uncertainty Quantification and Bayesian Computation}.


The joint probability density function of $U_1$ and $U_2$ is thus
$$
f_{U_1U_2}(u_1,u_2)=f_{U_1}(u_1)\cdot f_{U_2}(u_2)=\begin{cases}
  \frac{1}{\pi} &\mbox{if } u_1^2+u_2^2< 1\\
  0 & \mbox{otherwise }
\end{cases}
$$


If we define $\theta$ and $r$ as $\theta=\arctan\left(\frac{u_1}{u_2}\right)$ and $r=\sqrt{-2\log\left(u_1^2+u_2^2\right)}$ ,
then the inverse transform is given by
$u_1=\exp(-r^2/4)\sin(\theta)$ and $u_2=\exp(-r^2/4)\cos(\theta)$,

which we can verify by plugging the expressions for $u_1$ and $u_2$ into the formulae for $\theta$ and $r$.
$$
\begin{array}{rcl}
\arctan\left(\frac{u_1}{u_2}\right)&=&\arctan\left(\frac{\exp(-r^2/4)\sin(\theta)}{\exp(-r^2/4)\cos(\theta)}\right)=\arctan\left(\tan(\theta)\right)=\theta
\\
  \sqrt{-2\log\left(u_1^2+u_2^2\right)}&=&  \sqrt{-2\log\left(\left(\exp(-r^2/4)\sin(\theta)\right)^2+\left(\exp(-r^2/4)\cos(\theta)\right)^2\right)}\\
  &=&\sqrt{-2\log\left(\exp(-r^2/2)\left(\sin(\theta)^2+\cos(\theta)^2\right)\right)}\\
  &=&\sqrt{-2\log\left(\exp(-r^2/2)\right)}=\sqrt{r^2}=r
  \end{array}
  $$
  
The Jacobian of the inverse transformation is
$$
\begin{vmatrix}
\frac{\partial u_1}{\partial \theta} & \frac{\partial u_1}{\partial r}\\
\frac{\partial u_2}{\partial \theta}  & \frac{\partial u_2}{\partial r}
\end{vmatrix}
=\begin{vmatrix}
\exp(-r^2/4)\cos(\theta) &\frac{r}2\exp(-r^2/4)\sin(\theta)\\
-\exp(-r^2/4)\sin(\theta) & \frac{r}2\exp(-r^2/4)\cos(\theta)
\end{vmatrix}
=\frac{r}2\exp(-r^2/2)
$$
Thus the joint probability density function is for $0<\theta<2\pi$ and $r>0$ given by
$$
\begin{array}{rcl}
f_{\theta R}(\theta, r) &=&
f_{U_1U_2}(\exp(-r^2/4)\sin(\theta),\exp(-r^2/4)\cos(\theta))\cdot \begin{vmatrix}
\frac{\partial u_1}{\partial \theta} & \frac{\partial u_1}{\partial r}\\
\frac{\partial u_2}{\partial \theta}  & \frac{\partial u_2}{\partial r}
\end{vmatrix}
\\&=&\frac{1}{\pi}\cdot \frac{r}2\exp(-r^2/2)=\frac{r}{2\pi} \exp(-r^2/2),
\end{array}
$$
which is exactly the same as the joint probability density function of $\theta$ and $R$ which we have derived for the Box-Muller method. 

We will now show that  $R\cdot\sin(\theta)$ and $R\cdot\cos(\theta)$ from the Box-Muller method are exactly the same as  $\rho\cdot U_1$ and $\rho\cdot U_2$ from the polar Marsaglia method, which concludes the proof.
$$
\begin{array}{rcl}
R\cdot\sin(\theta)&=&\sqrt{-2\log\left(U_1^2+U_2^2\right)}\cdot \sin\left(\arctan\left(\frac{U_1}{U_2}\right)\right)=
    \sqrt{-2\log\left(U_1^2+U_2^2\right)}\cdot \underbrace{\frac{\frac{U_1}{U_2}}{\sqrt{1+\left(\frac{U_1}{U_2}\right)^2}}}_{=\frac{U_1}{\sqrt{U_1^2+U_2^2}}}\\
  &=&\underbrace{\sqrt{\frac{-2\log\left(U_1^2+U_2^2\right)}{{U_1^2+U_2^2}}}}_{=\rho} \cdot U_1=\rho\cdot U_1\\
 R\cdot\cos(\theta)&=&\sqrt{-2\log\left(U_1^2+U_2^2\right)}\cdot \cos\left(\arctan\left(\frac{U_1}{U_2}\right)\right)=
    \sqrt{-2\log\left(U_1^2+U_2^2\right)}\cdot \underbrace{\frac1{\sqrt{1+\left(\frac{U_1}{U_2}\right)^2}}}_{=\frac{U_2}{\sqrt{U_1^2+U_2^2}}}\\
  &=&\underbrace{\sqrt{\frac{-2\log\left(U_1^2+U_2^2\right)}{{U_1^2+U_2^2}}}}_{=\rho} \cdot U_2=\rho \cdot U_2,
  \end{array}
  $$
We have used that $\sin(\arctan(t))=\frac{t}{\sqrt{1+t^2}}$ and $\cos(\arctan(t))=\frac{1}{\sqrt{1+t^2}}$.
###[/supplement]


###[task, label="learnapply"] apply and sweep
In this task you will learn about the functions `apply` and `sweep`. `apply` is one of the most important "loop-avoidance" functions  in R.  Historically these functions used to be somewhat more efficient than loops, however this is not the case anymore. However, if used appropriately, they can make the code more compact and easier to read. 
  
The function `apply` applies a given function to the rows or column of a data set. Its syntax is
```r
apply(X, MARGIN, FUN, ...)
```
The arguments of `apply` are:

- `X` is the matrix or data frame (or array in more general) to be used.
- `MARGIN` indicates whether the function should be applied row-wise (`MARGIN=1`) or column-wise (`MARGIN=2`).
- `FUN` is the function to be applied. Additional arguments to the function can be supplied using the `...` argument.

More mathematically speaking, `apply` returns a vector `r`, the length of which equals the number of rows (`MARGIN=1`) or columns (`MARGIN=2`) of `X`. Its entries are

- $r_i=\texttt{FUN}(x_{i1},\dots, x_{ip})$ for $i=1,\ldots,n$ if `MARGIN=1`, and
- $r_j=\texttt{FUN}(x_{1j},\dots, x_{nj})$ for $j=1,\ldots,p$ if `MARGIN=2`.

For example, if we set
```{r}
X <- rbind(c(1, 0, 3),
           c(2, 4, 0))
apply(X, 1, max)                 # row-wise maxima
apply(X, 2, max)                 # column-wise maxima
``` 

We can also supply a user-defined function to `apply`. If we want to calculate the sum of the squares of each row $\sum_{j=1}^3 X_{ij}^2$ we can use
```{r}
apply(X, 1, function(x) sum(x^2))
```
In this case we can get away without a custom function by squaring the entries of `X` before supplying it to `apply`.
```{r}
apply(X^2, 1, sum)
```

Now consider a discrete bivariate distribution specified by the joint probability mass function
```{r}
pmf <- rbind(c(0.1, 0.1, 0.1, 0.1),
             c(0, 0.2, 0, 0),
             c(0.2, 0, 0.1, 0.1))
colnames(pmf) <- c("X=1", "X=2", "X=3", "X=4")
rownames(pmf) <- c("Y=1", "Y=2", "Y=3")
pmf
```
(This matrix is available in `t4.RData`).

(a) The marginal probability mass function of $X$ is given by the sum of each of the columns of the matrix. Use `apply` to calculate the marginal probability mass function of $X$.
(b) The marginal probability mass function of $Y$ is given by the sum of each of the rows of the matrix. Use `apply` to calculate the marginal probability mass function of $Y$.


The function `sweep` "sweeps" out a row or column statistic from a matrix. Its syntax is
```r
sweep(X, MARGIN, STATS, fun, ...)
```

The arguments of `sweep` are

- `X` is the matrix / dataframe (or array in more general) to be used.
- `MARGIN` indicates whether we should "sweep" row-wise (`MARGIN=1`) or column-wise (`MARGIN=2`).
- `STATS` is the vector to be "swept" out. 
- `FUN` is the function (or operator, in quotes) used to "sweep" out `STATS`. `FUN` must have at least two arguments, additional arguments can be passed on to `FUN` using the `...` argument. The default value of `FUN` is `-`. 

Mathematically speaking, `sweep` returns a matrix `R` of the same dimension as `X`, calculated as

- $R_{ij}=\texttt{FUN}(X_{ij}, STATS_i)$ if `MARGIN=1`, and 
- $R_{ij}=\texttt{FUN}(X_{ij}, STATS_j)$ if `MARGIN=2`

for $i=1,\ldots,n$ and $j=1,\ldots,p$.

For example we can subtract the mean of each column from `X` using
```{r}
X.means <- apply(X, 2, mean)      # We could have also used colMeans(X)
sweep(X, 2, X.means, "-")
```
  
(c) Use `sweep` to calculate the conditional probability mass functions of $X$ given $Y$. These are calculated by dividing each row by its sum. 
(d) Use `sweep` to calculate the conditional probability mass functions of $Y$ given $X$. These are calculated by dividing each column by its sum. 
(e) Can you also answer parts (a) to (d) using loops?

####[answer]

\mbox{ }

#####[video, videoid="qJixMaYXI7A", duration="10m24s"] Video model answers

We can use the following R code.

```{r}
# Define joint pmf
pmf <- rbind(c(0.1, 0.1, 0.1, 0.1),
             c(0, 0.2, 0, 0),
             c(0.2, 0, 0.1, 0.1))
colnames(pmf) <- c("X=1", "X=2", "X=3", "X=4")
rownames(pmf) <- c("Y=1", "Y=2", "Y=3")
pmf

# Obtain marginal pmfs 
marginal.x <- apply(pmf, 2, sum)
marginal.x <- colSums(pmf)            # alternative using colSums

marginal.y <- apply(pmf, 1, sum)
marginal.y <- rowSums(pmf)            # alternative using rowSums

# Obtain conditional pmfs
conditional.x.given.y <- sweep(pmf, 1, marginal.y, "/")
conditional.y.given.x <- sweep(pmf, 2, marginal.x, "/")

# Same thing using loops
marginal.x <- numeric(ncol(pmf))
for (j in seq_along(marginal.x))
    marginal.x <- sum(pmf[,j])

marginal.y <- numeric(nrow(pmf))
for (i in seq_along(marginal.y))
    marginal.y <- sum(pmf[i,])

conditional.of.x.given.y <- matrix(nrow=nrow(pmf), ncol=ncol(pmf))
for (i in 1:nrow(pmf))
    for (j in 1:ncol(pmf))      
        conditional.of.x.given.y[i,j] <- pmf[i,j]/marginal.y[i]

conditional.of.y.given.x <- matrix(nrow=nrow(pmf), ncol=ncol(pmf))
for (i in 1:nrow(pmf))
    for (j in 1:ncol(pmf))      
        conditional.of.y.given.x[i,j] <- pmf[i,j]/marginal.x[j]
```
####[/answer]
###[/task]

 
###[task]
Kernel density estimation is a technique that allows estimating the probability density function (p.d.f.) from a sample $\mathbf{x}=(x_1,\ldots,x_n)$ without making any parametric assumption. The kernel density estimate $\hat f_h$ at $\xi$ is
$$
\hat f_h(\xi)=\frac{1}{n\cdot h}\sum_{i=1}^n \phi\left(\frac{x_i-\xi}{h}\right),
$$
where $\phi(\cdot)$ is the p.d.f. of the $\textsf{N}(0,1)$ distribution (which can be obtained using the function `dnorm`). The scalar constant $h>0$ is a tuning parameter that controls how "wiggly" the estimated density will be.


(a) Write a function `kde` that takes the sample `x` (as a vector), `xi` ($\xi$, \emph{not} $x_i$) and `h` as arguments and that returns the estimate $\hat f_h(\xi)$ obtained for the sample $\mathbf{x}$. The default value of $h$ should be $1$.
(b) The vector `galaxies` (in `t4.RData`) contains velocities in 1000 km/sec of 82 galaxies from 6 well-separated conic sections of an unfilled survey of the Corona Borealis region.
 
	We want to estimate the p.d.f. of the velocities using a kernel density estimate. If the estimated density is multimodal, this suggests that there are voids and super-clusters in the far universe. 

	We want to evaluate $\hat f_h(\xi)$ for $\xi \in \{9, 9.1, 9.2, \ldots, 35\}$.

   	  i. Create a vector `xi` that contains the numbers $9$, $9.1$, \ldots, $35$.

	  ii. Create a vector `fhat` that contains the estimated density $\hat f_h(\xi)$ evaluated at each element of the vector `xi`.

	  iii. Finally, plot the estimated density by plotting `fhat` against `xi`. Does your plot suggest that the distribution of speeds is multimodal?

      iv. Repeat these steps for various choices of $h$ and comment on how the choice of $h$ influences the estimated density.

####[answer]

\mbox{ }

#####[video, videoid="2zz-xAE8J0E", duration="6m32s"] Video model answers

We can use the following R code.

```{r}
#' Evaluate kernel density estimate
#' @param x sample to be used to estimate the density 
#' @param xi value at which the density is to be evaluated (must be a scalar)
#' @param h bandwidth (default 1)
kde <- function(x, xi, h=1) {                
  1 / (length(x)*h) * sum(dnorm((xi-x)/h))   # The funny letter in the formula is a "xi"
}
```

Note that the function only accepts a scalar input (not an entire vector) for `xi`. If `xi` is a vector, we need to use a loop, otherwise we will obtain nonsensical results and maybe an error message.

```{r,  dev.args=list(pointsize=8)}
x <- galaxies                                  # Copy data into x          
xi <- seq(9, 35, by=0.1)                       # Create sequence xi
fhat <- numeric(length(xi))                    # Create empty vector to hold result
h <- 1
for (i in 1:length(xi))                        # Compute estimated density ..
  fhat[i] <- kde(x, xi[i], h=h)                # ... for every element of xi separately (using a loop)
plot(xi, fhat, type="l")
```

We can see (part iv.) that if we increase the bandwidth $h$, the density increasingly looks like a Gaussian. If we chose $h$ too small, the density will start consisting on many modes (one per observation if we choose $h$ very small). $h=1$ seems to be a reasonable choice.
  
####[/answer]
###[/task]

###[task] Josephus problem - challenging
According to Flavius Josephus' account of the siege of Yodfat, Flavius and his 40 soldiers were trapped in a cave, the exit of which was blocked by Romans. Flavius and his soldiers chose suicide over capture.
Flavius and the 40 soldiers formed a circle and every third person would get killed until only two persons remain, who would then surrender to the Romans. Find out who the two remaining soldiers are. 

Suppose Flavius and the 40 soldiers are numbered 1 to 41. Then the first person to be killed would be person 3, followed by person 6 etc. until person 39 gets killed. Persons 40 and 41 are initially spared. This is illustrated in the figure below. After person 41, the next person on the circle is person 1, who will be killed next. Persons 2 and 4 (person 3 having been killed already) will for the moment be spared, so person 5 is killed next etc.

```{r echo=FALSE, out.width='.75\\textwidth'}
knitr::include_graphics('josephus.pdf')
```
####[answer]

\mbox{ }

#####[video, videoid="KrOSFZa1qtI", duration="4m48s"] Video model answers

We can use the following R code.

```{r}  
alive <- rep(TRUE, 41)
position <- 1
count <- 0
while (sum(alive)>2) {
  if (alive[position]) {           # Check whether person at the current position is alive
    count <- count + 1             
    if (count==3) {                # If this is the third person who is still alive ...
      alive[position] <- FALSE    
      count <- 0
    }
  }
  if (position==41) {              # Move on to the next person
    position <- 1
  } else {
    position <- position + 1
  }
}
which(alive)
```  


####[/answer]

###[/task]

\newpage
