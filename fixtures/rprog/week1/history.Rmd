## About computers and computing

Before we take a look at R coding in more detail, let's take at computing in general. Data science wouldn't exist if computer weren't that powerful.

### A brief history of computing

###[video, videoid="mjDbSsKkVdc", duration="3m52s"]A brief history of computing

## Computer have become powerful2
The advances in computational power and data storage are the drivers behind the ascent of Data Science and Analytics. In this section we look at two simple examples illustrating how powerful desktop computers are nowadays. Don't worry too much about the details of the R code at this stage.

#### Matrix multiplication
Suppose we want to multiply two matrices, each having 1,000 rows and columns. How long would it take a "human computer"? Suppose that we can add or multiply two numbers in a second (which is rather optimistic), i.e. we manage to do 1 floating point operation per second.  The resulting matrix has 1,000 $\times$ 1,000 entries, i.e. we must compute 1,000,000 numbers, each of which is a sum of 1,000 products, meaning we need to carry out $2\cdot 10^9$ additions and multiplications, which would take us $2\cdot 10^9$ seconds, i.e. more than 32 years (working 24/7). Let's see how long R takes to do this (results are from a mid-range Core i5 processor bought a few years ago).

```{r, echo=TRUE}
n <- 1e3
A <- matrix(runif(n^2),nrow=n)          # Create a 1000x1000 matrix with random numbers.
B <- matrix(runif(n^2),nrow=n)          # Another 1000x1000 matrix with random numbers.
system.time(C <- A%*%B)                 # Time how long it takes to multiply them.
                                        # The third figure is the elapsed time in seconds.
```
So, this has only taken less than 1/10th of a second.

#### Sorting
 Suppose we have a data vector of 1,000,000 values. If we printed 5 values per row and 80 rows per page, the numbers would fill 2,500 pages. How long will it take to sort these values?
```{r, echo=TRUE}
n <- 1e6
x <- runif(n)                           # Create a vector with 1000000 random values.
system.time(sort(x))                    # Time how long it takes to sort them.
                                        # The third figure is the elapsed time in seconds.
```
Again, we obtained the result in much less than a second.


## Computers make mistakes

The enormous arithmetic power of modern computers can lead to the wrong impression that computers are infallible black boxes, which, regardless of what tasks we set them, obtain the right answer. Computers only have a finite precision and do not have the oversight most of us have and take for granted.

The following examples should act as a warning not to blindly trust a computer and as a reminder to be careful when using a computer to perform even basic arithmetic.

### Isn't addition commutative (i.e. the order of terms usually doesn't matter)?
All of us know that $10^{20}-10^{20} + 1=1$. Using R we obtain the same result:

```{r, echo=TRUE}
10^20-10^20+1
```

Of course $1 + 10^{20}-10^{20}=1$ as well, as this is the same sum, just with the terms re-arranged a little. According to R, however,

```{r, echo=TRUE}
1+10^20-10^20
```
None of us would have made this mistake, as we would see at once that the sum of $10^{20}$ and $-10^{20}$ is $0$, thus the answer must be $1$. The computer processes this sum from left to right, and for the computer $1+10^{20}\approx 10^{20}$, as $1$ is very small compared to $10^{20}$. In fact the next smallest number a computer can represent is $99,999,999,999,999,983,616$, which is much further away from $10^{20}-1$ than $10^{20}$ itself. Subtracting $10^{20}$ then yields the wrong result $0$. In other words, addition is not necessarily commutative on a computer, so the order of the terms might matter.

The reason behind this is that a computer only has finite precision, so we cannot represent arbitrarily large numbers, and there are "gaps" between the numbers. Like most other software, R uses [IEEE 754](http://en.wikipedia.org/wiki/IEEE_754) double precision floating point numbers. Floating point numbers are the computer implementation of scientific notation (like "$3\cdot 10^{-9}$"), i.e. the significant and the exponent are stored separately. Storing the exponent separately makes the decimal point "float". The largest number that can be represented this way is $2^{1024}\approx 1.7977\cdot 10^{308}$, which is large enough for most purposes. If a computation results in a value larger than this, a so-called arithmetic overflow occurs. In the past, this typically caused the program to abort. However, in IEEE 754 arithmetic and thus in R, the result is simply set to \texttt{+Inf} (or \texttt{-Inf}).

The reason causing the computer to get the wrong result in the above example are however the "gaps" between the numbers : between each number and next smaller (or larger) number there is a gap of about $2\cdot 10^{-16}$ times the number. And for $10^{20}$ this "gap" is larger than $1$ (see Figure 1).

![Illustration of the gaps between floating-point numbers and the nearest next smallest or largest number that can be represented](figure1.pdf)

Note that in our example (and in many other situations) we can ensure that this problem does not occur by making the computer carry out the operations in a certain order.

This is in no way a problem that is unique to R. In the above example, Excel obtains exactly the same (wrong) results as R.


### More supposedly simple arithmetic

You are used to rounding errors from your calculators. For example both on a computer and on a calculator $\frac{5}{6} - \frac{1}{6}\cdot 5$ is not $0$:

```{r, echo=TRUE}
5/6 - 1/6 * 5
```
A similar, but more surprising example is that $0.1+0.1+0.1-0.3$ is not $0$ on a computer:
```{r, echo=TRUE}
0.1+0.1+0.1-0.3
```
The result is almost (but only almost) $0$. Again, we would have expected the computer to get this right. (Again, Excel doesn't get this "right" either, just enter `=0.1+0.1+0.1-0.3>0` into a cell)

Almost all modern computers (as opposed to calculators) internally use a binary system instead of the decimal system we are used to. In binary representation, the digits of $0.1$ are periodic

$$
0.1 = 0\times 1 + 0 \times \frac12 + 0 \times \frac14 + 0\times\frac18 + 1 \times \frac1{16} + 1 \times \frac{1}{32}
+ 0 \times \frac1{64} + 0 \times \frac1{128} + 1 \times \frac1{256} + 1 \times \frac1{512}\dots
$$

i.e. on a computer 0.1 (decmial) is actually 0.000110011\dots (binary). So on a computer 0.1 is (just like $\frac56$) not a "nice" number.

####[task]Binary numbers
Explain why $0.3$ is $0.01001100110\ldots$ in binary representation.
#####[answer]
$$
0.3 = 0\times 1 + 0 \times \frac12 + 1 \times \frac14 + 0\times\frac18 + 0 \times \frac1{16} + 1 \times \frac{1}{32}
+ 1 \times \frac1{64} + 0 \times \frac1{128} + 0 \times \frac1{256} + 0 \times \frac1{512}\dots
$$
#####[/answer]
####[/task]

Thus our calculation is in binary numbers
$$0.000110011\ldots +0.000110011\ldots +  0.000110011\ldots - 0.010011001\ldots$$
As neither $0.1$, nor $0.3$ have a finite representation in a binary system a rounding error occurs.

To quote from the book *The Elements of Programming Style* by Kernighan and Plauger:

> $10.0$ times $0.1$ is hardly ever $1.0$.

For this reason, pocket calculators perform all their arithmetic in base 10 (even though this is a lot less efficient than base-2 arithmetic) so that school children are not tripped up by these issues.

The take-home message of these examples is that we should always expect small numerical errors and never test whether two numbers are exactly equal, but rather test whether their difference is quite small (say less than $10^{-8}$).

### Ariane V

Numerical overflow (and underflow) can have devastating effects. For example, the first Ariane V rocket was lost on its maiden flight in 1996 only 37 seconds after take-off, when it got out of control and was destroyed by its own safety mechanism.

 It turned out that the crash was triggered by the conversion of a 64-bit floating point number to a 16-bit signed integer. 64-bit floating point numbers can store much bigger numbers than 16-bit signed integers. When the on-board computer attempted to convert one such number, it was too large for a 16-bit signed integer and caused a hardware exception, which in turn caused parts of the on-board computer to shut down. The software engineers had disabled the software handling of this exception to make the code run faster. The resulting economic loss was more than Â£200 million.

### Medical studies

Data analysts and statisticians can also run into similar numerical problems. From the [New York Times of June 5th, 2002](http://query.nytimes.com/gst/fullpage.html?res=9C01E5DB1E3AF936A35755C0A9649C8B63):

> Revisiting their own data with new methods, scientists who conducted influential studies that linked sooty air pollution with higher death rates have lowered their estimate of the risk posed by bad-air days. ... The work has been published for several years in a variety of leading journals like *The New England Journal of Medicine* and *The American Journal of Epidemiology*. The project, the National Morbidity, Mortality and Air Pollution Study, was given extra weight by policy makers because of the reputation of the Health Effects Institute and the Johns Hopkins group, led by Dr. Jonathan M. Samet, chairman of epidemiology at the public health school there. ... As part of a continuing effort to check for flaws, those scientists in recent weeks used a new method to look at their figures and obtained different results. They re-examined the original figures and found that the problem lay with how they used off-the-shelf statistical software to identify telltale patterns that are somewhat akin to ripples from a particular rock tossed into a wavy sea. Instead of adjusting the program to the circumstances that they were studying, they used standard default settings for some calculations. That move apparently introduced a bias in the results, the team says in the papers on the Web.

The authors had used too lenient convergence criteria incorrectly suggesting that the model had already converged.


Don't worry too much about these issues for now, but beware that if you are not careful one day you will get tripped up by one of these issues. You will come across one or the other oddity which is due to numerical problems during the course or later on in your life as an R programmer.

<!--[if PDF]>
\newpage
<![endif]-->
